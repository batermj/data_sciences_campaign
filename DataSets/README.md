## Open Data Sources
* _**Availability and access**: the data must be available as a whole and at no more than a reasonable reproduction cost, preferably by downloading over the internet. The data must also be available in a convenient and modifiable form._
* _**Reuse and redistribution**: the data must be provided under terms that permit reuse and redistribution including the intermixing with other datasets. The data must be machine-readable._
* _**Universal participation**: everyone must be able to use, reuse and redistribute — there should be no discrimination against fields of endeavour or against persons or groups. For example, ‘non-commercial’ restrictions that would prevent ‘commercial’ use, or restrictions of use for certain purposes (e.g. only in education), are not allowed._

-- _Definition by the [Open Knowledge Foundation](https://okfn.org/opendata/)_

## Datasets Search Tools
+ [Google’s Dataset Search](https://toolbox.google.com/datasetsearch)

## Open Data

* [List of Public Datasets](https://github.com/caesar0301/awesome-public-datasets) - user-curated
* [DBpedia](http://wiki.dbpedia.org/Datasets) - utilizing a large multi-domain ontology
* [Public Data Sets on AWS](https://aws.amazon.com/datasets?_encoding=UTF8&jiveRedirect=1) - common web crawl corpus, NASA satellite imagery, Human Genome, Google Book NGrams, Wikipedia Traffic, Million Song Dataset, Federal Reserve Economic Data, PubChem, more.

## Temperature
* [National Aeronautics and Space AdministrationGoddard Institute for Space Studies](https://data.giss.nasa.gov/)

## Private Opened Data
* [New York Times](http://data.nytimes.com/) - vocabulary as linked open data; linked vocabulary of people, places, companies, etc.

## Governmental Data

[Compendium of Governmental Open Data Sources](http://datacatalogs.org/)

* [Data.gov (USA)](http://www.data.gov/)
* [Africa Open Data](http://africaopendata.org/dataset)
* [US Census](http://www.census.gov/data/developers/data-sets.html) - Population Estimates and Projections, Nonemployer Statistics and County Business Patterns, Economic Indicators Time Series, more.

## Non-Governmental Org Data

* [The World Bank](http://data.worldbank.org/topic/private-sector) - business regulation measures, company-level data in emerging markets, household consumption patterns, World Development Indicators, World Bank finances
* ^[Pew Research Center's Internet Project](http://www.pewinternet.org/datasets/pages/3/)

## Academic Data

[Inter-university Consortium for Political and Social Research Data Portal](http://www.icpsr.umich.edu/icpsrweb/ICPSR/access/subject.jsp)

* [Surveys of Economic Attitudes and Behavior](http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies?classification=ICPSR.IV.B.)
* [Continuing Series of Consumer Surveys](http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies?classification=ICPSR.IV.A.)
* [Historical and Contemporary Economic Processes and Indicators](http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies?classification=ICPSR.IV.C.)

## Truly Random Data

* [200,000+ Jeopardy! Questions in a JSON file](http://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/)
* [10,000 annotated images of cats](http://137.189.35.203/WebUI/CatDatabase/catData.html)

## Open Data Resources

* reddit [r/datasets](http://www.reddit.com/r/datasets/)
* [Open Data - Stack Exchange](http://opendata.stackexchange.com/) (discussion)

## Data Sets used in Data Sciences Courses

| No. | Path | Description | Original Link |
| :-- | :-- | :-- | :-- |
| 31 | pybk031master | Python Visualization |

## References of Awesome Datasets used for Data Mining Courses

| No. | Path | Description | Original Link |
| :--- | :--- | :--- | :--- |
| 1 | N/A | Open or Easy Access Clinical Data Sources for Biomedical Research | https://github.com/EpistasisLab/ClinicalDataSources |
| 2 | N/A | Machine Learning Repository | http://archive.ics.uci.edu/ml/datasets.html |
| 3 | N/A | United States Census Data |  |
| 4 | N/A | The United States Census  | https://cran.r-project.org/web/packages/choroplethr/ |
| 5 | N/A | FBI Crime Data  | https://ucr.fbi.gov/crime-in-the-u.s/2013/crime-in-the-u.s.-2013/tables/table-8/table_8_offenses_known_to_law_enforcement_by_state_by_city_2013.xls/view |
| 6 | N/A | CDC Cause of Death | http://wonder.cdc.gov/ |
| 7 | N/A | Medicare Hospital Quality | https://data.medicare.gov/data/hospital-compare# |
| 8 | N/A | SEER Cancer Incidence | https://seer.cancer.gov/faststats/selections.php?series=cancer |
| 9 | N/A | Bureau of Labor Statistics | https://www.bls.gov/data/ |
| 10 | N/A | The Bureau of Economic Analysis | https://www.bea.gov/national/index.htm |
| 11 | N/A | IMF Economic Data | http://data.imf.org/?sk=388DFA60-1D26-4ADE-B505-A05A558D9A42 |
| 12 | N/A | Enron Emails | http://www.cs.cmu.edu/~enron/ |
| 13 | N/A | Google N-Grams | https://aws.amazon.com/datasets/google-books-ngrams/ |
| 14 | N/A | Reddit Comments | https://www.kaggle.com/account/login?returnUrl=%2Fc%2Freddit-comments-may-2015%2F |
| 15 | N/A | Wikipedia | https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia |
| 16 | N/A | Lending Club | https://www.lendingclub.com/info/download-data.action |
| 17 | N/A | Walmart | https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data |
| 18 | N/A | Airbnb | http://tomslee.net/airbnb-data-collection-get-the-data |
| 19 | N/A | Yelp | https://www.yelp.com/dataset |
| 19 | N/A | The portal for public datasets | https://data.sfgov.org/ |
| TBC | TBC | TBC | TBC |


## References of Awesome Police Data Science警察相关数据科学
| 编号 | 数据集名称 | 数据集链接 |
| :---: | :--- |  :--- | 
| 1 | the uk police data repository |  http://ukpolice.njtierney.com | 

## References of Awesome Security Data Science安全数据科学

| 编号 | 数据集名称 | 数据集链接 |
| :---: | :--- |  :--- | 
| 1 | 安全相关数据样本集 |  http://www.secrepo.com/ | 
| 2 | DARPA 入侵检测数据集 |  https://www.ll.mit.edu/ideval/data/ | 
| 3 | Stratosphere IPS 数据集 |  https://www.stratosphereips.org/category/dataset.html | 
| 4 | 开放数据集 |  https://csr.lanl.gov/data/ | 
| 5 | NSA 的数据捕获 |  https://www.westpoint.edu/crc/SitePages/DataSets.aspx | 
| 6 | ADFA 入侵检测数据集 |  https://www.unsw.adfa.edu.au/australian-centre-for-cyber-security/cybersecurity/ADFA-IDS-Datasets/ | 
| 7 | NSL-KDD 数据集 |  https://github.com/defcom17/NSL_KDD | 
| 8 | 恶意 URL 数据集 |  http://www.sysnet.ucsd.edu/projects/url/ | 
| 9 | 多源安全事件数据集 |  https://csr.lanl.gov/data/cyber1/ | 
| 10 | 恶意软件训练集 |  http://marcoramilli.blogspot.cz/2016/12/malware-training-sets-machine-learning.html | 
| 11 | KDD Cup 1999 数据集 |  http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html | 
| 12 | Web 攻击载荷 |  https://github.com/foospidy/payloads | 
| 13 | WAF 恶意请求数据集 |  https://github.com/faizann24/Fwaf-Machine-Learning-driven-Web-Application-Firewall | 
| 14 | 恶意软件训练数据集 |  https://github.com/marcoramilli/MalwareTrainingSets | 
| 15 | Aktaion 数据集 |  https://github.com/jzadeh/Aktaion/tree/master/data | 
| 16 | DeepEnd 研究中的犯罪数据集 |  https://www.dropbox.com/sh/7fo4efxhpenexqp/AADHnRKtL6qdzCdRlPmJpS8Aa/CRIME?dl=0 | 
| 17 | 公开可用的 PCAP 文件数据集 |  http://www.netresec.com/?page=PcapFiles | 


## References of Awesome Public Datasets

+ A topic-centric list of high-quality open datasets in public domains. By everyone, for everyone!,https://github.com/awesomedata/awesome-public-datasets
+ An awesome list of (large-scale) public datasets on the Internet. (On-going collection), https://github.com/oana-co/awesome-public-datasets
+ COCO,http://cocodataset.org/
+ ImageNet,http://www.image-net.org/?spm=a2c4e.11153959.blogcont576274.13.74da3027ZZdc8l
+ Open Images,https://github.com/openimages/dataset?spm=a2c4e.11153959.blogcont576274.16.74da3027ZZdc8l
+ VisualQA,http://www.visualqa.org/
+ The Street View House Numbers (SVHN) Dataset,http://ufldl.stanford.edu/housenumbers
+ CIFAR-10,http://www.cs.toronto.edu/~kriz/cifar.html
+ Fashion--MNIST, https://github.com/zalandoresearch/fashion-mnist
+ Large Movie Review Dataset,http://ai.stanford.edu/~amaas/data/sentiment
+ sentiment140,http://help.sentiment140.com/for-students/
+ WordNet,https://wordnet.princeton.edu
+ https://corpus.byu.edu/
+ http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm
+ https://www.yelp.com/dataset
+ Shared Task: Machine Translation, http://statmt.org/wmt11/translation-task.html
+ Jakobovski/free-spoken-digit-dataset, https://github.com/Jakobovski/free-spoken-digit-dataset
+ FMA: A Dataset For Music Analysis https://arxiv.org/abs/1612.01840, https://github.com/mdeff/fma
+ Ballroom, http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html
+ LibriSpeech ASR corpus, http://www.openslr.org/
+ Visual Geometry Group,http://www.robots.ox.ac.uk/~vgg/data/voxceleb
+ twitter sentiment analysis,https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis
+ Age Detection of Indian Actors,https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection
+ Urban Sound Classification,https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification
+

## References of Awesome Computer Vision Public Datasets: 立体视觉
+ 立体视觉:Middlebury
+ 立体视觉:KITTI

## References of Awesome Computer Vision Public Datasets：人脸检测和识别
+ 人脸检测:FDDB
+ 人脸识别:MegaFace

## References of Awesome Computer Vision Public Datasets: 行人检测
+ 行人检测:USC
+ 行人检测:CVC
+ 行人检测:TUD
+ 行人检测:NICTA
+ 行人检测:ETH
+ 行人检测:MIT
+ 行人检测:INRIA
+ 行人检测:Daimler
+ 行人检测:Caltech
+ SUNCG


## References of Awesome 知识语料库
+ 语言知识库:WordNet 
+ 语言知识库:FrameNet 
+ 语言知识库:EDR 
+ 语言知识库:北京大学综合型语言知识库 
+ 语言知识库:知网 
+ 语言知识库:概念层次网络

## References of Awesome 开源篇章标注语料库
+ 篇章标注语料库:PDTB 篇章语料库
+ 篇章标注语料库:指代标注语料库 ARRAU
+ 篇章标注语料库:篇章结构图库 GraphBank
+ 篇章标注语料库:OntoNotes 语料库
+ 标注语料库:CASIA-CASSIL
+ Switchboard-DAMSL 语料

## References of Awesome 开源视频数据集
+ Sports-1M
+ [Youtube-8M](https://research.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html)
+ HUMOS
+ Kinetics
+ Google AVA
+ Youtube-BB
+ Open Images
+ Facebook SOA (Sciences, Objects & Actions)
+ Facebook motions dataset

## References of Awesome Natural-Image Datasets
+ [MNIST](http://yann.lecun.com/exdb/mnist/): handwritten digits: The most commonly used sanity check. Dataset of 25x25, centered, B&W handwritten digits. It is an easy task — just because something works on MNIST, doesn’t mean it works.
+ [CIFAR10 / CIFAR100](https://www.cs.utoronto.ca/~kriz/cifar.html): 32x32 color images with 10 / 100 categories. Not commonly used anymore, though once again, can be an interesting sanity check.
+ [Caltech 101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/): Pictures of objects belonging to 101 categories.
+ [Caltech 256](http://www.vision.caltech.edu/Image_Datasets/Caltech256/): Pictures of objects belonging to 256 categories.
+ [STL-10 dataset](http://cs.stanford.edu/~acoates/stl10/): is an image recognition dataset for developing unsupervised feature learning, deep learning, self-taught learning algorithms. Like CIFAR-10 with some modifications.
+ [The Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/): House numbers from Google Street View. Think of this as recurrent MNIST in the wild.
+ [NORB](http://www.cs.nyu.edu/~ylclab/data/norb-v1.0/): Binocular images of toy figurines under various illumination and pose.
+ [Pascal VOC](http://pascallin.ecs.soton.ac.uk/challenges/VOC/): Generic image Segmentation / classification — not terribly useful for building real-world image annotation, but great for baselines
+ [Labelme](http://labelme.csail.mit.edu/Release3.0/browserTools/php/dataset.php): A large dataset of annotated images.
+ [ImageNet](http://image-net.org/): The de-facto image dataset for new algorithms. Many image API companies have labels from their REST interfaces that are suspiciously close to the 1000 category; WordNet; hierarchy from ImageNet.
+ [LSUN](http://lsun.cs.princeton.edu/2016/): Scene understanding with many ancillary tasks (room layout estimation, saliency prediction, etc.) and an associated competition.
+ [MS COCO](http://mscoco.org/): Generic image understanding / captioning, with an associated competition.
+ [COIL 20](http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php): Different objects imaged at every angle in a 360 rotation.
+ [COIL100](http://www1.cs.columbia.edu/CAVE/software/softlib/coil-100.php): Different objects imaged at every angle in a 360 rotation.
+ [Google’s Open Images](https://research.googleblog.com/2016/09/introducing-open-images-dataset.html): A collection of 9 million URLs to images “that have been annotated with labels spanning over 6,000 categories” under Creative Commons.

## References of Awesome Geospatial data
*   [OpenStreetMap](https://wiki.openstreetmap.org/wiki/P\t.osm): Vector data for the entire planet under a free license. It contains (an older version of) the US Census Bureau’s data.
*   [Landsat8](https://landsat.usgs.gov/landsat-8): Satellite shots of the entire Earth surface, updated every several weeks.
*   [NEXRAD](https://www.ncdc.noaa.gov/data-access/radar-data/nexrad): Doppler radar scans of atmospheric conditions in the US.

## References of Awesome Artificial Datasets
*   [Arcade Universe](https://github.com/caglar/Arcade-Universe): - An artificial dataset generator with images containing arcade games sprites such as tetris *   pentomino/tetromino objects. This generator is based on the O. Breleux’s bugland dataset generator.
A collection of datasets inspired by the ideas from [BabyAISchool](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/BabyAISchool)
*   [BabyAIShapes Datasets](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/BabyAIShapesDatasets): distinguishing between 3 simple shapes
*   [BabyAIImageAndQuestion Datasets](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/BabyAIImageAndQuestionDatasets): a question-image-answer dataset 
Datasets generated for the purpose of an empirical evaluation of deep architectures ([DeepVsShallowComparisonICML2007](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepVsShallowComparisonICML2007)):
*   [MnistVariations](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/MnistVariations): introducing controlled variations in MNIST
*   [RectanglesData](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/RectanglesData): discriminating between wide and tall rectangles
*   [ConvexNonConvex](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/ConvexNonConvex): discriminating between convex and nonconvex shapes
*   [BackgroundCorrelation](http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/BackgroundCorrelation): controling the degree of correlation in noisy MNIST backgrounds.

## References of Awesome Facial Datasets
*   [Labelled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/): 13,000 cropped facial regions (using; [Viola-Jones](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework) that have been labeled with a name identifier. A subset of the people present have two images in the dataset — it’s quite common for people to train facial matching systems here.
*   [UMD Faces](http://www.umdfaces.io/) Annotated dataset of 367,920 faces of 8,501 subjects.
*   CASIA WebFace Facial dataset of 453,453 images over 10,575 identities after face detection. Requires some filtering for quality.
*   [MS-Celeb-1M](https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/) 1 million images of celebrities from around the world. Requires some filtering for best results on deep networks.
*   [Olivetti](http://www.cs.nyu.edu/~roweis/data.html): A few images of several different people.
*   [Multi-Pie](http://www.multipie.org/): The CMU Multi-PIE Face Database
*   [Face-in-Action](http://www.flintbox.com/public/project/5486/)
*   [JACFEE](http://www.humintell.com/jacfee/): Japanese and Caucasian Facial Expressions of Emotion
*   [FERET](http://www.itl.nist.gov/iad/humanid/feret/feret_master.html): The Facial Recognition Technology Database
*   [mmifacedb](http://www.mmifacedb.com/): MMI Facial Expression Database
*   [IndianFaceDatabase](http://vis-www.cs.umass.edu/~vidit/IndianFaceDatabase/)
*   [The Yale Face Database](http://vision.ucsd.edu/content/yale-face-database) and [The Yale Face Database B](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html)).
*   [Mut1ny Face/Head segmentation dataset](http://www.mut1ny.com/face-headsegmentation-dataset) Over 16k pixel-level segmented images of faces/head images

## References of Awesome nlp-datasets
Alphabetical list of free/public domain datasets with text data for use in Natural Language Processing (NLP). Most stuff here is just raw unstructured text data, if you are looking for annotated corpora or Treebanks refer to the sources at the bottom.

* Google just released two new data sets focusing on realistic conversational speech. I'm particularly excited about the Disfl-QA data set - "the first dataset containing contextual disfluencies in an information seeking setting", https://ai.googleblog.com/2021/08/two-new-datasets-for-conversational-nlp.html

*   [Apache Software Foundation Public Mail Archives](http://aws.amazon.com/de/datasets/apache-software-foundation-public-mail-archives/): all publicly available Apache Software Foundation mail archives as of July 11, 2011 (200 GB)

*   [Blog Authorship Corpus](http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm): consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. 681,288 posts and over 140 million words. (298 MB)

*   [Amazon Fine Food Reviews [Kaggle]](https://www.kaggle.com/snap/amazon-fine-food-reviews): consists of 568,454 food reviews Amazon users left up to October 2012. [Paper](http://i.stanford.edu/~julian/pdfs/www13.pdf). (240 MB)

*   [Amazon Reviews](https://snap.stanford.edu/data/web-Amazon.html): Stanford collection of 35 million amazon reviews. (11 GB)

*   [ArXiv](http://arxiv.org/help/bulk_data_s3): All the Papers on archive as fulltext (270 GB) + sourcefiles (190 GB).

*   [ASAP Automated Essay Scoring [Kaggle]](https://www.kaggle.com/c/asap-aes/data): For this competition, there are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. (100 MB)

*   [ASAP Short Answer Scoring [Kaggle]](https://www.kaggle.com/c/asap-sas/data): Each of the data sets was generated from a single prompt. Selected responses have an average length of 50 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students primarily in Grade 10. All responses were hand graded and were double-scored. (35 MB)

*   [Classification of political social media](https://www.crowdflower.com/data-for-everyone/): Social media messages from politicians classified by content. (4 MB)

*   [CLiPS Stylometry Investigation (CSI) Corpus](http://www.clips.uantwerpen.be/datasets/csi-corpus): a yearly expanded corpus of student texts in two genres: essays and reviews. The purpose of this corpus lies primarily in stylometric research, but other applications are possible. (on request)

*   [ClueWeb09 FACC](http://lemurproject.org/clueweb09/FACC1/): [ClueWeb09](http://lemurproject.org/clueweb09/) with Freebase annotations (72 GB)

*   [ClueWeb11 FACC](http://lemurproject.org/clueweb12/FACC1/): [ClueWeb11](http://lemurproject.org/clueweb12/) with Freebase annotations (92 GB)

*   [Common Crawl Corpus](http://aws.amazon.com/de/datasets/common-crawl-corpus/): web crawl data composed of over 5 billion web pages (541 TB)

*   [Cornell Movie Dialog Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html): contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts: 220,579 conversational exchanges between 10,292 pairs of movie characters, 617 movies (9.5 MB)

*   [Corporate messaging](http://aws.amazon.com/de/datasets/common-crawl-corpus/): A data categorization job concerning what corporations actually talk about on social media. Contributors were asked to classify statements as information (objective statements about the company or it’s activities), dialog (replies to users, etc.), or action (messages that ask for votes or ask users to click on links, etc.). (600 KB)

*   [Crosswikis](http://nlp.stanford.edu/data/crosswikis-data.tar.bz2/): English-phrase-to-associated-Wikipedia-article database. Paper. (11 GB) 

*   [DBpedia](http://aws.amazon.com/de/datasets/dbpedia-3-5-1/?tag=datasets%23keywords%23encyclopedic): a community effort to extract structured information from Wikipedia and to make this information available on the Web (17 GB)

*   [Death Row](http://www.tdcj.state.tx.us/death_row/dr_executed_offenders.html): last words of every inmate executed since 1984 online (HTML table)

*   [Del.icio.us](http://arvindn.livejournal.com/116137.html): 1.25 million bookmarks on delicious.com

*   [Disasters on social media](https://www.crowdflower.com/data-for-everyone/): 10,000 tweets with annotations whether the tweet referred to a disaster event (2 MB).

*   [Economic News Article Tone and Relevance](https://www.crowdflower.com/data-for-everyone/): News articles judged if relevant to the US economy and, if so, what the tone of the article was. Dates range from 1951 to 2014. (12 MB)

*   [Enron Email Data](http://aws.amazon.com/de/datasets/enron-email-data/): consists of 1,227,255 emails with 493,384 attachments covering 151 custodians (210 GB)

*   [Event Registry](http://eventregistry.org/): Free tool that gives real time access to news articles by 100.000 news publishers worldwide. [Has API](https://github.com/gregorleban/EventRegistry/). (query tool)

*   [Examiner.com - Spam Clickbait News Headlines [Kaggle]](https://www.kaggle.com/therohk/examine-the-examiner): 3 Million crowdsourced News headlines published by now defunct clickbait website The Examiner from 2010 to 2015. (200 MB)

*   [Federal Contracts from the Federal Procurement Data Center (USASpending.gov)](http://aws.amazon.com/de/datasets/federal-contracts-from-the-federal-procurement-data-center-usaspending-gov/): data dump of all federal contracts from the Federal Procurement Data Center found at USASpending.gov (180 GB)

*   [Flickr Personal Taxonomies](http://www.isi.edu/~lerman/downloads/flickr/flickr_taxonomies.html): Tree dataset of personal tags (40 MB)

*   [Freebase Data Dump](http://aws.amazon.com/de/datasets/freebase-data-dump/): data dump of all the current facts and assertions in Freebase (26 GB) 

*   [Freebase Simple Topic Dump](http://aws.amazon.com/de/datasets/freebase-simple-topic-dump/): data dump of the basic identifying facts about every topic in Freebase (5 GB)

*   [Freebase Quad Dump](http://aws.amazon.com/de/datasets/freebase-quad-dump/): data dump of all the current facts and assertions in Freebase (35 GB)

*   [GigaOM Wordpress Challenge [Kaggle]](https://www.kaggle.com/c/predict-wordpress-likes/data): blog posts, meta data, user likes (1.5 GB)

*   [Google Books Ngrams](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html): available also in hadoop format on amazon s3 (2.2 TB)

*   [Google Web 5gram](https://catalog.ldc.upenn.edu/LDC2006T13): contains English word n-grams and their observed frequency counts (24 GB)

*   [Gutenberg Ebook List](http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs): annotated list of ebooks (2 MB)

*   [Hansards text chunks of Canadian Parliament](http://www.isi.edu/natural-language/download/hansard/): 1.3 million pairs of aligned text chunks (sentences or smaller fragments) from the official records (Hansards) of the 36th Canadian Parliament. (82 MB)

*   [Harvard Library](http://library.harvard.edu/open-metadata#Harvard-Library-Bibliographic-Dataset): over 12 million bibliographic records for materials held by the Harvard Library, including books, journals, electronic resources, manuscripts, archival materials, scores, audio, video and other materials. (4 GB)

*   [Hate speech identification](https://github.com/t-davidson/hate-speech-and-offensive-language): Contributors viewed short text and identified if it a) contained hate speech, b) was offensive but without hate speech, or c) was not offensive at all. Contains nearly 15K rows with three contributor judgments per text string. (3 MB)

*   [Hillary Clinton Emails [Kaggle]](https://www.kaggle.com/kaggle/hillary-clinton-emails): nearly 7,000 pages of Clinton's heavily redacted emails (12 MB)

*   [Home Depot Product Search Relevance [Kaggle]](https://www.kaggle.com/c/home-depot-product-search-relevance/data): contains a number of products and real customer search terms from Home Depot's website. The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search/product pairs to multiple human raters. (65 MB)

*   [Identifying key phrases in text](https://www.crowdflower.com/data-for-everyone/): Question/Answer pairs + context; context was judged if relevant to question/answer. (8 MB)

*   [Jeopardy](http://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/): archive of 216,930 past Jeopardy questions (53 MB)

*   [200k English plaintext jokes](https://github.com/taivop/joke-dataset): archive of 208,000 plaintext jokes from various sources.

*   [Machine Translation of European Languages](http://statmt.org/wmt11/translation-task.html#download): (612 MB)

*   [Material Safety Datasheets](http://aws.amazon.com/de/datasets/material-safety-data-sheets/): 230,000 Material Safety Data Sheets. (3 GB)

*   [Million News Headlines - ABC Australia [Kaggle]](https://www.kaggle.com/therohk/million-headlines): 1.3 Million News headlines published by ABC News Australia from 2003 to 2017. (56 MB)

*   [MCTest](http://research.microsoft.com/en-us/um/redmond/projects/mctest/index.html): a freely available set of 660 stories and associated questions intended for research on the machine comprehension of text; for question answering (1 MB)

*   [NEGRA](http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/negra-corpus.html): A Syntactically Annotated Corpus of German Newspaper Texts. Available for free for all Universities and non-profit organizations. Need to sign and send form to obtain. (on request)

*   [News Headlines of India - Times of India [Kaggle]](https://www.kaggle.com/therohk/india-headlines-news-dataset): 2.7 Million News Headlines with category published by Times of India from 2001 to 2017. (185 MB)

*   [News article / Wikipedia page pairings](https://www.crowdflower.com/data-for-everyone/): Contributors read a short article and were asked which of two Wikipedia articles it matched most closely. (6 MB)

*   [NIPS2015 Papers (version 2) [Kaggle]](https://www.kaggle.com/benhamner/nips-2015-papers/version/2): full text of all NIPS2015 papers (335 MB)

*   [NYTimes Facebook Data](http://minimaxir.com/2015/07/facebook-scraper/): all the NYTimes facebook posts (5 MB)

*   [One Week of Global News Feeds [Kaggle]](https://www.kaggle.com/therohk/global-news-week): News Event Dataset of 1.4 Million Articles published globally in 20 languages over one week of August 2017. (115 MB)

*   [Objective truths of sentences/concept pairs](https://www.crowdflower.com/data-for-everyone/): Contributors read a sentence with two concepts. For example “a dog is a kind of animal” or “captain can have the same meaning as master.” They were then asked if the sentence could be true and ranked it on a 1-5 scale. (700 KB)

*   [Open Library Data Dumps](https://openlibrary.org/developers/dumps): dump of all revisions of all the records in Open Library. (16 GB)

*   [Personae Corpus](http://www.clips.uantwerpen.be/datasets/personae-corpus): collected for experiments in Authorship Attribution and Personality Prediction. It consists of 145 Dutch-language essays by 145 different students. (on request)

*   [Reddit Comments](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/): every publicly available reddit comment as of july 2015. 1.7 billion comments (250 GB)

*   [Reddit Comments (May ‘15) [Kaggle]](https://www.kaggle.com/reddit/reddit-comments-may-2015): subset of above dataset (8 GB)

*   [Reddit Submission Corpus](https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/): all publicly available Reddit submissions from January 2006 - August 31, 2015). (42 GB)

*   [Reuters Corpus](http://trec.nist.gov/data/reuters/reuters.html): a large collection of Reuters News stories for use in research and development of natural language processing, information retrieval, and machine learning systems. This corpus, known as "Reuters Corpus, Volume 1" or RCV1, is significantly larger than the older, well-known Reuters-21578 collection heavily used in the text classification community. Need to sign agreement and sent per post to obtain. (2.5 GB)

*   [SaudiNewsNet](https://github.com/ParallelMazen/SaudiNewsNet): 31,030 Arabic newspaper articles alongwith metadata, extracted from various online Saudi newspapers. (2 MB)

*   [SMS Spam Collection](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/): 5,574 English, real and non-enconded SMS messages, tagged according being legitimate (ham) or spam.  (200 KB) 

*   [SouthparkData](https://github.com/BobAdamsEE/SouthParkData): .csv files containing script information including: season, episode, character, & line. (3.6 MB)

*   [Stackoverflow](http://data.stackexchange.com/): 7.3 million stackoverflow questions + other stackexchanges (query tool)

*   [Twitter Cheng-Caverlee-Lee Scrape](https://archive.org/details/twitter_cikm_2010): Tweets from September 2009 - January 2010, geolocated. (400 MB)

*   [Twitter New England Patriots Deflategate sentiment](https://www.crowdflower.com/data-for-everyone/): Before the 2015 Super Bowl, there was a great deal of chatter around deflated footballs and whether the Patriots cheated. This data set looks at Twitter sentiment on important days during the scandal to gauge public sentiment about the whole ordeal. (2 MB)

*   [Twitter Progressive issues sentiment analysis](https://www.crowdflower.com/data-for-everyone/): tweets regarding a variety of left-leaning issues like legalization of abortion, feminism, Hillary Clinton, etc. classified if the tweets in question were for, against, or neutral on the issue (with an option for none of the above). (600 KB)

*   [Twitter Sentiment140](http://help.sentiment140.com/for-students/): Tweets related to brands/keywords. Website includes papers and research ideas. (77 MB)

*   [Twitter sentiment analysis: Self-driving cars](https://www.crowdflower.com/data-for-everyone/): contributors read tweets and classified them as very positive, slightly positive, neutral, slightly negative, or very negative. They were also prompted asked to mark if the tweet was not relevant to self-driving cars. (1 MB)

*   [Twitter Tokyo Geolocated Tweets](http://followthehashtag.com/datasets/200000-tokyo-geolocated-tweets-free-twitter-dataset/): 200K tweets from Tokyo. (47 MB)

*   [Twitter UK Geolocated Tweets](http://followthehashtag.com/datasets/170000-uk-geolocated-tweets-free-twitter-dataset/): 170K tweets from UK. (47 MB)

*   [Twitter USA Geolocated Tweets](http://followthehashtag.com/datasets/free-twitter-dataset-usa-200000-free-usa-tweets/): 200k tweets from the US (45MB)

*   [Twitter US Airline Sentiment [Kaggle]](https://www.kaggle.com/crowdflower/twitter-airline-sentiment): A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as "late flight" or "rude service"). (2.5 MB)

*   [U.S. economic performance based on news articles](https://www.crowdflower.com/data-for-everyone/): News articles headlines and excerpts ranked as whether relevant to U.S. economy. (5 MB)

*   [Urban Dictionary Words and Definitions [Kaggle]](https://www.kaggle.com/therohk/urban-dictionary-words-dataset): Cleaned CSV corpus of 2.6 Million of all Urban Dictionary words, definitions, authors, votes as of May 2016. (238 MB)

*   [Wesbury Lab Usenet Corpus](http://aws.amazon.com/de/datasets/the-westburylab-usenet-corpus/): anonymized compilation of postings from 47,860 English-language newsgroups from 2005-2010 (40 GB)

*   [Wesbury Lab Wikipedia Corpus](http://www.psych.ualberta.ca/~westburylab/downloads/westburylab.wikicorp.download.html) Snapshot of all the articles in the English part of the Wikipedia that was taken in April 2010. It was processed, as described in detail below, to remove all links and irrelevant material (navigation text, etc) The corpus is untagged, raw text. Used by [Stanford NLP](https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9060444488071171966&as_sdt=5) (1.8 GB).

*   [Wikipedia Extraction (WEX)](http://aws.amazon.com/de/datasets/wikipedia-extraction-wex/): a processed dump of english language wikipedia (66 GB)

*   [Wikipedia XML Data](http://aws.amazon.com/de/datasets/wikipedia-xml-data/): complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML. (500 GB)

*   [Yahoo! Answers Comprehensive Questions and Answers](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): Yahoo! Answers corpus as of 10/25/2007. Contains 4,483,032 questions and their answers. (3.6 GB)

*   [Yahoo! Answers consisting of questions asked in French](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): Subset of the Yahoo! Answers corpus from 2006 to 2015 consisting of 1.7 million questions posed in French, and their corresponding answers. (3.8 GB)

*   [Yahoo! Answers Manner Questions](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): subset of the Yahoo! Answers corpus from a 10/25/2007 dump, selected for their linguistic properties. Contains 142,627 questions and their answers. (104 MB)

*   [Yahoo! HTML Forms Extracted from Publicly Available Webpages](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): contains a small sample of pages that contain complex HTML forms, contains 2.67 million complex forms. (50+ GB)

*   [Yahoo! Metadata Extracted from Publicly Available Web Pages](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): 100 million triples of RDF data (2 GB)

*   [Yahoo N-Gram Representations](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): This dataset contains n-gram representations. The data may serve as a testbed for query rewriting task, a common problem in IR research as well as to word and sentence similarity task, which is common in NLP research. (2.6 GB)

*   [Yahoo! N-Grams, version 2.0](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): n-grams (n = 1 to 5), extracted from a corpus of 14.6 million documents (126 million unique sentences, 3.4 billion running words) crawled from over 12000 news-oriented sites (12 GB)

*   [Yahoo! Search Logs with Relevance Judgments](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): Annonymized Yahoo! Search Logs with Relevance Judgments (1.3 GB)

*   [Yahoo! Semantically Annotated Snapshot of the English Wikipedia](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l): English Wikipedia dated from 2006-11-04 processed with a number of publicly-available NLP tools. 1,490,688 entries. (6 GB)

*   [Yelp](https://www.yelp.com/academic_dataset): including restaurant rankings and 2.2M reviews (on request)

*   [Youtube](https://www.reddit.com/r/datasets/comments/3gegdz/17_millions_youtube_videos_description/): 1.7 million youtube videos descriptions (torrent)

*   [Awesome public datasets/NLP](https://github.com/caesar0301/awesome-public-datasets#natural-language) (includes more lists)
*   [AWS Public Datasets](http://aws.amazon.com/de/datasets/)
*   [CrowdFlower: Data for Everyone](https://www.crowdflower.com/data-for-everyone/) (lots of little surveys they conducted and data obtained by crowdsourcing for a specific task)
*   [Kaggle 1](https://www.kaggle.com/datasets), [2](https://www.kaggle.com/competitions) (make sure though that the kaggle competition data can be used outside of the competition!)
*   [Open Library](https://openlibrary.org/developers/dumps)
*   [Quora](https://www.quora.com/Datasets-What-are-the-major-text-corpora-used-by-computational-linguists-and-natural-language-processing-researchers-and-what-are-the-characteristics-biases-of-each-corpus) (mainly annotated corpora)
*   [/r/datasets](https://www.reddit.com/r/datasets) (endless list of datasets, most is scraped by amateurs though and not properly documented or licensed)
*   [rs.io](http://rs.io/100-interesting-data-sets-for-statistics/) (another big list)
*   [Stackexchange: Opendata](http://opendata.stackexchange.com/)
*   [Stanford NLP group](http://www-nlp.stanford.edu/links/statnlp.html) (mainly annotated corpora and TreeBanks or actual NLP tools)
*   [Yahoo! Webscope](http://webscope.sandbox.yahoo.com/) (also includes papers that use the data that is provided)
*   Cooperative Driving Dataset (CODD), https://github.com/eduardohenriquearnold/CODD
*   [Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework](https://arxiv.org/abs/2202.06767)
*   


## References of Awesome Text Datasets
*   [20 newsgroups](http://qwone.com/~jason/20Newsgroups/): Classification task, mapping word occurences to newsgroup ID. One of the classic datasets for text classification) usually useful as a benchmark for either pure classification or as a validation of any IR / indexing algorithm.
*   [Reuters News dataset](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection): (Older) purely classification-based dataset with text from the newswire. Commonly used in tutorial.
*   [Penn Treebank](https://www.cis.upenn.edu/~treebank/): Used for next word prediction or next character prediction.
*   [UCI’s Spambase](https://archive.ics.uci.edu/ml/datasets/Spambase): (Older) classic spam email dataset from the famous UCI Machine Learning Repository. Due to details of how the dataset was curated, this can be an interesting baseline for learning personalized spam filtering.
*   [Broadcast News](http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC97S44): Large text dataset, classically used for next word prediction.
*   [Text Classification Datasets](https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M): From; Zhang et al., 2015; An extensive set of eight datasets for text classification. These are the benchmark for new text classification baselines. Sample size of 120K to 3.6M, ranging from binary to 14 class problems. Datasets from DBPedia, Amazon, Yelp, Yahoo! and AG.
*   [WikiText](http://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/): A large language modeling corpus from quality Wikipedia articles, curated by Salesforce MetaMind.
*   [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/): The Stanford Question Answering Dataset — broadly useful question answering and reading comprehension dataset, where every answer to a question is posed as a segment of text.
*   [Billion Words dataset](http://www.statmt.org/lm-benchmark/): A large general-purpose language modeling dataset. Often used to train distributed word representations such as word2vec.
*   [Common Crawl](https://commoncrawl.org/the-data/): Petabyte-scale crawl of the web — most frequently used for learning word embeddings. Available for free from Amazon S3. Can also be useful as a network dataset for it’s a crawl of the WWW.
*   [Google Books Ngrams](https://aws.amazon.com/datasets/google-books-ngrams/): Successive words from Google books. Offers a simple method to explore when a word first entered wide usage.
*   [Yelp Open Dataset](https://www.yelp.com/dataset): The Yelp dataset is a subset of Yelp businesses, reviews, and user data for use in NLP.

## References of Awesome Question answering
*   [Maluuba News QA Dataset](https://datasets.maluuba.com/NewsQA): 120K Q&A pairs on CNN news articles.
*   [Quora Question Pairs](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs): first dataset release from Quora containing duplicate / semantic similarity labels.
*   [CMU Q/A Dataset](https://www.cs.cmu.edu/~ark/QA-data/): Manually-generated factoid question/answer pairs with difficulty ratings from Wikipedia articles.
*   [Maluuba goal-oriented dialogue](https://datasets.maluuba.com/Frames): Procedural conversational dataset where the dialogue aims at accomplishing a task or taking a decision. Often used to work on chat bots.
*   [bAbi](https://research.fb.com/projects/babi/): Synthetic reading comprehension and question answering datasets from Facebook AI Research (FAIR).
*   [The Children’s Book Test](http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz): Baseline of (Question + context, Answer) pairs extracted from Children’s books available through Project Gutenberg. Useful for question-answering (reading comprehension) and factoid look-up.


## References of Awesome Sentiment
*   [Multidomain sentiment analysis dataset](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/) An older, academic dataset.
*   [IMDB](http://ai.stanford.edu/~amaas/data/sentiment/): An older, relatively small dataset for binary sentiment classification. Fallen out of favor for benchmarks in the literature in lieu of larger datasets.
*   [Stanford Sentiment Treebank](http://nlp.stanford.edu/sentiment/code.html): Standard sentiment dataset with fine-grained sentiment annotations at every node of each sentence’s parse tree.

## References of Awesome Recommendation and ranking systems
*   [Movielens](https://grouplens.org/datasets/movielens/): Movie ratings dataset from the Movielens website, in various sizes ranging from demo to mid-size.
*   [Million Song Dataset](https://www.kaggle.com/c/msdchallenge): Large, metadata-rich, open source dataset on Kaggle that can be good for people experimenting with hybrid recommendation systems.
*   [Last.fm](http://grouplens.org/datasets/hetrec-2011/): Music recommendation dataset with access to underlying social network and other metadata that can be useful for hybrid systems.
*   [Book-Crossing dataset](http://www.informatik.uni-freiburg.de/~cziegler/BX/): From the Book-Crossing community. Contains 278,858 users providing 1,149,780 ratings about 271,379 books.
*   [Jester](http://www.ieor.berkeley.edu/~goldberg/jester-data/): 4.1 million continuous ratings (-10.00 to +10.00) of 100 jokes from 73,421 users.
*   [Netflix Prize](http://www.netflixprize.com/): Netflix released an anonymized version of their movie rating dataset; it consists of 100 million ratings, done by 480,000 users who have rated between 1 and all of the 17,770 movies. First major Kaggle style data challenge. Only available unofficially, as privacy issues arose.
* ZhihuRec Dataset,清华大学信息检索实验室（THUIR）联合知乎推出当前规模最大的富上下文信息推荐数据集ZhihuRec。ZhihuRec数据集是清华大学信息检索实验室（THUIR）与知乎公司合作，基于知乎问答社区构建的一个具有富上下文信息的大规模推荐数据集。该数据集中的曝光数接近1亿，并具有目前为止最丰富的上下文信息。它可以被用于各种推荐方法，如协作过滤、基于内容的推荐、基于序列的推荐、知识增强的推荐和混合推荐等。此外，由于 ZhihuRec 数据集中信息丰富，不仅可以将它应用于推荐研究，还可以将它应用于用户建模（如性别预测、用户兴趣预测）、跨平台应用（查询平台和推荐平台）等有趣的课题。, https://github.com/THUIR/ZhihuRec-Dataset


## References of Awesome Networks and Graphs
*   [Amazon Co-Purchasing](http://snap.stanford.edu/data/#amazon): Amazon Reviews crawled data from “the users who bought this also bought…” section of Amazon, as well as Amazon review data for related products. Good for experimenting with recommendation systems in networks.
*   [Friendster Social Network Dataset](https://archive.org/details/friendster-dataset-201107): Before their pivot as a gaming website, Friendster released anonymized data in the form of friends lists for 103,750,348 users.
*   [Stanford Large Network Dataset Collection] (http://snap.stanford.edu/data/index.html): Stanford Large Network Dataset Collection

## References of Awesome Speech Datasets
*   [2000 HUB5 English](https://catalog.ldc.upenn.edu/LDC2002T43): English-only speech data used most recently in the Deep Speech paper from Baidu.
*   [LibriSpeech](http://www.openslr.org/12/): Audio books data set of text and speech. Nearly 500 hours of clean speech of various audio books read by multiple speakers, organized by chapters of the book containing both the text and the speech.
*   [VoxForge](http://www.voxforge.org/): Clean speech dataset of accented english. Useful for instances in which you expect to need robustness to different accents or intonations.
*   [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1): English-only speech recognition dataset.
*   [CHIME](http://spandh.dcs.shef.ac.uk/chime_challenge/data.html): Noisy speech recognition challenge dataset. Dataset contains real simulated and clean voice recordings. Real being actual recordings of 4 speakers in nearly 9000 recordings over 4 noisy locations, simulated is generated by combining multiple environments over speech utterances and clean being non-noisy recordings.
*   [TED-LIUM](http://www-lium.univ-lemans.fr/en/content/ted-lium-corpus): Audio transcription of TED talks. 1495 TED talks audio recordings along with full text transcriptions of those recordings.


## References of Awesome Symbolic Music Datasets
*   [Piano-midi.de](http://www.piano-midi.de/): classical piano pieces
*   [Nottingham](http://abc.sourceforge.net/NMD/): over 1000 folk tunes
*   [MuseData](http://musedata.stanford.edu/): electronic library of classical music scores
*   [JSB Chorales](http://www.jsbchorales.net/index.shtml): set of four-part harmonized chorales

## References of Awesome Miscellaneous Datasets
*   [CMU Motion Capture Database](http://mocap.cs.cmu.edu/)
*   [Brodatz dataset](http://www.ux.uis.no/~tranden/brodatz.html): texture modeling
300 terabytes of high-quality data from the Large Hadron Collider (LHC) at CERN
*   [NYC Taxi dataset](http://opendata.cern.ch/search?ln=en&p=Run2011A+AND+collection%3ACMS-Primary-Datasets+OR+collection%3ACMS-Simulated-Datasets+OR+collection%3ACMS-Derived-Datasets): NYC taxi data obtained as a result of a FOIA request, led to privacy issues.
*   [Uber FOIL dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml): Data for 4.5M pickups in NYC from an *   [Uber FOIL request](https://github.com/fivethirtyeight/uber-tlc-foil-response).
*   [Criteo click stream dataset](http://research.criteo.com/outreach/): Large Internet advertisement dataset from a major EU retargeter.

## References of Awesome Health,Biomedical data & Biology Data
*   [EU Surveillance Atlas of Infectious Diseases](http://ecdc.europa.eu/en/data-tools/atlas/Pages/atlas.aspx)
*   [Merck Molecular Activity Challenge](https://www.kaggle.com/c/MerckActivity/data)
*   [Musk dataset](https://archive.ics.uci.edu/ml/datasets/Musk+(Version+2)): The Musk database describes molecules occurring in different conformations. Each molecule is either musk or non-musk and one of the conformations determines this property.
*   [Gene Expression Omnibus](https://www.ncbi.nlm.nih.gov/geo/)
*   [KEGG](http://www.genome.jp/kegg/kegg2.html)
*   [Gene Ontology Consortium](http://www.geneontology.org) biological ontology
*   [REACTOME](https://reactome.org)
*   [STRING](https://string-db.org)
*   [新发布的MedMNIST (v2)：生物医学图像领域的MNIST，包括12个2D数据集（70万+样本）和6个3D数据集（1万+样本）](https://medmnist.com/)

## References of Awesome Weather data 
*   [NCEI's Radar Archive includes the Next Generation Weather Radar System (NEXRAD) and Terminal Doppler Weather Radar (TDWR) networks. ](https://www.ncdc.noaa.gov/data-access/radar-data)


## References of Awesome Government & statistics data
*   [Data USA](http://datausa.io/): The most comprehensive visualization of US public data
*   [EU Gender statistics database](http://eige.europa.eu/gender-statistics)
*   [The Netherlands’ Nationaal Georegister (Dutch)](http://www.nationaalgeoregister.nl/geonetwork/srv/dut/search#fast=index&from=1&to=50&any_OR_geokeyword_OR_title_OR_keyword=landinrichting*&relation=within)
*   [United Nations Development Programme Projects](http://open.undp.org/#2016)

## References of Awesome Airline data 
*   [Airline on-time performance](http://stat-computing.org/dataexpo/2009/ )


## Datasets for Autonomous Driving Projects
+ Driving Dataset, The Audi Autonomous Driving Dataset (A2D2) features over 41,000 labeled with 38 features. Around 2.3 TB in total, A2D2 is split by annotation type (i.e. semantic segmentation, 3D bounding box). https://www.a2d2.audi/a2d2/en.html
+ ApolloScape Dataset, ApolloScape is an evolving research project that aims to foster innovation across all aspects of autonomous driving, from perception to navigation and control. Via their website, users can explore a variety of simulation tools and over 100K street view frames, 80k lidar point cloud and 1000km trajectories for urban traffic., https://apolloscape.auto
+ Argoverse Dataset, The Argoverse dataset includes 3D tracking annotations for 113 scenes and over 324,000 unique vehicle trajectories for motion forecasting., https://www.argoverse.org
+ Berkeley DeepDrive Dataset, Also known as BDD 100K, the DeepDrive dataset gives users access to 100,000 annotated videos and 10 tasks to evaluate image recognition algorithms for autonomous driving. The dataset represents more than 1000 hours of driving experience with more than 100 million frames, as well as information on geographic, environmental, and weather diversity., 
+ CityScapes Dataset, CityScapes is a large-scale dataset focused on the semantic understanding of urban street scenes in 50 German cities. It features semantic, instance-wise, and dense pixel annotations for 30 classes grouped into 8 categories. The entire dataset includes 5,000 annotated images with fine annotations, and an additional 20,000 annotated images with coarse annotations.
+ Comma2k19 Dataset, This dataset includes 33 hours of commute time recorded on highway 280 in California. Each 1-minute scene was captured on a 20km section of highway driving between San Jose and San Francisco. The data was collected using comma EONs, which features a road-facing camera, phone GPS, thermometers and a 9-axis IMU.
+ Google-Landmarks Dataset, Published by Google in 2018, the Landmarks dataset is divided into two sets of images to evaluate recognition and retrieval of human-made and natural landmarks. The original dataset contains over 2 million images depicting 30 thousand unique landmarks from across the world. In 2019, Google published Landmarks-v2, an even larger dataset with 5 million images and 200k landmarks., https://ai.googleblog.com/2018/03/google-landmarks-new-dataset-and.html
+ KITTI Vision Benchmark Suite, First released in 2012 by Geiger et al, the KITTI dataset was released with the intent of advancing autonomous driving research with a novel set of real-world computer vision benchmarks. One of the first ever autonomous driving datasets, KITTI boasts over 4000 academic citations and counting., https://www.cvlibs.net/datasets/kitti-360/
+ LeddarTech PixSet Dataset, Launched in 2021, Leddar PixSet is a new, publicly available dataset for autonomous driving research and development that contains data from a full AV sensor suite (cameras, LiDARs, radar, IMU), and includes full-waveform data from the Leddar Pixell, a 3D solid-state flash LiDAR sensor. The dataset contains 29k frames in 97 sequences, with more than 1.3M 3D boxes annotated, https://leddartech.com/solutions/leddar-pixset-dataset/
+ Level 5 Open Data, Published by popular rideshare app Lyft, the Level5 dataset is another great source for autonomous driving data. It includes over 55,000 human-labeled 3D annotated frames, surface map, and an underlying HD spatial semantic map that is captured by 7 cameras and up to 3 LiDAR sensors that can be used to contextualize the data., https://level-5.global/data/
+ nuScenes Dataset, Developed by Motional, the nuScenes dataset is one of the largest open-source datasets for autonomous driving. Recorded in Boston and Singapore using a full sensor suite (32-beam LiDAR, 6 360° cameras and radars), the dataset contains over 1.44 million camera images capturing a diverse range of traffic situations, driving maneuvers, and unexpected behaviors. Examples from the nuScenes dataset: images collected from clear weather (col 1), nighttime (col 2), rain (col 3) and construction zones (col 4)., https://www.nuscenes.org
+ Oxford Radar RobotCar Dataset, The Oxford RobotCar Dataset contains over 100 recordings of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different environmental conditions, including weather, traffic and pedestrians, along with longer term changes such as construction and roadworks., https://robotcar-dataset.robots.ox.ac.uk
+ PandaSet, PandaSet was the first open-source AV dataset available for both academic and commercial use. It contains 48,000 camera images, 16,000 LiDAR sweeps, 28 annotation classes, and 37 semantic segmentation labels taken from a full sensor suite.
+ Google-Landmarks: A New Dataset and Challenge for Landmark Recognition, https://ai.googleblog.com/2018/03/google-landmarks-new-dataset-and.html
+ Udacity Self Driving Car Dataset, Online education platform Udacity has open sourced access to a variety of projects for autonomous driving, including neural networks trained to predict steering angles of the car, camera mounts, and dozens of hours of real driving data., https://public.roboflow.com/object-detection/self-driving-car
+ Waymo Open Dataset, The Waymo Open dataset is an open-source multimodal sensor dataset for autonomous driving. Extracted from Waymo self-driving vehicles, the data covers a wide variety of driving scenarios and environments. It contains 1000 types of different segments where each segment captures 20 seconds of continuous driving, corresponding to 200,000 frames at 10 Hz per sensor., https://waymo.com/open/
