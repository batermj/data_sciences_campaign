Top 100K Papers List
- Convention: [xxx] means belong to what category of AI fields
- Convention: <Reference> means it is a paper in the reference of the paper just under reproducing
- Priority: #1, SOTAs (state-of-the-art) Papers with Codes with SOTA performance first; 
- Priority: #2, Latest Papers (Hot Paper from Top Companies, Universities, Institutes) with Codes with SOTA performance; 
- Priority: #3, Latest Papers (Highly Cited Paper) with Codes with SOTA performance; 
- Priority: #4, Classics Papers with Codes with SOTA performance; 

# [Milestone of Paper Reproduction for Jul.01-Jul.30, 2023]
+ [ ] From Zero to AI Research Scientist Full Resources Guide, https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide
+ [ ] Google‚Äôs BERT
+ [ ] OpenAI‚Äôs GPT-2
+ [ ] [RL] Deep reinforcement learning from human preferences, https://arxiv.org/abs/1706.03741
+ [ ] [NLP] Lost in the Middle: How Language Models Use Long Contexts, https://arxiv.org/abs/2307.03172
+ [ ] [CV] Generating Images with Multimodal Language Models, https://arxiv.org/abs/2305.17216
+ [ ] [MLP] Scaling MLPs: A Tale of Inductive Bias, https://arxiv.org/abs/2306.13575
+ [ ] [Transformers] Compositional Attention: Disentangling Search and Retrieval, https://arxiv.org/abs/2110.09419
+ [ ] [Transformers] The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning, https://attentionneuron.github.io
+ [ ] [Transformers] Awesome NLP Paper Discussions, https://github.com/huggingface/awesome-papers#planned-discussions
+ [ ] [Transformers] Compositional Attention: Disentangling Search and Retrieval, https://arxiv.org/abs/2110.09419
+ [ ] [Transformers] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, https://arxiv.org/pdf/2205.14135.pdf
+ [ ] [Transformers] Attention is not Explanation, https://arxiv.org/pdf/1902.10186.pdf
+ [ ] [Transformers] Ultimate-Awesome-Transformer-Attention, https://github.com/cmhungsteve/Awesome-Transformer-Attention
+ [ ] [Transformers] OptFormer: Towards Universal Hyperparameter Optimization with Transformers, https://ai.googleblog.com/2022/08/optformer-towards-universal.html
+ [ ] [Transformers] Scaling Transformer to 1M tokens and beyond with RMT, https://arxiv.org/abs/2304.11062
+ [ ] [Transformers] Formal Algorithms for Transformers, https://arxiv.org/abs/2207.09238
+ [ ] [Transformers] DeepNet: Scaling Transformers to 1,000 Layers, https://arxiv.org/abs/2203.00555
+ [ ] [Transformers] MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER, https://arxiv.org/pdf/2110.02178.pdf
+ [ ] [Transformers] Mobile-Former: Bridging MobileNet and Transformer, https://arxiv.org/abs/2108.05895
+ [ ] [Transformers] SCENIC: A JAX Library for Computer Vision Research and Beyond, https://arxiv.org/pdf/2110.11403.pdf
+ [ ] [Transformers] Recipe for a General, Powerful, Scalable Graph Transformer, https://arxiv.org/abs/2205.12454
+ [ ] [Transformers] Decision Transformer: Reinforcement Learning via Sequence Modeling, https://arxiv.org/abs/2106.01345
+ [ ] [Transformers] Attention Bottlenecks for Multimodal Fusion, https://arxiv.org/pdf/2107.00135.pdf
+ [ ] [Transformers] Multimodal Transformer for Unaligned Multimodal Language Sequences, https://arxiv.org/pdf/1906.00295.pdf
+ [ ] [Transformers] Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case, https://arxiv.org/abs/2001.08317
+ [ ] [Transformers] Multivariate Time Series Forecasting with Transformers, https://towardsdatascience.com/multivariate-time-series-forecasting-with-transformers-384dc6ce989b
+ [ ] [Transformers] Transformers Can Do Bayesian Inference, https://arxiv.org/abs/2112.10510
+ [ ] [Transformers] Transformer Quality in Linear Time, https://arxiv.org/pdf/2202.10447.pdf
+ [ ] [Transformers] HyperMixer: An MLP-based Green AI Alternative to Transformers, https://arxiv.org/abs/2203.03691
+ [ ] [Transformers] Bringing Old Films Back to Life, https://arxiv.org/pdf/2203.17276.pdf
+ [ ] [Transformers] Specformer: Spectral Graph Neural Networks Meet Transformers, https://arxiv.org/abs/2303.01028
+ [ ] [Transformers] Self-Attention with Relative Position Representations, https://arxiv.org/abs/1803.02155
+ [ ] [Transformers] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ [ ] [Transformers] TransformerFusion: Monocular RGB Scene Reconstruction using Transformers, https://arxiv.org/abs/2107.02191
+ [ ] [Transformers] BumbleBee: A Transformer for Music, https://arxiv.org/abs/2107.03443
+ [ ] [Transformers] Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation, https://github.com/NVIDIA-Merlin/publications/tree/main/2021_acm_recsys_transformers4rec
+ [ ] [Transformers] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
+ [ ] [Transformers] Thinking Like Transformers, https://arxiv.org/abs/2106.06981
+ [ ] [Transformers] FNet: Mixing Tokens with Fourier Transforms, https://arxiv.org/abs/2105.03824
+ [ ] [Transformers] Convolutional Xformers for Vision, https://arxiv.org/abs/2201.10271
+ [ ] [Transformers] Transformer Memory as a Differentiable Search Index, https://arxiv.org/abs/2202.06991
+ [ ] Flat-Lattice-Transformer, https://github.com/LeeSureman/Flat-Lattice-Transformer
+ [ ] MobileViT: A mobile-friendly Transformer-based model for image classification, https://keras.io/examples/vision/mobilevit/
+ [ ] Scenic is developed in JAX and uses Flax., https://github.com/google-research/scenic
+ [ ] [Transformers] [Microsoft] Unveiling Transformers with LEGO: a synthetic reasoning task, https://arxiv.org/abs/2206.04301
+ [ ] [Transformers] [Google] Frequency Effects on Syntactic Rule Learning in Transformers, https://arxiv.org/pdf/2109.07020.pdf
+ [ ] [Transformers] [Google] Multi-Game Decision Transformers, https://sites.google.com/view/multi-game-transformers
+ [ ] [Transformers] Levenshtein Transformer, https://arxiv.org/pdf/1905.11006.pdf
+ [ ] [Transformers][Survey] An Impartial Take to the CNN vs Transformer Robustness Contest, https://arxiv.org/pdf/2207.11347.pdf
+ [ ] [Transformers][Survey] A Survey of Transformers, https://arxiv.org/abs/2106.04554
+ [ ] [Transformers][Survey] Transformers in Vision: A Survey, https://arxiv.org/abs/2101.01169
+ [ ] [Transformers][Survey] A Survey on Vision Transformer, https://arxiv.org/abs/2012.12556
+ [ ] [Transformers][Survey] A Survey of Visual Transformers, https://arxiv.org/abs/2111.06091
+ [ ] [Transformers][Survey] 2021-A Survey of Transformers, https://www.jianshu.com/p/98d82f83b6ba
+ [ ] [Transformers][Survey] AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, https://arxiv.org/abs/2108.05542
+ [ ] [Transformers][Survey] Efficient Transformers: A Survey, https://arxiv.org/pdf/2009.06732.pdf
+ [ ] [Transformers][Survey] Pure Transformers are Powerful Graph Learners, https://arxiv.org/abs/2207.02505
+ [ ] [Transformers][HuggingFace] ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
+ [ ] [Transformers][HuggingFace] ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.
+ [ ] [Transformers][HuggingFace] AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.
+ [ ] [Transformers][HuggingFace] Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.
+ [ ] [Transformers][HuggingFace] Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.
+ [ ] [Transformers][HuggingFace] BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.
+ [ ] [Transformers][HuggingFace] BARThez (from √âcole polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.
+ [ ] [Transformers][HuggingFace] BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.
+ [ ] [Transformers][HuggingFace] BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.
+ [ ] [CNN][Codes] Convolutional Neural Networks for Text, https://lena-voita.github.io/nlp_course/models/convolutional.html
+ [ ] [CNN][Paper] LeNet-5, GradientBased Learning Applied to Document Recognition, http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
+ [ ] [CNN][Paper] AlexNet, ImageNet Classification with Deep Convolutional Neural Networks, https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
+ [ ] [CNN][Paper] VGG, Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556
+ [ ] [VGG] VGG Introduced by Simonyan et al. in Very Deep Convolutional Networks for Large-Scale Image Recognition, https://paperswithcode.com/method/vgg
+ [ ] [CNN][Paper] Networks in Networks and 1x1 Convolutions, Network In Network, https://arxiv.org/abs/1312.4400
+ [ ] [CNN][Paper] Inception Network Motivation, Going Deeper with Convolutions, https://arxiv.org/abs/1409.4842
+ [ ] [CNN][Paper] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, https://arxiv.org/abs/1704.04861
+ [ ] [CNN][Paper] MobileNetV2: Inverted Residuals and Linear Bottlenecks, https://arxiv.org/abs/1801.04381
+ [ ] [CNN][Paper] Searching for MobileNetV3, https://arxiv.org/abs/1905.02244
+ [ ] [CNN][Paper] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, https://arxiv.org/abs/1905.11946
+ [ ] [Survey][LLM] A Survey of Large Language Models, https://arxiv.org/abs/2303.18223, https://github.com/RUCAIBox/LLMSurvey
+ [ ] [LLM] LoRA: Low-Rank Adaptation of Large Language Models, https://arxiv.org/abs/2106.09685
+ [ ] [Transformers] Vision Transformers (ViTs)
+ [ ] [Transformers] Swin Transformer, v1, v2
+ [ ] [Transformers] DeiT
+ [ ] [CNN][Codes] MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, https://github.com/facebookresearch/moco
+ [ ] [CNN][Codes] Masked Autoencoders: A PyTorch Implementation, https://github.com/facebookresearch/mae
+ [ ] [CNN][Codes] Detectron2, https://github.com/facebookresearch/detectron2
+ [ ] [CNN][Paper] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, https://arxiv.org/abs/1312.6229
+ [ ] [CNN][Course] OverFeat Integrated    Recogni.on,    Localiza.on    and    Detec.on    using Convolu.onal    Networks Sermanet    et.    al Presenta.on    by    Eric    Holmdahl, http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf
+ [ ] [CNN][Paper] You Only Look Once: Unified, Real-Time Object Detection, https://arxiv.org/abs/1506.02640
+ [ ] [CNN][Paper] A Comprehensive Review of YOLO: From YOLOv1 to YOLOv8 and Beyond, https://arxiv.org/abs/2304.00501
+ [ ] [CNN][Paper] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,https://arxiv.org/abs/2207.02696
+ [ ] [CNN][Paper] YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications, https://arxiv.org/abs/2209.02976
+ [ ] [CNN][Codes] YOLOv5 üöÄ is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite., https://pytorch.org/hub/ultralytics_yolov5/#:~:text=YOLOv5%20üöÄ%20is%20a%20family,Model
+ [ ] [CNN][Paper] YOLOv4: Optimal Speed and Accuracy of Object Detection, https://arxiv.org/abs/2004.10934
+ [ ] [CNN][Paper] YOLOv3: An Incremental Improvement, https://arxiv.org/abs/1804.02767
+ [ ] [CNN][Paper] YOLO9000: Better, Faster, Stronger, https://arxiv.org/abs/1612.08242
+ [ ] [CNN][Blog] YOLO Object Detection Explained, https://www.datacamp.com/blog/yolo-object-detection-explained
+ [ ] [CNN][Paper] R-CNN, 2013 rich feature hierarchies for accurate object detection and semantic segmentation, https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf, https://ieeexplore.ieee.org/document/6909475
+ [ ] [CNN][Blog] Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5), https://www.arxiv-vanity.com/papers/1311.2524/
+ [ ] [CNN][Paper] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, https://arxiv.org/abs/1506.01497
+ [ ] [CNN][Codes] Faster R-CNN, https://paperswithcode.com/method/faster-r-cnn
+ [ ] [CNN][Paper] Fast R-CNN
+ [ ] [CNN][Paper] RFCN
+ [ ] [CNN][Paper] Mask RCNN
+ [ ] [CNN][Paper] SSD
+ [ ] [CNN][Paper] U-Net: Convolutional Networks for Biomedical Image Segmentation, https://arxiv.org/abs/1505.04597
+ [ ] [CNN][Paper] U-Net, https://paperswithcode.com/method/u-net
+ [ ] [CNN][Blog] Understand Semantic segmentation with the Fully Convolutional Network U-Net step-by-step, https://pallawi-ds.medium.com/understand-semantic-segmentation-with-the-fully-convolutional-network-u-net-step-by-step-9d287b12c852
+ [ ] [CNN][Paper] Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs, https://arxiv.org/abs/1701.08816 
+ [ ] [CNN][Paper] Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks, https://arxiv.org/abs/1705.03820
+ [ ] [CNN][Paper] DeepFace: Closing the Gap to Human-Level Performance in Face Verification, https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf, https://ranzato.github.io
+ [ ] [CNN][Paper] FaceNet: A Unified Embedding for Face Recognition and Clustering, https://arxiv.org/abs/1503.03832
+ [ ] [CNN][Paper] Visualizing and Understanding Convolutional Networks, https://arxiv.org/abs/1311.2901
+ [ ] [CNN][Paper] A Neural Algorithm of Artistic Style, https://arxiv.org/abs/1508.06576
+ [ ] [CNN][Codes] SAM (Segment Anything Model), https://huggingface.co/docs/transformers/main/en/model_doc/sam
+ [ ] [CNN][Paper] Hyena Hierarchy: Towards Larger Convolutional Language Models, https://arxiv.org/abs/2302.10866
+ [ ] [CNN][Paper] Going Deeper with Convolutions, https://arxiv.org/abs/1409.4842
+ [ ] CLIP: Connecting text and images, https://openai.com/research/clip, Codes: https://github.com/openai/CLIP; Paper: Learning Transferable Visual Models From Natural Language Supervision, https://arxiv.org/abs/2103.00020
+ [ ] Image GPT, https://openai.com/research/image-gpt
+ [ ] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard, Zhu, Chen, Kalenichenko, Wang, Weyand, Andreetto, & Adam, 2017), https://arxiv.org/abs/1704.04861, https://github.com/fchollet/deep-learning-with-python-notebooks
+ [ ] MobileNetV2: Inverted Residuals and Linear Bottlenecks (Sandler, Howard, Zhu, Zhmoginov &Chen, 2018), https://arxiv.org/abs/1801.04381
+ [ ] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan & Le, 2019), https://arxiv.org/abs/1905.11946
+ [ ] You Only Look Once: Unified, Real-Time Object Detection (Redmon, Divvala, Girshick & Farhadi, 2015), https://arxiv.org/abs/1506.02640
+ [ ] YOLO9000: Better, Faster, Stronger (Redmon & Farhadi, 2016), https://arxiv.org/abs/1612.08242
+ [ ] YAD2K (GitHub: allanzelener), https://github.com/allanzelener/YAD2K
+ [ ] YOLO: Real-Time Object Detection, https://pjreddie.com/darknet/yolo/
+ [ ] Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs (Novikov, Lenis, Major, Hlad≈Øvka, Wimmer & B√ºhler, 2017), https://arxiv.org/abs/1701.08816
+ [ ] Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks (Dong, Yang, Liu, Mo & Guo, 2017), https://arxiv.org/abs/1705.03820
+ [ ] U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger, Fischer & Brox, 2015), https://arxiv.org/abs/1505.04597
+ [ ] FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff, Kalenichenko & Philbin, 2015), https://arxiv.org/pdf/1503.03832.pdf
+ [ ] DeepFace: Closing the Gap to Human-Level Performance in Face Verification (Taigman, Yang, Ranzato & Wolf), https://scontent.ffjr7-1.fna.fbcdn.net/v/t39.8562-6/240890413_887772915161178_4705912772854439762_n.pdf?_nc_cat=109&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=F5lzaUIsBnAAX8Z7wVC&_nc_ht=scontent.ffjr7-1.fna&oh=00_AfC8DK3sP5sye19oUs9dVjfLPdibMfA-3Z-MuBENbTogqg&oe=645556BF
+ [ ] [FaceNet,DeepFace] facenet (GitHub: davidsandberg), https://github.com/davidsandberg/facenet
+ [ ] How to Develop a Face Recognition System Using FaceNet in Keras (Jason Brownlee, 2019), https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/
+ [ ] keras-facenet/notebook/tf_to_keras.ipynb (GitHub: nyoki-mtl), https://github.com/nyoki-mtl/keras-facenet/blob/master/notebook/tf_to_keras.ipynb
+ [ ] A Neural Algorithm of Artistic Style (Gatys, Ecker & Bethge, 2015), https://arxiv.org/abs/1508.06576
+ [ ] Convolutional neural networks for artistic style transfer, https://harishnarayanan.org/writing/artistic-style-transfer/
+ [ ] TensorFlow Implementation of "A Neural Algorithm of Artistic Style‚Äù, http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style
+ [ ] Very Deep Convolutional Networks For Large-Scale Image Recognition (Simonyan & Zisserman, 2015), https://arxiv.org/pdf/1409.1556.pdf
+ [ ] Pretrained models (MatConvNet), https://www.vlfeat.org/matconvnet/pretrained/
+ [ ] Dropout: A Simple Way to Prevent Neural Networks from Overfitting, https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
+ [ ] Dropout is NOT All You Need to Prevent Gradient Leakage, https://arxiv.org/abs/2208.06163
+ [ ] Dropout, https://paperswithcode.com/method/dropout
+ [ ] Modified Dropout for Training Neural Network, https://www.cs.cmu.edu/~epxing/Class/10715/project-reports/DuyckLeeLei.pdf
+ [x] Attention Is All You Need, https://arxiv.org/abs/1706.03762
+ [x] [CNN][ResNet] ResNet, Deep Residual Learning for Image Recognition, https://arxiv.org/abs/1512.03385, https://paperswithcode.com/method/resnet
+ [x] [CNN] A ConvNet for the 2020s, https://arxiv.org/abs/2201.03545, Codes: https://github.com/facebookresearch/ConvNeXt, https://huggingface.co/docs/transformers/main/model_doc/convnext, https://pytorch.org/vision/main/models/convnext.html, https://paperswithcode.com/paper/convnext-v2-co-designing-and-scaling-convnets
+ [x] [CNN][TextCNN] Convolutional Neural Networks for Sentence Classification, https://arxiv.org/abs/1408.5882

# [Papers to be for Reproduction]
+ [Roadmap] Deep Learning Papers Reading Roadmap, https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap

## 2023.Jul.04 - for Classical Convolution Neural Networks related Models
+ [CNN] Character-level Convolutional Networks for Text Classification, https://arxiv.org/abs/1509.01626
+ [CNN] A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification, http://arxiv.org/abs/1510.03820
+ [VGG] VGG Introduced by Simonyan et al. in Very Deep Convolutional Networks for Large-Scale Image Recognition, https://paperswithcode.com/method/vgg
+ [CNN][Book] Chapter 9 Convolutional Networks, https://www.deeplearningbook.org/contents/convnets.html
+ [CNN][Standford CS-230] Convolutional Neural Networks cheatsheet, https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#
+ [CNN][Standford] Welcome to the Deep Learning Tutorial! http://ufldl.stanford.edu/tutorial/
+ [CNN][Codes] https://github.com/kaiminghe
+ TART: A plug-and-play Transformer module for task-agnostic reasoning, https://arxiv.org/abs/2306.07536
+ Preference Ranking Optimization for Human Alignment, https://arxiv.org/abs/2306.17492
+ Restart Sampling for Improving Generative Processes, https://arxiv.org/abs/2306.14878
+ Bring Your Own Data! Self-Supervised Evaluation for Large Language Models, https://arxiv.org/abs/2306.13651
+ Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects, https://arxiv.org/abs/2306.10125
+ Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations, https://arxiv.org/abs/2306.09255
+ Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, https://arxiv.org/abs/1911.08265
+ LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering, https://arxiv.org/abs/2305.03403
+ Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation, https://arxiv.org/abs/2306.07954
+ Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow, https://arxiv.org/abs/2306.07209
+ Evaluating the Social Impact of Generative AI Systems in Systems and Society, https://arxiv.org/abs/2306.05949
+ Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models, https://arxiv.org/abs/2306.08997
+ DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection, https://arxiv.org/abs/2306.10347
+ Reinforcement Learning, An Introduction, 2nd Edition, Richard S. Sutton and Andrew G. Barto, http://incompleteideas.net/book/RLbook2020.pdf
+ Towards Integrative AI, Xuedong Huang, Microsoft Corporation, USA
