Top 100K Papers List
- Convention: [xxx] means belong to what category of AI fields
- Convention: <Reference> means it is a paper in the reference of the paper just under reproducing
- Priority: #1, SOTAs (state-of-the-art) Papers with Codes with SOTA performance first; 
- Priority: #2, Latest Papers (Hot Paper from Top Companies, Universities, Institutes) with Codes with SOTA performance; 
- Priority: #3, Latest Papers (Highly Cited Paper) with Codes with SOTA performance; 
- Priority: #4, Classics Papers with Codes with SOTA performance; 

# [Top 30 engineering blogs]
+ Get the 614 blogs + RSS feeds here, https://github.com/kilimchoi/engineering-blogs
+ Google Developers Blog, https://developers.googleblog.com
+ Microsoft Developer Blogs, https://devblogs.microsoft.com
+ Apple Machine Learning Journal, https://machinelearning.apple.com
+ Tesla Engineering Blog, https://www.tesla.com/blog
+ Nvidia Developer Blog, https://developer.nvidia.com/blog/
+ OpenAI tech blog, https://openai.com/blog
+ Facebook Engineering, https://engineering.fb.com
+ AWS Architecture Blog, https://aws.amazon.com/blogs/architecture/
+ Instagram Engineering, https://instagram-engineering.com/?gi=280eef5f24ac
+ LinkedIn Engineering, https://engineering.linkedin.com/blog
+ Pinterest Engineering Blog, https://medium.com/@Pinterest_Engineering
+ Slack Engineering, https://slack.engineering
+ Netflix Tech Blog:, https://netflixtechblog.com
+ Uber Engineering, https://www.uber.com/en-AE/blog/abu-dhabi/engineering/
+ Figma Engineering, https://www.figma.com/blog/engineering/
+ GitHub Engineering, https://githubengineering.com
+ Stack Overflow tech blog, https://stackoverflow.blog
+ Spotify Engineering, https://engineering.atspotify.com
+ Shopify Engineering, https://shopify.engineering
+ Airbnb Engineering & Data Science, https://medium.com/airbnb-engineering
+ Twitter Engineering, https://blog.twitter.com/engineering/en_us
+ Dropbox Tech Blog, https://dropbox.tech
+ Engineering at Quora, https://quoraengineering.quora.com
+ Square Corner Blog, https://developer.squareup.com/blog/
+ Palantir Blog, https://blog.palantir.com/?gi=0ada26c400d5
+ Yelp Engineering Blog, https://engineeringblog.yelp.com
+ Cloudflare Blog, https://blog.cloudflare.com
+ Stripe Engineering Blog, https://stripe.com/blog/engineering
+ Discord Engineering, https://discord.com/category/engineering
+ Notion Engineering, https://www.notion.so/blog


# [Courses & Videos]
+ [ ] [UK] Top 100 AI startups in UK, https://www.ai-startups.org/country/UK/

# Datasets
+ [Dataset] ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning, https://arxiv.org/abs/2311.03721

# [Physics]
+ [Computational Physics] [Chaotic Dynamics] [ML] AI-Lorenz: A physics-data-driven framework for black-box and gray-box identification of chaotic systems with symbolic regression, https://arxiv.org/abs/2312.14237, https://github.com/mariodeflorio/AI-Lorenz
+ [ ] The Feynman Lectures on Physics, Volume I, https://www.feynmanlectures.caltech.edu/I_toc.html
+ [ ] The Feynman Lectures on Physics, Volume II mainly electromagnetism and matter Feynman • Leighton • Sands, https://www.feynmanlectures.caltech.edu/II_toc.html
+ [ ] The Feynman Lectures on Physics, Volume III quantum mechanics Feynman • Leighton • Sands, https://www.feynmanlectures.caltech.edu/III_toc.html 

# [Players]
+ Ping Li, Distinguished Engineer, LinkedIn Ads, https://pltrees.github.io/

# [Courses & Videos]
+ [ ] [Stanford] Stanford CS229: Machine Learning Course, https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU
+ [ ] [Stanford] Stanford CS224W: Machine Learning with Graphs - YouTube, https://www.youtube.com/playlist?app=desktop&list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn
+ [ ] [Stanford] Statistical Learning Stanford Online, https://www.youtube.com/playlist?list=PLoROMvodv4rOzrYsAxzQyHb8n_RWNuS1e
+ [ ] [Stanford] Prompt Engineering" for effectively using generative ai llms, tools and platforms such as chatGPT, etc., by Aleksandar Popovic., https://www.youtube.com/playlist?list=PLYio3GBcDKsPP2_zuxEp8eCulgFjI5a3g
+ [ ] [Stanford] CS221 - Artificial Intelligence: Principles and Techniques by Percy Liang and Dorsa Sadigh, https://www.youtube.com/playlist?list=PLoROMvodv4rOca_Ovz1DvdtWuz8BfSWL2
+ [ ] [Stanford] CS229 - Machine Learning by Andrew Ng, https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU
+ [ ] [Stanford] CS230 - Deep Learning by Andrew Ng, https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb
+ [ ] [Stanford] CS231n - Convolutional Neural Networks for Visual Recognition by Fei-Fei Li and Andrej Karpathy, https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv
+ [ ] [Stanford] CS224n - Natural Language Processing with Deep Learning by Christopher Manning, https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ
+ [ ] [Stanford] CS224w - Machine Learning with Graphs by Jure Leskovec, https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn
+ [ ] [Stanford] CS234 - Reinforcement Learning by Emma Brunskill, https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u
+ [ ] [Stanford] CS330 - Deep Multi-task and Meta Learning by Chelsea Finn, https://www.youtube.com/playlist?list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI
+ [ ] [Stanford] CS25 - Transformers United by Divyansh Garg, Steven Feng, and Rylan Schaeffer, https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM
+ [ ] [Databricks][LLM] Large Language Models: Application through Production, https://www.youtube.com/playlist?list=PLTPXxbhUt-YWSR8wtILixhZLF9qB_1yZm
+ [ ] [University College London (UCL)] COMP M050 Reinforcement Learning by David Silver:, https://www.youtube.com/watch?v=2pWv7GOvuf0
+ [ ] [MIT] 6.S191 - Introduction to Deep Learning by Alexander Amini and Ava Amini:, https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI
+ [ ] [MIT] 6.S094 - Deep Learning by Lex Fridman, https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf
+ [ ] [MIT] 6.S192 - Deep Learning for Art, Aesthetics, and Creativity by Ali Jahanian, https://www.youtube.com/playlist?list=PLCpMvp7ftsnIbNwRnQJbDNRqO6qiN3EyH
+ [ ] [CMU] CS/LTI 11-711: Advanced NLP by Graham Neubig, https://www.youtube.com/playlist?list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK70z
+ [ ] [CMU] CS/LTI 11-747: Neural Networks for NLP by Graham Neubig, https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV
+ [ ] [CMU] CS/LTI 11-737: Multilingual NLP by Graham Neubig, https://www.youtube.com/playlist?list=PL8PYTP1V4I8BhCpzfdKKdd1OnTfLcyZr7
+ [ ] [CMU] CS/LTI 11-777: Multimodal Machine Learning by Louis-Philippe Morency, https://www.youtube.com/channel/UCqlHIJTGYhiwQpNuPU5e2gg/videos
+ [ ] [CMU] CS/LTI 11-785: Introduction to Deep Learning by Bhiksha Raj and Rita Singh, https://www.youtube.com/channel/UC8hYZGEkI2dDO8scT8C5UQA/playlists
+ [ ] [CMU] CS/LTI Low Resource NLP Bootcamp 2020 by Graham Neubig, https://www.youtube.com/playlist?list=PL8PYTP1V4I8A1CpCzURXAUa6H4HO7PF2c
+ [ ] [The University of Texas at Austin] CS388: Natural Language Processing (online MS version), https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html, https://www.youtube.com/playlist?list=PLofp2YXfp7TZZ5c7HEChs0_wfEfewLDs7

# [Mathematics]
+ [Algebra] [Linear Algebra] Linear Algebra Done Right Sheldon Axler, https://linear.axler.net
+ [Graph Theory] An introduction to graph theory, https://arxiv.org/abs/2308.04512
+ [Statistics] Applying statistical learning theory to deep learning, https://arxiv.org/abs/2311.15404

# [Milestone of Paper Reproduction for Jul.01-Jul.30, 2023]
+ [ ] [AI] [Survey] Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks,https://arxiv.org/abs/2311.09247
+ [ ] [AI] [Survey] Latest Trends in Artificial Intelligence Technology: A Scoping Review, https://arxiv.org/abs/2305.04532
+ [ ] [AI] [HCI] Generative Agents: Interactive Simulacra of Human Behavior, https://arxiv.org/abs/2304.03442, https://github.com/joonspk-research/generative_agents
+ [ ] [AI] Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals and Industrial Pragmatism, https://arxiv.org/abs/2403.01832
+ [ ] [AI] Consciousness in Artificial Intelligence: Insights from the Science of Consciousness, https://arxiv.org/abs/2308.08708
+ [ ] [CV][In-context learning (ICL)] Med-Flamingo: a Multimodal Medical Few-shot Learner, https://arxiv.org/abs/2307.15189, https://huggingface.co/med-flamingo/med-flamingo, https://github.com/snap-stanford/med-flamingo, https://www.marktechpost.com/2023/08/02/meet-med-flamingo-a-unique-foundation-model-which-is-capable-of-performing-multimodal-in-context-learning-specialized-for-the-medical-domain/
+ [ ] [Autonomous Driving] SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving, https://arxiv.org/abs/2303.09551, https://github.com/weiyithu/SurroundOcc
+ [ ] https://index.quantumstat.com
+ [ ] [Robotics] VIMA: General Robot Manipulation with Multimodal Prompts, https://arxiv.org/abs/2210.03094
+ [ ] [Robotics] RoboAgent Towards Sample Efficient Robot Manipulation with Semantic Augmentations and Action Chunking, https://robopen.github.io/media/roboagent.pdf, https://github.com/robopen/roboagent/, https://robopen.github.io/roboset/
+ [ ] [Difussion Model] DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models， https://arxiv.org/abs/2312.09767
+ [ ] [Difussion Model] Erasing Concepts from Diffusion Models, https://arxiv.org/abs/2303.07345, https://civitai.com/models
+ [ ] [Foundation Model] [Robotics] Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis, https://arxiv.org/abs/2312.08782
+ [ ] [Foundation Model] Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks, https://arxiv.org/abs/2311.06242
+ [ ] [Foundation Model] NASA and IBM Openly Release Geospatial AI Foundation Model for NASA Earth Observation Data, https://www.earthdata.nasa.gov/news/impact-ibm-hls-foundation-model, https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M
+ [ ] [Foundation Model] [Causal NLP] Towards Causal Foundation Model: on Duality between Causal Inference and Attention, https://arxiv.org/abs/2310.00809
+ [ ] [LLM] [Serving] [RAG] The Power of Noise: Redefining Retrieval for RAG Systems, https://arxiv.org/abs/2401.14887
+ [ ] [LLM] [Serving] [RAG] RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture, https://arxiv.org/abs/2401.08406
+ [ ] [LLM] [Serving] [Inferencing] [Intel] Efficient LLM inference solution on Intel GPU, https://huggingface.co/papers/2401.05391
+ [ ] [LLM] [Serving] http://devv.ai
+ [ ] [LLM] [Serving] FireAttention — Serving Open Source Models 4x faster than vLLM by quantizing with ~no tradeoffs, https://blog.fireworks.ai/fireattention-serving-open-source-models-4x-faster-than-vllm-by-quantizing-with-no-tradeoffs-a29a85ad28d0
+ [ ] [LLM] [Datasets] Datasets for Large Language Models: A Comprehensive Survey, https://arxiv.org/abs/2402.18041
+ [ ] [LLM] A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization, https://arxiv.org/abs/2403.01152
+ [ ] [LLM] Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models, https://arxiv.org/abs/2402.17177
+ [ ] [LLM] Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon, https://arxiv.org/abs/2401.03462
+ [ ] [LLM] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, https://huggingface.co/papers/2402.17764
+ [ ] [LLM] TOOLVERIFIER: Generalization to New Tools via Self-Verification, https://arxiv.org/abs/2402.14158
+ [ ] [LLM] Reflexion: Language Agents with Verbal Reinforcement Learning, https://arxiv.org/abs/2303.11366
+ [ ] [LLM] An Empirical Evaluation of LLMs for Solving Offensive Security Challenges, https://arxiv.org/abs/2402.11814
+ [ ] [LLM] [Hallucination] A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models
+ [ ] [LLM] [Chatbot] LMSYS Chatbot Arena Leaderboard, https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
+ [ ] [LLM] [AI] Thousands of AI Authors on the Future of AI, https://arxiv.org/abs/2401.02843
+ [ ] [LLM] [Mathematics] OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset, https://huggingface.co/papers/2402.10176
+ [ ] [LLM] [GPT] AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling, https://arxiv.org/abs/2402.12226
+ [ ] [LLM] [Video] [Google] VideoPrism: A Foundational Visual Encoder for Video Understanding, https://huggingface.co/papers/2402.13217
+ [ ] [LLM] Cascade Speculative Drafting for Even Faster LLM Inference, https://arxiv.org/abs/2312.11462
+ [ ] [LLM] Speculative Streaming: Fast LLM Inference without Auxiliary Models, https://arxiv.org/abs/2402.11131
+ [ ] [LLM] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models, https://arxiv.org/abs/2401.01335
+ [ ] [LLM] Rethinking Machine Unlearning for Large Language Models, https://arxiv.org/abs/2402.08787
+ [ ] [LLM] Secrets of RLHF in Large Language Models Part II: Reward Modeling, https://arxiv.org/abs/2401.06080
+ [ ] [LLM] TrustLLM: Trustworthiness in Large Language Models, https://arxiv.org/abs/2401.05561
+ [ ] [LLM] Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM, https://arxiv.org/abs/2401.02994
+ [ ] [LLM] Direct Preference Optimization: Your Language Model is Secretly a Reward Model, https://arxiv.org/abs/2305.18290
+ [ ] [LLM] Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models, https://huggingface.co/papers/2401.01335, https://huggingface.co/UCLA-AGI
+ [ ] [LLM] Direct Preference Optimization: Your Language Model is Secretly a Reward Model, https://arxiv.org/abs/2305.18290
+ [ ] [LLM] TinyLlama: An Open-Source Small Language Model, https://arxiv.org/abs/2401.02385
+ [ ] [LLM] [Book] Build a Large Language Model (From Scratch), https://www.manning.com/books/build-a-large-language-model-from-scratch
+ [ ] [LLM] [Microsoft] Improving Text Embeddings with Large Language Models, https://arxiv.org/abs/2401.00368
+ [ ] [LLM] LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models, https://huggingface.co/papers/2310.08659, https://github.com/yxli2123/LoftQ
+ [ ] [LLM] Retrieval-Augmented Generation for Large Language Models: A Survey, https://arxiv.org/abs/2312.10997
+ [ ] [LLM] If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents, https://arxiv.org/abs/2401.00812
+ [ ] [LLM] TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones, https://arxiv.org/abs/2312.16862
+ [ ] [LLM] Excellent roadmap to learn about the LLM end to end workflow, https://github.com/mlabonne/llm-course
+ [ ] [LLM] Mindstorms in Natural Language-Based Societies of Mind, https://arxiv.org/abs/2305.17066
+ [ ] [LLM] SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling, https://arxiv.org/abs/2312.15166
+ [ ] [LLM] An LLM Compiler for Parallel Function Calling, https://arxiv.org/abs/2312.04511, https://github.com/SqueezeAILab/LLMCompiler
+ [ ] [LLM] Using sequences of life-events to predict human lives, https://www.nature.com/articles/s43588-023-00573-5.pdf
+ [ ] [LLM] SMPL A Skinned Multi-Person Linear Model, https://smpl.is.tue.mpg.de
+ [ ] [LLM] HUGS: Human Gaussian Splats, https://arxiv.org/abs/2311.17910
+ [ ] [LLM] Apple quietly released an open source multimodal LLM in October, https://venturebeat.com/ai/apple-quietly-released-an-open-source-multimodal-llm-in-october/
+ [ ] [LLM] VideoPoet A large language model for zero-shot video generation, https://sites.research.google/videopoet/
+ [ ] [LLM] Mini-GPTs: Efficient Large Language Models through Contextual Pruning, https://arxiv.org/abs/2312.12682
+ [ ] [LLM] Improving Automatic VQA Evaluation Using Large Language Models, https://arxiv.org/abs/2310.02567
+ [ ] [LLM] Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos, https://arxiv.org/abs/2312.04746, https://quilt-llava.github.io
+ [ ] [LLM] Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation, https://arxiv.org/abs/2305.01210
+ [ ] [LLM] NeurIPS 2023 Reading List (WIP), https://pretty-sodium-5e0.notion.site/NeurIPS-2023-Reading-List-WIP-99a05177288f4dc98ee25291b3168b83
+ [ ] [LLM] Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI), https://arxiv.org/abs/2312.10904
+ [ ] [LLM] LLaMA-Factory, https://github.com/hiyouga/LLaMA-Factory
+ [ ] [LLM] LongNet: Scaling Transformers to 1,000,000,000 Tokens, https://arxiv.org/abs/2307.02486
+ [ ] [LLM] CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare, https://arxiv.org/abs/2312.11541
+ [ ] [LLM] LLM in a flash: Efficient Large Language Model Inference with Limited Memory, https://huggingface.co/papers/2312.11514
+ [ ] [LLM] An In-depth Look at Gemini's Language Abilities, https://arxiv.org/abs/2312.11444
+ [ ] [LLM] Mathematical discoveries from program search with large language models, https://www.nature.com/articles/s41586-023-06924-6
+ [ ] [LLM] Magicoder: Source Code Is All You Need, https://arxiv.org/abs/2312.02120, https://github.com/ise-uiuc/magicoder
+ [ ] [LLM] Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling, https://arxiv.org/abs/2304.01373
+ [ ] [LLM] TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties, https://arxiv.org/abs/2308.03051
+ [ ] [LLM] Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models, https://arxiv.org/abs/2312.06585
+ [ ] [LLM] Large Language Models for Mathematicians, https://arxiv.org/abs/2312.04556
+ [ ] [LLM] Mistral 7B, https://arxiv.org/abs/2310.06825, https://mistral.ai/product/
+ [ ] [LLM] Large Language Models on Graphs: A Comprehensive Survey, https://arxiv.org/abs/2312.02783
+ [ ] [LLM] Chain of Code: Reasoning with a Language Model-Augmented Code Emulator, https://arxiv.org/abs/2312.04474
+ [ ] [LLM] Calibrated Language Models Must Hallucinate, https://arxiv.org/abs/2311.14648
+ [ ] [LLM] Language Models: A Guide for the Perplexed, https://arxiv.org/abs/2311.17301
+ [ ] [LLM] CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation, https://huggingface.co/papers/2311.18775
+ [ ] [LLM] [Algorithm] The Efficiency Spectrum of Large Language Models: An Algorithmic Survey, https://arxiv.org/abs/2312.00678
+ [ ] [LLM] [Data Management] Data Management For Large Language Models: A Survey, https://arxiv.org/abs/2312.01700
+ [ ] [LLM] [OpenAI] [Mathematical Reasoning] Let's Verify Step by Step, https://arxiv.org/abs/2305.20050
+ [ ] [LLM] [Alignment] A General Theoretical Paradigm to Understand Learning from Human Preferences, https://arxiv.org/abs/2310.12036
+ [ ] [LLM] [Causal NLP] CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models, https://arxiv.org/abs/2312.04350, https://github.com/causalNLP/cladder
+ [ ] [LLM] [Causal NLP] Understanding Causality with Large Language Models: Feasibility and Opportunities, https://arxiv.org/pdf/2304.05524.pdf
+ [ ] [LLM] [Causal NLP] Causal Parrots: Large Language Models May Talk Causality But Are Not Causal, https://openreview.net/pdf?id=tv46tCzs83
+ [ ] [LLM] [Causal NLP] Causal Reasoning and Large Language Models: Opening a New Frontier for Causality, https://arxiv.org/pdf/2305.00050.pdf
+ [ ] [LLM] [Causal NLP] Passive learning of active causal strategies in agents and language models, https://arxiv.org/pdf/2305.16183.pdf
+ [ ] [LLM] [Causal NLP] Can Large Language Models Infer Causation from Correlation? https://arxiv.org/abs/2306.05836
+ [ ] [LLM] [Multimodal] Guiding Instruction-based Image Editing via Multimodal Large Language Models, https://arxiv.org/abs/2309.17102
+ [ ] [LLM] [Multimodal] Multimodal Foundation Models: From Specialists to General-Purpose Assistants, https://arxiv.org/abs/2309.10020
+ [ ] [LLM] [Compression] PB-LLM: Partially Binarized Large Language Models, https://arxiv.org/abs/2310.00034, https://github.com/hahnyuan/PB-LLM
+ [ ] [LLM] [Compression] A Survey on Model Compression for Large Language Models, https://arxiv.org/abs/2308.07633
+ [ ] [LLM] [Prompting] PromptBench: A Unified Library for Evaluation of Large Language Models, https://arxiv.org/abs/2312.07910, https://github.com/microsoft/promptbench
+ [ ] [LLM] [Prompting] Prompt engineering, https://platform.openai.com/docs/guides/prompt-engineering
+ [ ] [LLM] [Prompting] LLM prompting guide, https://huggingface.co/docs/transformers/main/en/tasks/prompting
+ [ ] [LLM] [Reasoning] Orca 2: Teaching Small Language Models How to Reason, https://arxiv.org/abs/2311.11045
+ [ ] [LLM] [Reasoning] The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction, https://arxiv.org/abs/2312.13558
+ [ ] [LLM] [Search] Enhancing the Ranking Context of Dense Retrieval Methods through Reciprocal Nearest Neighbors, https://arxiv.org/abs/2305.15720
+ [ ] [LLM] ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up? https://arxiv.org/abs/2311.16989
+ [ ] [LLM] GPT is becoming a Turing machine: Here are some ways to program it, https://arxiv.org/abs/2303.14310
+ [ ] [LLM] Sparse Fine-tuning for Inference Acceleration of Large Language Models, https://arxiv.org/abs/2310.06927
+ [ ] [LLM] GENEPT: A SIMPLE BUT HARD-TO-BEAT FOUNDATION MODEL FOR GENES AND CELLS BUILT FROM CHATGPT, https://www.biorxiv.org/content/10.1101/2023.10.16.562533v1.full.pdf,%20https://github.com/yiqunchen/GenePT
+ [ ] [LLM] Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook, https://arxiv.org/abs/2310.10196, https://github.com/qingsongedu/Awesome-TimeSeries-SpatioTemporal-LM-LLM
+ [ ] [LLM] A taxonomy and review of generalization research in NLP, https://www.nature.com/articles/s42256-023-00729-y
+ [ ] [LLM] DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines, https://github.com/stanfordnlp/dspy
+ [ ] [LLM] Large Language Models Are Zero-Shot Time Series Forecasters, https://arxiv.org/abs/2310.07820
+ [ ] [LLM] Voyager: An Open-Ended Embodied Agent with Large Language Models, https://arxiv.org/abs/2305.16291, https://github.com/MineDojo/Voyager, https://github.com/MineDojo
+ [ ] [LLM] Communicative Agents for Software Development, https://arxiv.org/abs/2307.07924
+ [ ] [LLM] LLM Inference Performance Engineering: Best Practices, https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices
+ [ ] [LLM] Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models, https://arxiv.org/abs/2310.06692, https://github.com/Anni-Zou/Meta-CoT
+ [ ] [LLM] Think before you speak: Training Language Models With Pause Tokens, https://arxiv.org/abs/2310.02226
+ [ ] [LLM] A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics, https://arxiv.org/abs/2310.05694
+ [ ] [LLM] Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution, https://arxiv.org/abs/2309.16797
+ [ ] [LLM] Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models, https://arxiv.org/abs/2310.04406
+ [ ] [LLM] Retentive Network: A Successor to Transformer for Large Language Models, https://browse.arxiv.org/abs/2307.08621
+ [ ] [LLM] Train and Deploy Mistral 7B with Hugging Face on Amazon SageMaker, https://www.philschmid.de/sagemaker-mistral
+ [ ] [LLM] Physics of Language Models: Part 1, Context-Free Grammar, https://arxiv.org/abs/2305.13673
+ [ ] [LLM] Training and inference of large language models using 8-bit floating point, https://arxiv.org/abs/2309.17224
+ [ ] [LLM] A Path Towards Autonomous Machine Intelligence, https://openreview.net/pdf?id=BZ5a1r-kVsf, https://worldmodels.github.io, https://openai.com/research/unsupervised-sentiment-neuron, 
+ [ ] [LLM] Language Models Represent Space and Time, https://arxiv.org/abs/2310.02207, https://github.com/wesg52/world-models
+ [ ] [LLM] QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models, https://arxiv.org/abs/2309.14717
+ [ ] [LLM] The Internal State of an LLM Knows When its Lying, https://arxiv.org/abs/2304.13734
+ [ ] [LLM] A Hackers' Guide to Language Models, https://www.youtube.com/watch?v=jkrNMKz9pWU
+ [ ] [LLM] Adapting Large Language Models via Reading Comprehension, https://arxiv.org/abs/2309.09530, https://github.com/microsoft/LMOps
+ [ ] [LLM] Large Language Models are not Fair Evaluators, https://arxiv.org/abs/2305.17926, https://github.com/i-Eval/FairEval
+ [ ] [LLM] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A", https://arxiv.org/abs/2309.12288
+ [ ] [LLM] Graphologue: Exploring Large Language Model Responses with Interactive Diagrams, https://arxiv.org/abs/2305.11473, https://arxiv.org/abs/2305.11473, https://graphologue.app
+ [ ] [LLM] PDFTriage: Question Answering over Long, Structured Documents, https://arxiv.org/abs/2309.08872
+ [ ] [LLM] Language Modeling Is Compression, https://arxiv.org/abs/2309.10668
+ [ ] [LLM] Compositional Foundation Models for Hierarchical Planning, https://arxiv.org/abs/2309.08587, https://hierarchical-planning-foundation-model.github.io
+ [ ] [LLM] Explaining grokking through circuit efficiency, https://arxiv.org/abs/2309.02390Explaining grokking through circuit efficiency, https://arxiv.org/abs/2309.02390
+ [ ] [LLM] DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models, https://arxiv.org/abs/2309.03883, https://github.com/voidism/DoLa
+ [ ] [LLM] Efficient Memory Management for Large Language Model Serving with PagedAttention, https://arxiv.org/abs/2309.06180, https://github.com/vllm-project/vllm
+ [ ] [LLM] Nougat: Neural Optical Understanding for Academic Documents, https://arxiv.org/abs/2308.13418
+ [ ] [LLM] NExT-GPT: Any-to-Any Multimodal LLM, https://arxiv.org/abs/2309.05519
+ [ ] [LLM] Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions, https://arxiv.org/abs/2308.11483
+ [ ] [LLM] Open challenges in LLM research, https://huyenchip.com/2023/08/16/llm-research-open-challenges.html
+ [ ] [LLM] A Survey of Hallucination in Large Foundation Models, https://arxiv.org/abs/2309.05922
+ [ ] [LLM] XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models, https://arxiv.org/abs/2308.01263, https://github.com/paul-rottger/exaggerated-safety
+ [ ] [LLM] A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need? https://arxiv.org/abs/2303.11717
+ [ ] [LLM] TabLLM: Few-shot Classification of Tabular Data with Large Language Models, https://arxiv.org/abs/2210.10723
+ [ ] [LLM] Spread Your Wings: Falcon 180B is here, https://huggingface.co/spaces/tiiuae/falcon-180b-demo, https://huggingface.co/models?sort=trending&search=Falcon-180b, https://huggingface.co/blog/falcon-180b
+ [ ] [LLM] Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Modelsm, https://arxiv.org/abs/2308.16149, https://huggingface.co/inception-mbzuai/jais-13b-chatm, https://huggingface.co/inception-mbzuai/jais-13b-chat, https://www.inceptioniai.org/jais/docs/DeveloperManual.pdf, https://www.inceptioniai.org/jais/docs/Whitepaper.pdf
+ [ ] [LLM] GPT Engineer, https://github.com/AntonOsika/gpt-engineer
+ [ ] [LLM] Fast Model Editing at Scale,, https://arxiv.org/abs/2110.11309
+ [ ] [LLM] Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs, https://arxiv.org/abs/2307.14988
+ [ ] [LLM] Locating and Editing Factual Associations in GPT, https://arxiv.org/abs/2202.05262
+ [ ] [LLM] [Transformer] Mass-Editing Memory in a Transformer, https://arxiv.org/abs/2210.07229
+ [ ] [LLM] [Transformer] PMET: Precise Model Editing in a Transformer, https://arxiv.org/abs/2308.08742
+ [ ] [LLM] Evaluating the Ripple Effects of Knowledge Editing in Language Models, https://arxiv.org/abs/2307.12976
+ [ ] [LLM] Prompt2Model: Generating Deployable Models from Natural Language Instructions, https://arxiv.org/abs/2308.12261, https://github.com/neulab/prompt2model
+ [ ] [LLM] UniversalNER A case study on targeted distillation from LLMs, https://universal-ner.github.io
+ [ ] [LLM] Language to Rewards for Robotic Skill Synthesis, https://arxiv.org/abs/2306.08647
+ [ ] [LLM] LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models, https://arxiv.org/abs/2308.11462
+ [ ] [LLM] Prompt2Model: Generating Deployable Models from Natural Language Instructions, https://arxiv.org/abs/2308.12261
+ [ ] [LLM] A Survey on Large Language Model based Autonomous Agents, https://arxiv.org/abs/2308.11432
+ [ ] [LLM] IT3D: Improved Text-to-3D Generation with Explicit View Synthesis, https://arxiv.org/abs/2308.11473
+ [ ] [LLM] Giraffe: Adventures in Expanding Context Lengths in LLMs, https://arxiv.org/abs/2308.10882
+ [ ] [LLM] Instruction Tuning for Large Language Models: A Survey, https://arxiv.org/abs/2308.10792
+ [ ] [LLM] Anti-hype LLM reading list, https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e
+ [ ] [LLM] Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs? https://arxiv.org/abs/2308.10168
+ [ ] [LLM] ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs, https://arxiv.org/abs/2307.16789, https://github.com/openbmb/toolbench
+ [ ] [LLM] Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities, https://arxiv.org/abs/2308.12833
+ [ ] [LLM] Code Llama: Open Foundation Models for Code, https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/, https://ai.meta.com/blog/code-llama-large-language-model-coding
+ [ ] [LLM] Graph of Thoughts: Solving Elaborate Problems with Large Language Models, https://arxiv.org/abs/2308.09687, https://github.com/spcl/graph-of-thoughts
+ [ ] [LLM] Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation, https://arxiv.org/abs/2305.15011
+ [ ] [LLM] LangChain for LLM Application Development, https://learn.deeplearning.ai/langchain/lesson/2/models,-prompts-and-parsers
+ [ ] [LLM] Open challenges in LLM research, https://huyenchip.com/2023/08/16/llm-research-open-challenges.html
+ [ ] [LLM] Platypus: Quick, Cheap, and Powerful Refinement of LLMs, https://arxiv.org/abs/2308.07317
+ [ ] [LLM] OctoPack: Instruction Tuning Code Large Language Models, https://arxiv.org/abs/2308.07124
+ [ ] [LLM] Open LLMs These LLMs (Large Language Models) are all licensed for commercial use (e.g., Apache 2.0, MIT, OpenRAIL-M). Contributions welcome! https://github.com/eugeneyan/open-llms?utm_source=substack&utm_medium=email
+ [ ] [LLM] Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies, https://arxiv.org/abs/2308.03188
+ [ ] [LLM] [Low-rank adaptation (LoRA)] Stack More Layers Differently: High-Rank Training Through Low-Rank Updates, https://arxiv.org/abs/0002307.05695
+ [ ] [LLM] LLM As DBA, https://arxiv.org/abs/2308.05481
+ [ ] [LLM] AgentBench: Evaluating LLMs as Agents, https://arxiv.org/abs/2308.03688
+ [ ] [LLM] LLongMA 2: A Llama-2 8k model,https://www.reddit.com/r/LocalLLaMA/comments/154us99/llongma_2_a_llama2_8k_model/?rdt=41147
+ [ ] [LLM] Inference of LLaMA model in pure C/C++, https://github.com/ggerganov/llama.cpp
+ [ ] [LLM] Purple Llama, https://github.com/facebookresearch/PurpleLlama, https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/
+ [ ] [LLM] 𝗕𝗮𝗯𝘆 𝗟𝗹𝗮𝗺𝗮 𝟮, llama2.c, https://github.com/karpathy/llama2.c
+ [ ] [LLM] http://Llama2.rs (https://github.com/srush/llama2.rs) A one-file Rust Llama2.c
+ [ ] [LLM] Quantized models from 𝗧𝗵𝗲𝗕𝗹𝗼𝗸𝗲, https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ
+ [ ] [LLM] Responsible-Use-Guide.pdf, https://github.com/facebookresearch/llama/blob/main/Responsible-Use-Guide.pdf
+ [ ] [LLM] [Meta] Llama 2 Fine-tuning / Inference Recipes and Examples, https://github.com/facebookresearch/llama-recipes
+ [ ] [LLM] [𝗛𝘂𝗴𝗴𝗶𝗻𝗴𝗙𝗮𝗰𝗲], Llama 2 is here - get it on Hugging Face, https://huggingface.co/blog/llama2
+ [ ] [LLM] [OPpenLLM] What's going on with the Open LLM Leaderboard?, https://huggingface.co/blog/evaluating-mmlu-leaderboard
+ [ ] [LLM] [Intel] Accelerate Llama 2 with Intel AI Hardware and Software Optimizations, https://www.intel.com/content/www/us/en/developer/articles/news/llama2.html
+ [ ] [LLM] [Qualicomm] Qualcomm Works with Meta to Enable On-device AI Applications Using Llama 2, https://www.qualcomm.com/news/releases/2023/07/qualcomm-works-with-meta-to-enable-on-device-ai-applications-usi
+ [ ] [LLM] [Google] The Path to Achieve Ultra-Low Inference Latency With LLaMA 65B on PyTorch/XLA, https://pytorch.org/blog/path-achieve-low-inference-latency/
+ [ ] [LLM] [Amazon] Llama 2 foundation models from Meta are now available in Amazon SageMaker JumpStart, https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/
+ [ ] [LLM] [Microsoft] Microsoft and Meta expand their AI partnership with Llama 2 on Azure and Windows, https://blogs.microsoft.com/blog/2023/07/18/microsoft-and-meta-expand-their-ai-partnership-with-llama-2-on-azure-and-windows/
+ [ ] [LLM] [Microsoft] Llama 2 Powered By ONNX, https://github.com/microsoft/Llama-2-Onnx
+ [ ] [LLM] [CV] [Health] ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders, https://arxiv.org/abs/2308.01317
+ [ ] [LLM] When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities, https://arxiv.org/abs/2307.16376
+ [ ] [LLM] [CV] 3D-LLM: Injecting the 3D World into Large Language Models, https://arxiv.org/abs/2307.12981, https://github.com/UMass-Foundation-Model/3D-LLM, https://vis-www.cs.umass.edu/3dllm/
+ [ ] [LLM] [RL] RLCD: Reinforcement Learning from Contrast Distillation for Language Model Alignment, https://arxiv.org/abs/2307.12950
+ [ ] [LLM] How is ChatGPT's behavior changing over time?, https://arxiv.org/abs/2307.09009
+ [ ] [LLM] Challenges and Applications of Large Language Models, https://arxiv.org/abs/2307.10169
+ [ ] [LLM][Meta] Llama 2: Open Foundation and Fine-Tuned Chat Models, https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/, https://huggingface.co/meta-llama,
+ [ ] [LLM][Meta] LLaMA: Open and Efficient Foundation Language Models, https://arxiv.org/abs/2302.13971
+ [ ] [LLM][Evaluation] AGIEval This repository contains information about AGIEval, data, code and output of baseline systems for the benchmark, https://github.com/microsoft/agieval
+ [ ] [LLM][LangChain] ReAct: Synergizing Reasoning and Acting in Language Models, https://arxiv.org/abs/2210.03629, The Problem With LangChain LangChain is complicated, so it must be better. Right? https://minimaxir.com/2023/07/langchain-problem/
+ [ ] [LLM][In-context learning (ICL)] Schema-learning and rebinding as mechanisms of in-context learning and emergence, https://arxiv.org/abs/2307.01201
+ [ ] [LLM][In-context learning (ICL)] A Survey on In-context Learning, https://arxiv.org/abs/2301.00234, https://github.com/dqxiu/icl_paperlist, https://www.connectedpapers.com/main/8aa98fbfb6f1e979dead13ce24075503fe47658e/A-Survey-for-In%20context-Learning/graph
+ [ ] [LLM][Meta] Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning, https://ai.meta.com/research/publications/scaling-autoregressive-multi-modal-models-pretraining-and-instruction-tuning/
+ [ ] [LLM][GPT-3] Language Models are Few-Shot Learners, https://arxiv.org/abs/2005.14165
+ [ ] [LLM] OpenAI Cookbook, https://github.com/openai/openai-cookbook
+ [ ] [LLM] Coding framework LlamaIndex enables data interaction with LLMs, https://github.com/jerryjliu/llama_index
+ [ ] [LLM] Multimodal Chain-of-Thought Reasoning in Language Models, https://arxiv.org/abs/2302.00923, https://github.com/amazon-science/mm-cot
+ [ ] [LLM][Survey] A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization, https://arxiv.org/abs/2403.01152
+ [ ] [LLM][Survey] Large Language Models: A Survey, https://arxiv.org/abs/2402.06196
+ [ ] [LLM][Survey] Understanding LLMs: A Comprehensive Overview from Training to Inference, https://arxiv.org/abs/2401.02038
+ [ ] [LLM][Survey] A Survey of Large Language Models, https://arxiv.org/abs/2303.18223, https://github.com/RUCAIBox/LLMSurvey
+ [ ] [LLM][Survey] Papers and resources for LLMs evaluation, https://github.com/MLGroupJLU/LLM-eval-survey
+ [ ] [LLM] DoRA: Weight-Decomposed Low-Rank Adaptation, https://arxiv.org/abs/2402.09353
+ [ ] [LLM] LoRA: Low-Rank Adaptation of Large Language Models, https://arxiv.org/abs/2106.09685
+ [ ] [LLM] Lost in the Middle: How Language Models Use Long Contexts, https://arxiv.org/abs/2307.03172
+ [ ] [LLM] Retrieval meets Long Context Large Language Models, https://arxiv.org/abs/2310.03025
+ [ ] [LLM] LLM Calibration and Automatic Hallucination Detection via Pareto Optimal Self-supervision, https://arxiv.org/abs/2306.16564
+ [ ] [Diffusion Models] EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions, https://huggingface.co/papers/2402.17485
+ [ ] [Diffusion Models] Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models, https://arxiv.org/abs/2311.12092
+ [ ] [Diffusion Models] ART⋅V: Auto-Regressive Text-to-Video Generation with Diffusion Models, https://arxiv.org/abs/2311.18834
+ [ ] [Hands-on]Setting Python Development Environment with VScode and Docker, https://github.com/RamiKrispin/vscode-python
+ [ ] Topological Deep Learning: Going Beyond Graph Data, https://arxiv.org/abs/2206.00606
+ [ ] Google’s BERT
+ [ ] OpenAI’s GPT-2
+ [ ] Provably Faster Gradient Descent via Long Steps, https://arxiv.org/abs/2307.06324
+ [ ] AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models		1331	DeepMind, European Molecular Biology Laboratory	UK	academia 
+ [ ] ColabFold: making protein folding accessible to all		1138	Harvard University, Max Planck Institute for Multidisciplinary Sciences, Michigan State University, Seoul National University, University of Tokyo	Germany, Japan, South Korea, USA	academia 
+ [ ] A ConvNet for the 2020s	857	835	Meta, UC Berkeley	USA	industry 
+ [ ] Hierarchical Text-Conditional Image Generation with CLIP Latents	105	718	OpenAI	USA	industry 
+ [ ] PaLM: Scaling Language Modeling with Pathways	445	426	Google	USA	industry 
+ [ ] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding	2462	390	Google	USA	industry 
+ [ ] Instant Neural Graphics Primitives with a Multiresolution Hash Encoding	11	342	NVIDIA	USA	industry 
+ [ ] SignalP 6.0 predicts all five types of signal peptides using protein language models		274	Copenhagen University Hospital, ETH Zurich, Stanford University, Stockholm University, Technical University of Denmark, University of Copenhagen, Wellcome Genome Campus	Denmark, Sweden, Switzerland, UK, USA	academia 
+ [ ] Swin Transformer V2: Scaling Up Capacity and Resolution	87	266	Huazhong University of Science and Technology, Microsoft, Tsinghua University, University of Science and Technology of China, Xi’an Jiaotong University	China, USA	industry 
+ [ ] Training language models to follow instructions with human feedback	448	254	OpenAI	USA	industry 
+ [ ] Chain of Thought Prompting Elicits Reasoning in Large Language Models	378	224	Google	USA	industry 
+ [ ] Flamingo: a Visual Language Model for Few-Shot Learning	71	218	DeepMind	UK	industry 
+ [ ] Classifier-Free Diffusion Guidance	53	194	Google	Netherlands, USA	industry 
+ [ ] Magnetic control of tokamak plasmas through deep reinforcement learning		194	DeepMind, École Polytechnique Fédérale de Lausanne, Meta	Switzerland, UK, USA	industry 
+ [ ] data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language		191	Meta	USA	industry 
+ [ ] OPT: Open Pre-trained Transformer Language Models	812	187	Meta	USA	industry 
+ [ ] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation	79	184	Salesforce	USA	industry 
+ [ ] A Generalist Agent	231	180	DeepMind	UK	industry 
+ [ ] LaMDA: Language Models for Dialog Applications	473	180	Google	USA	industry 
+ [ ] CMT: Convolutional Neural Networks Meet Vision Transformers		172	Huawei, University of Sydney	Australia, China	academia 
+ [ ] Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model	271	158	Microsoft, NVIDIA	USA	industry 
+ [ ] What Makes Good In-Context Examples for GPT-3?		157	Duke University, Meta, Microsoft	USA	industry 
+ [ ] Ensemble unsupervised autoencoders and Gaussian mixture model for cyberattack detection		145	Ningbo University of Technology, Tongji University	China	academia 
+ [ ] Training Compute-Optimal Large Language Models		144	DeepMind	UK	industry
+ [ ] Learning robust perceptive locomotion for quadrupedal robots in the wild	3	141	ETH Zurich, Intel, Korea Advanced Institute of Science and Technology	South Korea, Switzerland, USA	academia 
+ [ ] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances	82	135	Everyday Robots, Google	USA	industry 
+ [ ] How Do Vision Transformers Work?	193	129	NAVER, Yonsei University	South Korea	academia 
+ [ ] Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs	30	127	Aberystwyth University, Megvii, Tsinghua University	China, UK	industry 
+ [ ] Large Language Models are Zero-Shot Reasoners	862	124	Google, University of Tokyo	Japan, USA	academia 
+ [ ] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time		122	Columbia University, Google, Meta, Tel Aviv University, University of Washington	Israel, USA	academia 
+ [ ] Patches Are All You Need?	117	116	Bosch, Carnegie Mellon University	USA	academia, industry 
+ [ ] Competition-Level Code Generation with AlphaCode		113	DeepMind	UK	industry 
+ [ ] TensoRF: Tensorial Radiance Fields	73	110	Adobe, ShanghaiTech University, UC San Diego, University of Tubingen	China, Germany, USA	academia 
+ [ ] Video Diffusion Models		103	Google	Netherlands, USA	industry 
+ [ ] Data Analytics for the Identification of Fake Reviews Using Supervised Learning		102	Albaha University, Dr. Babasaheb Ambedkar Marathwada University, King Faisal University, Nahrain University	India, Iraq, Saudi Arabia	academia 
+ [ ] Visual Prompt Tuning	26	102	Cornell University, Meta, University of Copenhagen	Denmark, USA	industry 
+ [ ] [Neural Video Editing] INVE: Interactive Neural Video Editing, https://arxiv.org/abs/2307.07663, https://gabriel-huang.github.io/inve/
+ [ ] [CV] DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection	15	100	Hong Kong University of Science and Technology, International Digital Economy Academy, Tsinghua University	China	academia 
+ [ ] [CV] DINOv2: Learning Robust Visual Features without Supervision, https://arxiv.org/abs/2304.07193, https://huggingface.co/docs/transformers/main/model_doc/dinov2
+ [ ] [CV] [Meta] PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning, https://arxiv.org/abs/2308.03977, https://github.com/facebookresearch/PUG, https://pug.metademolab.com/?utm_source=linkedin&utm_medium=organic_social&utm_campaign=pug&utm_content=video
+ [ ] VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training	66	100	Nanjing University, Shanghai AI Lab, Tencent	China	academia 
+ [ ] Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?	199	99	Allen Institute for Artificial Intelligence, Meta, University of Washington	USA	academia, industry 
+ [ ] BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers	11	96	Nanjing University, Shanghai AI Lab, University of Hong Kong	China	academia 
+ [ ] Conditional Prompt Learning for Vision-Language Models	51	93	Nanyang Technological University	Singapore	academia 
+ [ ] Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution	151	93	Stanford University	USA	academia 
+ [ ] Measuring and Improving the Use of Graph Information in Graph Neural Networks	1	93	Chinese University of Hong Kong, National University of Singapore	China, Singapore	academia 
+ [ ] Exploring Plain Vision Transformer Backbones for Object Detection	205	91	Meta	USA	industry 
+ [ ] GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation	26	90	CIFAR, HEC Montreal, Mila, Stanford University, University of Montreal	Canada, USA	academia 
+ [ ] OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework	91	88	Alibaba Group	China	industry 
+ [ ] Block-NeRF: Scalable Large Scene Neural View Synthesis	641	86	Google, UC Berkeley, Waymo	USA	industry 
+ [ ] Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents	24	86	Carnegie Mellon University, Google, UC Berkeley	USA	industry 
+ [ ] Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models	881	81	AI Objectives Institute, Allen Institute for Artificial Intelligence, Amazon, Amelia, Amirkabir University of Technology, Anthropic, Apergo, Arizona State University, Bauhaus-Universität Weimar, Bluevine, Carnegie Mellon University, Carnegie Robotics, Charles River Analytics, Columbia University, Complutense University of Madrid, Conjecture, Cornell University, De Anza College, DeepMind, Duke Kunshan University, Duke University, ETH Zurich, EleutherAI, Ford Motor Company, Fraunhofer Institute for Integrated Circuits, Georgia Institute of Technology, Google, Hacettepe University, Harker School, Harvard University, Heidelberg Institute for Theoretical Studies, Hong Kong University of Science and Technology, IBM, Illinois Mathematics and Science Academy, Imperial College London, Indian Institute of Technology Madras, Juelich Research Center, KU Leuven, Karlsruhe Institute of Technology, King’s College London, Koç University, Lawrence Berkeley National Laboratory, Leipzig University, Ludwig Maximilian University of Munich, MIT, ML Collective, Martin-Luther-University Halle-Wittenberg, Max Planck Institute for Intelligent Systems, Max Planck Institute for Mathematics in the Sciences, McGill University, McMaster University, Meta, Microsoft, Mila, MosaicML, NAVER, NUST School of Electrical Engineering and Computer Science, National Public School, HSR, National Research Council Canada, National University of Singapore, NeuralSpace, Neurospin, New York University, NoOverfitting Lab, OpenAI, Ought, Peking University, Penn State University, Princeton University, Queen’s University, Research Institutes of Sweden, Rice University, Rutgers University, Saarland University, Salesforce, Sapienza University of Rome, Sharif University of Technology, Stanford University, Strathmore University, Synthego Corporation, Technion, Tel Aviv University, Thapar Institute of Engineering and Technology, Thomson Reuters Special Services, TomTom, Toyota Technological Institute at Chicago, Tufts University, UC Berkeley, UC Irvine, UC Los Angeles, UC San Diego, Umeå University, UnifyID labs, University of Amsterdam, University of Bristol, University of Cambridge, University of Edinburgh, University of Hamburg, University of Heidelberg, University of Hong Kong, University of Illinois Urbana-Champaign, University of Memphis, University of Michigan, University of Milano-Bicocca, University of North Carolina at Chapel Hill, University of Notre Dame, University of Oxford, University of Pennsylvania, University of Potsdam, University of Southern California, University of Tehran, University of Texas at Austin, University of Toronto, University of Tsukuba, University of Tubingen, University of Utah, University of Virginia, University of Washington, University of Wisconsin-Madison, Valencia Polytechnic University, Wrocław University of Science and Technology, Yale University	Belgium, Canada, China, France, Germany, India, Iran, Israel, Italy, Japan, Kenya, Netherlands, Pakistan, Poland, Singapore, South Korea, Spain, Sweden, Switzerland, Turkey, UK, USA	academia 
+ [ ] Outracing champion Gran Turismo drivers with deep reinforcement learning		80	Sony	Japan	industry 
+ [ ] BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning	10	77	Everyday Robots, Google, Stanford University, UC Berkeley	USA	academia 
+ [ ] DN-DETR: Accelerate DETR Training by Introducing Query DeNoising		74	Hong Kong University of Science and Technology, International Digital Economy Academy, Tsinghua University	China	academia 
+ [ ] Emergent Abilities of Large Language Models	442	74	DeepMind, Google, Stanford University, University of North Carolina at Chapel Hill	UK, USA	academia, industry 
+ [ ] Equivariant Diffusion for Molecule Generation in 3D	131	73	École Polytechnique Fédérale de Lausanne, University of Amsterdam	Netherlands, Switzerland	academia 
+ [ ] Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images	6	73	NVIDIA, Vanderbilt University	USA	industry 
+ [ ] GPT-NeoX-20B: An Open-Source Autoregressive Language Model	50	72	EleutherAI		industry 
+ [ ] Online reinforcement learning multiplayer non-zero sum games of continuous-time Markov jump linear systems		72	Anhui University, Chengdu University, Murdoch University, University of Kragujevac	Australia, China, Serbia	academia 
+ [ ] Self-consistency improves chain of thought reasoning in language models	290	71	Google	USA	industry 
+ [ ] Detecting Twenty-thousand Classes using Image-level Supervision	35	70	Meta, University of Texas at Austin	USA	industry 
+ [ ] Image fusion in the loop of high-level vision tasks: A semantic-aware real-time infrared and visible image fusion network		68	Wuhan University	China	academia 
+ [ ] LAION-5B: An open large-scale dataset for training next generation image-text models	53	66	Juelich Research Center, LAION, Stability AI, Technical University of Darmstadt, Technical University of Munich, UC Berkeley, University of Washington	Germany, USA	industry 
+ [ ] Denoising Diffusion Restoration Models		65	NVIDIA, Stanford University, Technion	Israel, USA	industry 
+ [ ] From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions, https://arxiv.org/abs/2310.07699
+ [ ] VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance	175	64	AiDock, Booz Allen Hamilton, EleutherAI, Georgia Institute of Technology	Israel, USA	industry 
+ [ ] CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields	33	63	City University of Hong Kong, Microsoft, Snap Inc., University of Southern California	China, USA	academia 
+ [ ] Solving Quantitative Reasoning Problems with Language Models	139	63	Google	USA	industry 
+ [ ] Masked Autoencoders As Spatiotemporal Learners	120	61	Meta	USA	industry 
+ [ ] Why do tree-based models still outperform deep learning on tabular data?	646	60	CNRS, INRIA, Sorbonne University	France	academia 
+ [ ] Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language	499	59	Google	USA	industry 
+ [ ] ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond	2	59	JD Explore Academy, University of Sydney	Australia, China	academia, industry 
+ [ ] Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks	178	58	Microsoft	USA	industry 
+ [ ] Language-driven Semantic Segmentation	95	57	Apple, Cornell University, Intel, University of Copenhagen	Denmark, USA	industry 
+ [ ] Vision-Language Pre-Training with Triple Contrastive Learning	34	56	Amazon, University of Texas at Arlington	USA	academia 
+ [ ] Deep Reinforcement Learning-Based Path Control and Optimization for Unmanned Ships		55	Sipivt, Tongji University	China	industry 
+ [ ] EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction	208	54	MIT	USA	academia 
+ [ ] Omnivore: A Single Model for Many Visual Modalities	89	54	Meta	USA	industry 
+ [ ] Quantifying Memorization Across Neural Language Models	106	54	Cornell University, Google, University of Pennsylvania	USA	industry 
+ [ ] DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection	36	53	Google, Johns Hopkins University	USA	industry 
+ [ ] Genetic Algorithm-Based Trajectory Optimization for Digital Twin Robots		53	Wuhan University of Science and Technology	China	academia 
+ [ ] Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors	280	53	Meta	USA	industry 
+ [ ] Discovering faster matrix multiplication algorithms with reinforcement learning DeepMind	UK	industry 
+ [ ] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation	221	52	Boston University, Google	USA	industry 
+ [ ] PETR: Position Embedding Transformation for Multi-View 3D Object Detection	4	52	Megvii	China	industry 
+ [ ] Protein structure predictions to atomic accuracy with AlphaFold		51	DeepMind	UK	industry 
+ [ ] ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges	2	50	Queen Mary University of London	UK	academia 
+ [ ] HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video	72	50	Google, University of Washington	USA	academia, industry 
+ [ ] UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models	38	49	Allen Institute for Artificial Intelligence, Carnegie Mellon University, George Mason University, Google, Meta, Penn State University, Salesforce, ServiceNow Research, Shanghai AI Lab, Stanford University, UC Berkeley, University of Edinburgh, University of Hong Kong, University of Illinois Urbana-Champaign, University of Washington, University of Waterloo, Yale University	Canada, China, UK, USA	academia 
+ [ ] A Systematic Evaluation of Large Language Models of Code	61	48	Carnegie Mellon University	USA	academia 
+ [ ] Robust Speech Recognition via Large-Scale Weak Supervision	40	48	OpenAI	USA	industry 
+ [ ] Diffusion Models: A Comprehensive Survey of Methods and Applications	274	47	Beijing University of Posts and Telecommunications, Carnegie Mellon University, HEC Montreal, Mila, OpenAI, Peking University, UC Los Angeles, UC Merced	Canada, China, USA	academia 
+ [ ] Can language models learn from explanations in context?	113	46	DeepMind	UK	industry 
+ [ ] NELA-GT-2021: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles	9	46	Rensselaer Polytechnic Institute, University of Tennessee Knoxville	USA	academia 
+ [ ] ActionFormer: Localizing Moments of Actions with Transformers		44	4Paradigm Inc., Nanjing University, University of Wisconsin-Madison	China, USA	academia 
+ [ ] DeiT III: Revenge of the ViT	115	44	Meta, Sorbonne University	France, USA	academia, industry 
+ [ ] Least-to-Most Prompting Enables Complex Reasoning in Large Language Models		44	Google	USA	industry 
+ [ ] Diffusion-LM Improves Controllable Text Generation	253	43	Stanford University	USA	academia 
+ [ ] Overview of The Shared Task on Homophobia and Transphobia Detection in Social Media Comments		41	Indian Institute of Information Technology and Management, Madurai Kamaraj University, National University of Ireland Galway, SSN College of Engineering	India, Ireland	academia 
+ [ ] Text and Code Embeddings by Contrastive Pre-Training	23	40	OpenAI	USA	industry 
+ [ ] Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality	125	40	Hugging Face, Meta, University College London, University of Waterloo	Canada, UK, USA	industry 
+ [ ] BLOOM: A 176B-Parameter Open-Access Multilingual Language Model	325	39	BigScience Team	France	industry 
+ [ ] Red Teaming Language Models with Language Models	40	39	DeepMind, New York University	UK, USA	industry 
+ [ ] Transformer Memory as a Differentiable Search Index	372	39	Google	USA	industry 
+ [ ] Torsional Diffusion for Molecular Conformer Generation	109	38	Harvard University, MIT	USA	academia 
+ [ ] Unified Contrastive Learning in Image-Text-Label Space	66	37	Microsoft	USA	industry 
+ [ ] Benchmarking Generalization via In-Context Instructions on 1, 600
+ [ ] Language Tasks	149	36	Allen Institute for Artificial Intelligence, Amirkabir University of Technology, Arizona State University, Columbia University, Factored AI, Government Polytechnic Rajkot, Indian Institute of Technology Kharagpur, Indian Institute of Technology Madras, Johns Hopkins University, Microsoft, National Institute of Technology Karnataka, National University of Singapore, PSG College Of Technology, Sharif University of Technology, Stanford University, Tata Consultancy Services, UC Berkeley, University of Amsterdam, University of Massachusetts Amherst, University of Washington, Zycus Infotech
+ [ ] Highly accurate protein structure prediction with AlphaFold		8783	DeepMind, Seoul National University	South Korea, UK	industry 
+ [ ] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows	383	5389	Microsoft	USA	industry 
+ [ ] Learning Transferable Visual Models From Natural Language Supervision	178	3658	OpenAI	USA	industry 
+ [ ] Accurate prediction of protein structures and interactions using a three-track neural network		1659	Harvard University, Lawrence Berkeley National Laboratory, North-West University, Stanford University, UC Berkeley, University of British Columbia, University of Cambridge, University of Graz, University of Texas Southwestern Medical Center, University of Victoria, University of Washington, University of the Free State	Austria, Canada, South Africa, UK, USA 
+ [ ] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions	69	1306	Inception Institute of AI, Nanjing University, Nanjing University of Science and Technology, SenseTime, University of Hong Kong	China, UAE	academia 
+ [ ] Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers	20	1280	Fudan University, Meta, Tencent, University of Oxford, University of Surrey	China, UK, USA	academia 
+ [ ] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?		1241	Black in AI, University of Washington	USA	academia 
+ [ ] Masked Autoencoders Are Scalable Vision Learners	843	1234	Meta	USA	industry 
+ [ ] Emerging Properties in Self-Supervised Vision Transformers	269	1219	INRIA, Meta, Sorbonne University	France, USA	industry 
+ [ ] Review of deep learning: concepts, CNN architectures, challenges, applications, future directions		1210	Queensland University of Technology	Australia	academia 
+ [ ] nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation		1178	DeepMind, German Cancer Research Center, Heidelberg University Hospital, University of Heidelberg	Germany, UK	academia 
+ [ ] Zero-Shot Text-to-Image Generation	155	1177	OpenAI	USA	industry 
+ [ ] TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation	46	998	East China Normal University, Johns Hopkins University, PAII Inc., Stanford University, University of Electronic Science and Technology of China	China, USA	academia 
+ [ ] Barlow Twins: Self-Supervised Learning via Redundancy Reduction	1076	951	Meta, New York University	USA	industry 
+ [ ] Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet	13	912	National University of Singapore, YITU Technology	China, Singapore	academia 
+ [ ] MLP-Mixer: An all-MLP Architecture for Vision	671	896	Google	USA	industry 
+ [ ] SimCSE: Simple Contrastive Learning of Sentence Embeddings	85	866	Princeton University, Tsinghua University	China, USA	academia 
+ [ ] Coordinate Attention for Efficient Mobile Network Design	49	860	National University of Singapore	Singapore	academia 
+ [ ] SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers	100	831	California Institute of Technology, NVIDIA, Nanjing University, University of Hong Kong	China, USA	academia 
+ [ ] BEiT: BERT Pre-Training of Image Transformers	143	785	Harbin Institute of Technology, Microsoft	China, USA	industry 
+ [ ] CvT: Introducing Convolutions to Vision Transformers		761	McGill University, Microsoft	Canada, USA	industry 
+ [ ] Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision	41	759	Google	USA	industry 
+ [ ] Transformers in Vision: A Survey	158	757	Inception Institute of AI, Mohamed bin Zayed University of Artificial Intelligence, Monash University, University of Central Florida	Australia, UAE, USA	academia 
+ [ ] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing	201	737	Carnegie Mellon University, National University of Singapore	Singapore, USA	academia 
+ [ ] EfficientNetV2: Smaller Models and Faster Training	666	730	Google	USA	industry 
+ [ ] Is Space-Time Attention All You Need for Video Understanding?	84	729	Dartmouth College, Meta	USA	academia, industry 
+ [ ] ViViT: A Video Vision Transformer	66	713	Google	USA	industry 
+ [ ] Diffusion Models Beat GANs on Image Synthesis	566	694	OpenAI	USA	industry 
+ [ ] An Empirical Study of Training Self-Supervised Vision Transformers	76	601	Meta	USA	industry 
+ [ ] The Power of Scale for Parameter-Efficient Prompt Tuning	227	594	Google	USA	industry 
+ [ ] SwinIR: Image Restoration Using Swin Transformer	34	578	ETH Zurich, KU Leuven	Belgium, Switzerland	academia 
+ [ ] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity	120	576	Google	USA	industry 
+ [ ] Protein complex prediction with AlphaFold-Multimer		561	DeepMind	UK	industry 
+ [ ] Bottleneck Transformers for Visual Recognition	46	542	Google, UC Berkeley	USA	industry 
+ [ ] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units	43	534
+ [ ] Alias-Free Generative Adversarial Networks	77	520	Aalto University, NVIDIA	Finland, USA	industry 
+ [ ] Towards Causal Representation Learning	117	504	CIFAR, ETH Zurich, Google, Max Planck Institute for Intelligent Systems, Mila, University of Montreal	Canada, Germany, Switzerland, USA	academia 
+ [ ] Vision Transformers for Dense Prediction	360	486	Intel	USA	industry 
+ [ ] Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges		480	DeepMind, Imperial College London, New York University, Qualcomm, Twitter	UK, USA	industry 
+ [ ] High-Resolution Image Synthesis with Latent Diffusion Models	210	480	Ludwig Maximilian University of Munich, Runway, University of Heidelberg	Germany, USA	academia 
+ [ ] Segmenter: Transformer for Semantic Segmentation	59	468	INRIA	France	academia 
+ [ ] RepVGG: Making VGG-style ConvNets Great Again		467	Aberystwyth University, Hong Kong University of Science and Technology, Megvii, Tsinghua University	China, UK	industry 
+ [ ] Multiscale Vision Transformers	99	452	Meta, UC Berkeley	USA	industry 
+ [ ] CoAtNet: Marrying Convolution and Attention for All Data Sizes		442	Google	USA	industry 
+ [ ] CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification	34	435	IBM, MIT	USA	academia, industry 
+ [ ] ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision		435	Kakao Brain, Kakao Enterprise, NAVER	South Korea	industry 
+ [ ] Video Swin Transformer	32	415	Huazhong University of Science and Technology, Microsoft, Tsinghua University, University of Science and Technology of China	China, USA	industry 
+ [ ] End-to-End Video Instance Segmentation With Transformers		411	Meituan, University of Adelaide	Australia, China	industry 
+ [ ] StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery	602	401	Adobe, Hebrew University of Jerusalem, Tel Aviv University	Israel, USA	academia 
+ [ ] Evaluating Large Language Models Trained on Code	934	400	Anthropic, OpenAI, Zipeline	USA	industry 
+ [ ] Improved Denoising Diffusion Probabilistic Models	50	397	OpenAI	USA	industry 
+ [ ] VinVL: Revisiting Visual Representations in Vision-Language Models	3	373	Microsoft, University of Washington	USA	industry 
+ [ ] ABCDM: An Attention-based Bidirectional CNN-RNN Deep Model for sentiment analysis		361	Deakin University, Nanyang Technological University, Ngee Ann Polytechnic, University of Shahrekord	Australia, Iran, Singapore	academia 
+ [ ] Out-of-Distribution Generalization via Risk Extrapolation (REx)		354	McGill University, Meta, Mila, University of Montreal, University of Toronto, Vector	Canada, USA	academia 
+ [ ] UNETR: Transformers for 3D Medical Image Segmentation	55	351	NVIDIA, Vanderbilt University	USA	industry 
+ [ ] ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases		344	Meta, École normale supérieure	France, USA	industry 
+ [ ] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation	30	337	Salesforce	USA	industry 
+ [ ] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models	1614	333	OpenAI	USA	industry 
+ [ ] Perceiver: General Perception with Iterative Attention		329	OpenAI	USA	industry 
+ [ ] Scaling Vision Transformers	237	324	Google	USA	industry 
+ [ ] VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning	241	314	INRIA, Meta, New York University	France, USA	academia, industry 
+ [ ] Machine learning accelerated computational fluid dynamics	19	312	Google, Harvard University	USA	industry 
+ [ ] “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI		310	Google	USA	industry 
+ [ ] Per-Pixel Classification is Not All You Need for Semantic Segmentation	69	309	Meta, University of Illinois Urbana-Champaign	USA	industry 
+ [ ] Finetuned Language Models Are Zero-Shot Learners	402	307	Google	USA	industry 
+ [ ] Multitask Prompted Training Enables Zero-Shot Task Generalization	640	300	ASUS, BigScience Team, Birla Institute of Technology and Science, Pilani, Booz Allen Hamilton, Brown University, Charles River Analytics, EleutherAI, Hugging Face, Hyperscience, IBM, IMATAG, INRIA, IRISA, Institute for Infocomm Research, King Fahd University of Petroleum and Minerals, NAVER, Nanyang Technological University, New York University, Parity, SAP, SambaNova Systems, Snorkel AI, Stanford University, UC Berkeley, UC San Diego, University of Rome, University of Virginia, VU Amsterdam, Walmart, ZEALS	France, Germany, India, Italy, Japan, Netherlands, Saudi Arabia, Singapore, South Korea, Taiwan, UK, USA	industry 
+ [ ] TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up	77	297	IBM, MIT, UC Santa Barbara, University of Texas at Austin	USA	academia 
+ [ ] Scene Text Detection and Recognition: The Deep Learning Era.		294	ByteDance, Carnegie Mellon University, Megvii	China, USA	industry 
+ [ ] PlenOctrees for Real-time Rendering of Neural Radiance Fields	71	278	UC Berkeley, University of Southern California	USA	academia 
+ [ ] High-Performance Large-Scale Image Recognition Without Normalization	179	275	DeepMind	UK	industry 
+ [ ] Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields	33	268	Google, UC Berkeley	USA	industry 
+ [ ] GPT Understands, Too	87	264	Beijing Academy of Artificial Intelligence, MIT, Recurrent AI, Tsinghua University	China, USA	academia 
+ [ ] Less Is More: ClipBERT for Video-and-Language Learning via Sparse Sampling		260	Microsoft, University of North Carolina at Chapel Hill	USA	industry 
+ [ ] SimMIM: A Simple Framework for Masked Image Modeling	76	257	Microsoft, Tsinghua University, Xi’an Jiaotong University	China, USA	industry 
+ [ ] VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text	285	255	Columbia University, Cornell University, Google	USA	industry 
+ [ ] Restormer: Efficient Transformer for High-Resolution Image Restoration	52	247	Google, Inception Institute of AI, Mohamed bin Zayed University of Artificial Intelligence, Monash University, UC Merced, Yonsei University	Australia, South Korea, UAE, USA	academia, industry 
+ [ ] Understanding adversarial attacks on deep learning based medical image analysis systems.	1	246	Beihang University, Chinese Academy of Sciences, National Institute of Informatics, Shanghai Jiao Tong University, University of Melbourne	Australia, China, Japan	academia 
+ [ ] FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search	1	245	Xiaomi	China	industry 
+ [ ] Calibrate Before Use: Improving Few-Shot Performance of Language Models	90	243	UC Berkeley, UC Irvine, University of Maryland	USA	academia 
+ [ ] Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision	1	242	Microsoft, Peking University	China, USA	industry 
+ [ ] IBRNet: Learning Multi-View Image-Based Rendering	8	241	Cornell University, Google, Princeton University	USA	academia, industry 
+ [ ] E(n) Equivariant Graph Neural Networks	60	238	Bosch, University of Amsterdam	Germany, Netherlands	academia, industry 
+ [ ] LoFTR: Detector-Free Local Feature Matching with Transformers	95	238	SenseTime, Zhejiang University	China	academia 
+ [ ] Plant leaf disease classification using EfficientNet deep learning model		237	Iskenderun Technical University, Karabuk University, Kastamonu University	Turkey	academia 
+ [ ] How Attentive are Graph Attention Networks?	56	234	Carnegie Mellon University, Technion	Israel, USA	academia 
+ [ ] Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking	15	234	Hefei Comprehensive National Science Center, University of Science and Technology of China	China	academia 
+ [ ] MDETR - Modulated Detection for End-to-End Multi-Modal Understanding		233	Meta, New York University	USA	academia 
+ [ ] Learning to Prompt for Vision-Language Models	61	231	Nanyang Technological University	Singapore	academia 
+ [ ] SimVLM: Simple Visual Language Model Pretraining with Weak Supervision		231	Carnegie Mellon University, Google, University of Washington	USA	industry 
+ [ ] Scaling Language Models: Methods, Analysis & Insights from Training Gopher		229	DeepMind	UK	industry 
+ [ ] How to Train Your Robot with Deep Reinforcement Learning; Lessons We've Learned	42	220	Google, UC Berkeley, X, The Moonshot Factory	USA	academia, industry 
+ [ ] Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts	8	217	Google	USA	industry 
+ [ ] Model-Contrastive Federated Learning	1	209	National University of Singapore, UC Berkeley	Singapore, USA	academia 
+ [ ] SpeechBrain: A General-Purpose Speech Toolkit	117	208	Aalto University, Academia Sinica, Avignon Université, HEC Montreal, Indian Institute of Technology Madras, Marche Polytechnic University, McGill University, Mila, NVIDIA, Ohio State University, Samsung, Toulouse Institute of Computer Science Research, Toyota Technological Institute at Chicago, University of Cambridge, University of Edinburgh, University of Montreal, University of Sherbrooke	Canada, Finland, France, India, Italy, South Korea, Taiwan, UK, USA	academia 
+ [ ] MagFace: A Universal Representation for Face Recognition and Quality Assessment	10	206	Aibee	China	industry 
+ [ ] Offline Reinforcement Learning as One Big Sequence Modeling Problem	110	200	UC Berkeley	USA	academia 
+ [ ] Unified Pre-training for Program Understanding and Generation	16	200	Columbia University, UC Los Angeles	USA	academia 
+ [ ] Image Super-Resolution via Iterative Refinement	401	198	Google	USA	industry 
+ [ ] FastNeRF: High-Fidelity Neural Rendering at 200FPS	164	194	Microsoft	USA	industry 
+ [ ] BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models	95	194	Technical University of Darmstadt	Germany	academia 
+ [ ] Measurement and Fairness.
+ [ ] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale	142	12042	Google	USA	industry
+ [ ] A Simple Framework for Contrastive Learning of Visual Representations	16	8476	Google	USA	industry 
+ [ ] Language Models are Few-Shot Learners	331	7903	OpenAI	USA	industry 
+ [ ] YOLOv4: Optimal Speed and Accuracy of Object Detection	20	7860	Academia Sinica	Taiwan	industry 
+ [ ] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.	53	6362	Google	USA	industry 
+ [ ] Momentum Contrast for Unsupervised Visual Representation Learning	8	6060	Meta	USA	industry 
+ [ ] End-to-End Object Detection with Transformers	43	4998	Meta, Paris Dauphine University	France, USA	industry 
+ [ ] Analyzing and Improving the Image Quality of StyleGAN	44	3101	Aalto University, NVIDIA	Finland, USA	industry 
+ [ ] EfficientDet: Scalable and Efficient Object Detection	7	3081	Google	USA	industry 
+ [ ] Advances and Open Problems in Federated Learning	5	2921	Australian National University, Carnegie Mellon University, Cornell University, Emory University, École Polytechnique Fédérale de Lausanne, Georgia Institute of Technology, Google, Hong Kong University of Science and Technology, INRIA, IT University of Copenhagen, MIT, Nanyang Technological University, Princeton University, Rutgers University, Stanford University, UC Berkeley, UC San Diego, University of Illinois Urbana-Champaign, University of Oulu, University of Pittsburgh, University of Southern California, University of Virginia, University of Warwick, University of Washington, University of Wisconsin-Madison	Australia, China, Denmark, Finland, France, Singapore, Switzerland, UK, USA	industry 
+ [ ] Unsupervised Cross-lingual Representation Learning at Scale	6	2857	Meta	USA	industry 
+ [ ] Bootstrap your own latent: A new approach to self-supervised Learning	13	2827	DeepMind, Imperial College London	UK	industry 
+ [ ] Training data-efficient image transformers & distillation through attention	45	2558	Meta, Sorbonne University	France, USA	industry 
+ [ ] Random Erasing Data Augmentation.	2	2453	University of Technology Sydney, Xiamen University	Australia, China	academia 
+ [ ] nuScenes: A Multimodal Dataset for Autonomous Driving	3	2366	nuTonomy	USA	industry 
+ [ ] NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis	188	2283	Google, UC Berkeley, UC San Diego	USA	academia 
+ [ ] ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators		2142	CIFAR, Google, Stanford University	Canada, USA	academia 
+ [ ] Improved protein structure prediction using potentials from deep learning		2121	DeepMind, Francis Crick Institute, University College London	UK	industry 
+ [ ] Transformers: State-of-the-Art Natural Language Processing		2071	Hugging Face	USA	industry 
+ [ ] Unsupervised Learning of Visual Features by Contrasting Cluster Assignments	12	1847	INRIA, Meta	France, USA	industry 
+ [ ] Supervised Contrastive Learning	29	1835	Boston University, Google, MIT, Snap Inc.	USA	industry 
+ [ ] Improved Baselines with Momentum Contrastive Learning	1	1782	Meta	USA	industry 
+ [ ] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations	34	1777	Meta	USA	industry 
+ [ ] Exploring Simple Siamese Representation Learning	73	1767	Meta	USA	industry 
+ [ ] ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks	5	1713	Dalian University of Technology, Harbin Institute of Technology, Tianjin University	China	academia 
+ [ ] RandAugment: Practical Automated Data Augmentation with a Reduced Search Space	3	1684	Google	USA	industry 
+ [ ] Self-Training With Noisy Student Improves ImageNet Classification	19	1674	Carnegie Mellon University, Google	USA	industry 
+ [ ] Longformer: The Long-Document Transformer	55	1650	Allen Institute for Artificial Intelligence	USA	industry 
+ [ ] FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence		1603	Google	USA	industry 
+ [ ] Face2Face: Real-time Face Capture and Reenactment of RGB Videos	1	1532	Max Planck Institute for Informatics, Stanford University, University of Erlangen-Nuremberg	Germany, USA	academia 
+ [ ] Image Segmentation Using Deep Learning: A Survey	13	1450	Qualcomm, Snapchat, UC Los Angeles, University of Extremadura, University of Texas at Dallas, University of Waterloo	Canada, Spain, USA	academia 
+ [ ] Unsupervised Data Augmentation for Consistency Training	11	1345	Carnegie Mellon University, Google	USA	industry 
+ [ ] Big Self-Supervised Models are Strong Semi-Supervised Learners	16	1314	Google	USA	industry 
+ [ ] SpanBERT: Improving Pre-training by Representing and Predicting Spans	2	1307	Allen Institute for Artificial Intelligence, Meta, Princeton University, University of Washington	USA	industry
+ [ ] Conformer: Convolution-augmented Transformer for Speech Recognition	51	1214	Google	USA	industry 
+ [ ] Scalability in Perception for Autonomous Driving: Waymo Open Dataset		1209	Google, Waymo	USA	industry 
+ [ ] Denoising Diffusion Probabilistic Models	108	1206	UC Berkeley	USA	academia 
+ [ ] Don't Stop Pretraining: Adapt Language Models to Domains and Tasks	10	1150	Allen Institute for Artificial Intelligence, University of Washington	USA	academia, industry 
+ [ ] LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation		1141	Hefei University of Technology, Kuaishou Technology, National University of Singapore, University of Science and Technology of China	China, Singapore	academia 
+ [ ] Open Graph Benchmark: Datasets for Machine Learning on Graphs	22	1112	Harvard University, Microsoft, Stanford University, Technical University Dortmund	Germany, USA	academia 
+ [ ] Dense Passage Retrieval for Open-Domain Question Answering	23	1106	Meta, Princeton University, University of Washington	USA	industry 
+ [ ] SCAFFOLD: Stochastic Controlled Averaging for Federated Learning	1	1052	École Polytechnique Fédérale de Lausanne, Google, New York University	Switzerland, USA	industry 
+ [ ] Data-Efficient Image Recognition with Contrastive Predictive Coding		1031	DeepMind, UC Berkeley	UK, USA	industry 
+ [ ] Stanza: A Python Natural Language Processing Toolkit for Many Human Languages		1016	Stanford University	USA	academia 
+ [ ] ResNeSt: Split-Attention Networks	5	994	Amazon, ByteDance, Meta, SenseTime, Snap Inc., UC Davis	China, USA	industry 
+ [ ] Self-Supervised Learning of Pretext-Invariant Representations	2	993	Meta	USA	industry 
+ [ ] Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks	2	971	Microsoft, University of Washington	USA	industry 
+ [ ] Implicit Neural Representations with Periodic Activation Functions	38	963	Stanford University	USA	academia 
+ [ ] TinyBERT: Distilling BERT for Natural Language Understanding		954	Huawei, Huazhong University of Science and Technology	China	industry 
+ [ ] Big Bird: Transformers for Longer Sequences	10	929	Google	USA	industry 
+ [ ] BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning	2	924	Cornell University, Element, UC Berkeley, UC San Diego	Canada, USA	academia 
+ [ ] StarGAN v2: Diverse Image Synthesis for Multiple Domains		916	NAVER	South Korea	industry 
+ [ ] PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection		899	Chinese Academy of Sciences, Chinese University of Hong Kong, National Laboratory of Pattern Recognition, SenseTime	China	academia 
+ [ ] A Primer in BERTology: What we know about how BERT works	20	892	University of Copenhagen, University of Massachusetts Lowell	Denmark, USA	academia 
+ [ ] RAFT: Recurrent All-Pairs Field Transforms for Optical Flow	10	860	Princeton University	USA	academia 
+ [ ] Multilingual Denoising Pre-training for Neural Machine Translation	1	859	Meta	USA	industry 
+ [ ] Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection		850	Beijing University of Posts and Telecommunications, Chinese Academy of Sciences, National Laboratory of Pattern Recognition, University of Chinese Academy of Sciences, Westlake University	China	academia 
+ [ ] Knowledge Distillation: A Survey	19	850	University of London, University of Sydney	Australia, UK	academia 
+ [ ] RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds	3	848	National University of Defense Technology, Sun Yat-sen University, University of Oxford	China, UK	academia 
+ [ ] SuperGlue: Learning Feature Matching With Graph Neural Networks	10	839	ETH Zurich, Magic Leap	Switzerland, USA	industry 
+ [ ] Generative Pretraining From Pixels		838	OpenAI	USA	industry 
+ [ ] Pre-trained models for natural language processing: A survey	10	832	Fudan University	China	academia 
+ [ ] Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks		811	University of Tubingen	Germany	academia 
+ [ ] Deep Learning for Person Re-identification: A Survey and Outlook		806	Beijing Institute of Technology, Inception Institute of AI, Salesforce, Singapore Management University, University of Surrey, Wuhan University	China, Singapore, UAE, UK, USA	academia, industry 
+ [ ] Object-Contextual Representations for Semantic Segmentation		800	Chinese Academy of Sciences, Microsoft, University of Chinese Academy of Sciences	China, USA	industry 
+ [ ] Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains	14	774	Google, UC Berkeley, UC San Diego	USA	academia 
+ [ ] Scaled-YOLOv4: Scaling Cross Stage Partial Network		758	Academia Sinica, Intel, Providence University	Taiwan, USA	academia, industry 
+ [ ] Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere	3	755	MIT	USA	academia 
+ [ ] Fast is better than free: Revisiting adversarial training		744	Bosch, Carnegie Mellon University	Germany, USA	academia, industry 
+ [ ] Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting	35	741	Beihang University, Rutgers University, Sharjah Economic Development Department, UC Berkeley	China, UAE, USA	industry 
+ [ ] CodeBERT: A Pre-Trained Model for Programming and Natural Languages	30	736	Harbin Institute of Technology, Microsoft, Sun Yat-sen University	China, USA	industry 
+ [ ] Big Transfer (BiT): General Visual Representation Learning	51	704	Google	USA	industry 
+ [ ] Pre-Trained Image Processing Transformer	3	703	Huawei, Peking University, Peng Cheng Laboratory, University of Sydney	Australia, China	academia 
+ [ ] Rethinking Attention with Performers	38	697	Alan Turing Institute, DeepMind, Google, University of Cambridge	UK, USA	academia, industry 
+ [ ] What makes for good views for contrastive learning	8	686	Brown University, Google, MIT	USA	academia 
+ [ ] Score-Based Generative Modeling through Stochastic Differential Equations	54	684	Google, Stanford University	USA	industry 
+ [ ] Graph Contrastive Learning with Augmentations	2	682	Google, Texas A&M University, University of Science and Technology of China, University of Texas at Austin	China, USA	academia 
+ [ ] Interpreting the Latent Space of GANs for Semantic Face Editing	14	675	Chinese University of Hong Kong	China	academia 
+ [ ] Linformer: Self-Attention with Linear Complexity	8	668	Meta	USA	industry 
+ [ ] MaskGAN: Towards Diverse and Interactive Facial Image Manipulation		653	Chinese University of Hong Kong, SenseTime, University of Hong Kong	China	academia 
+ [ ] Simple and Deep Graph Convolutional Networks	1	653	Alibaba Group, Fudan University, Renmin University of China	China	industry 
+ [ ] REALM: Retrieval-Augmented Language Model Pre-Training	11	640	Google	USA	industry 
+ [ ] Single Path One-Shot Neural Architecture Search with Uniform Sampling	1	627	Hong Kong University of Science and Technology, Megvii, Tsinghua University	China	industry 
+ [ ] Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing	18	602	Microsoft	USA	industry 
+ [ ] Tracking Objects as Points	3	589	Intel, University of Texas at Austin	USA	academia 
+ [ ] Adaptive Federated Optimization	2	588	Google	USA	industry 
+ [ ] Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference	7	581	Ludwig Maximilian University of Munich, Sulzer GmbH	Germany	academia 
+ [ ] Making Pre-trained Language Models Better Few-shot Learners	13	576	MIT, Princeton University	USA	academia 
+ [ ] Recipes for building an open-domain chatbot	12	575	Meta	USA	industry 
+ [ ] Meshed-Memory Transformer for Image Captioning		572	University of Modena and Reggio Emilia	Italy	academia 
+ [ ] Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training.		568	Microsoft, Peking University	China, USA	industry 
+ [ ] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention	17	562	École Polytechnique Fédérale de Lausanne, Idiap Research Institute, University of Geneva, University of Washington	Switzerland, USA	academia 
+ [ ] Learning to Simulate Complex Physics with Graph Networks	46	547	DeepMind, Stanford University	UK, USA	industry 
+ [ ] Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment.		546	Agency for Science, Technology and Research, MIT, University of Hong Kong	China, Singapore, USA	academia 
+ [ ] XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization	1	536	Carnegie Mellon University, DeepMind, Google	UK, USA	industry 
+ [ ] On Adaptive Attacks to Adversarial Example Defenses	3	535	Google, MIT, Stanford University, University of Tubingen	Germany, USA	academia 
+ [ ] Circle Loss: A Unified Perspective of Pair Similarity Optimization		534	Australian National University, Beihang University, Megvii, Tsinghua University	Australia, China	industry 
+ [ ] Explaining machine learning classifiers through diverse counterfactual explanations.		527	Microsoft, University of Colorado Boulder	USA	academia 
+ [ ] Efficient Transformers: A Survey	355	524	Google	USA	industry 
+ [ ] Exploring Self-attention for Image Recognition		523	Chinese University of Hong Kong, Intel
+ [ ] [ML][CMU] Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB, https://arxiv.org/abs/2307.07539
+ [ ] [DeepMind] AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models, https://academic.oup.com/nar/article/50/D1/D439/6430488
+ [ ] [DeepMind] Highly accurate protein structure prediction with AlphaFold, https://www.nature.com/articles/s41586-021-03819-2
+ [ ] [Protein] ColabFold: making protein folding accessible to all, https://www.nature.com/articles/s41592-022-01488-1
+ [ ] [Protein] Accurate prediction of protein structures and interactions using a three-track neural network, https://www.science.org/doi/epdf/10.1126/science.abj8754
+ [ ] [OpenAI] Hierarchical Text-Conditional Image Generation with CLIP Latents, https://arxiv.org/abs/2204.06125
+ [ ] [OpenAI] Learning Transferable Visual Models From Natural Language Supervision, https://arxiv.org/abs/2103.00020
+ [ ] [NVIDIA] NVIDIA Deep Learning Examples for Tensor Cores, https://github.com/NVIDIA/DeepLearningExamples
+ [ ] [Google] Google Research Publication database, https://research.google/pubs/
+ [ ] [Google] PaLM: Scaling Language Modeling with Pathways, https://arxiv.org/abs/2204.02311
+ [ ] [Google] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, https://arxiv.org/abs/1910.10683
+ [ ] Bootstrap your own latent: A new approach to self-supervised Learning, https://arxiv.org/abs/2006.07733
+ [ ] [Meta] Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering, https://virtual2023.aclweb.org/paper_P452.html?utm_source=linkedin&utm_medium=organic_social&utm_campaign=acl2023&utm_content=image#paper
+ [ ] [Meta] Meta-training with Demonstration Retrieval for Efficient Few-shot Learning, https://arxiv.org/abs/2307.00119
+ [ ] [Meta] UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units, https://arxiv.org/abs/2212.08055
+ [ ] [U. Washington] On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? https://dl.acm.org/doi/10.1145/3442188.3445922
+ [ ] [CV] Fast Segment Anything, https://github.com/CASIA-IVA-Lab/FastSAM
+ [ ] [Knowledge Graph] Generating Faithful Text From a Knowledge Graph with Noisy Reference Text, https://arxiv.org/abs/2308.06488
+ [ ] [Knowledge Distillation] Knowledge Distillation: A Survey, https://arxiv.org/abs/2006.05525
+ [ ] [Knowledge Distillation] Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes, https://arxiv.org/abs/2305.02301
+ [ ] [SLAM] GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting, https://arxiv.org/abs/2311.11700
+ [ ] [SLAM] DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras, https://arxiv.org/abs/2108.10869, https://github.com/princeton-vl/DROID-SLAM
+ [ ] [Model Compression] Model Compression for Deep Neural Networks: A Survey
+ [ ] [Model Compression] A Survey on Deep Neural Network Compression: Challenges, Overview, and Solutions, https://arxiv.org/abs/2010.03954, https://paperswithcode.com/paper/a-survey-on-deep-neural-network-compression/review/
+ [ ] [Model Compression] A Survey of Model Compression and Acceleration for Deep Neural Networks, https://arxiv.org/abs/1710.09282
+ [ ] [Model Compression] A comprehensive survey on model compression and acceleration, https://link.springer.com/article/10.1007/s10462-020-09816-7
+ [ ] [Model Compression] Deep neural networks compression: A comparative survey and choice recommendations, https://www.sciencedirect.com/science/article/pii/S0925231222014643
+ [ ] [Model Compression] Model-Compression-Papers, https://github.com/chester256/Model-Compression-Papers
+ [ ] [Model Compression] Compression of Deep Learning Models for Text: A Survey, https://dl.acm.org/doi/full/10.1145/3487045
+ [ ] [Model Compression] Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey, https://ieeexplore.ieee.org/document/9043731
+ [ ] [Model Compression] Deep model compression for mobile platforms: A survey, https://ieeexplore.ieee.org/document/8727762
+ [ ] [Model Compression] DEEPCOMPRESSION:COMPRESSINGDEEPNEURAL NETWORKSWITHPRUNING,TRAINEDQUANTIZATION ANDHUFFMANCODING, https://arxiv.org/pdf/1510.00149)
+ [ ] [GNN][Time Series] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection, https://arxiv.org/abs/2307.03759
+ [ ] [Time Series] Explaining Time Series via Contrastive and Locally Sparse Perturbations, https://arxiv.org/abs/2401.08552, https://github.com/zichuan-liu/contralsp
+ [ ] [Time Series] TSMixer: An all-MLP architecture for time series forecasting, https://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html
+ [ ] [Time Series] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection, https://arxiv.org/abs/2307.03759
+ [ ] [Time Series] MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns, https://arxiv.org/abs/2107.13462, https://nixtla.github.io/statsforecast/src/core/models.html#mstl
+ [ ] [RL] [Pinterest] Large-scale Reinforcement Learning for Diffusion Models, https://huggingface.co/papers/2401.12244
+ [ ] [RL] Pearl: A Production-ready Reinforcement Learning Agent, https://arxiv.org/abs/2312.03814, https://github.com/facebookresearch/pearl, https://pearlagent.github.io
+ [ ] [RL] Champion-level drone racing using deep reinforcement learning, https://www.nature.com/articles/s41586-023-06419-4
+ [ ] [RL] AnIntroductiontoDeep ReinforcementLearning, https://www.ics.uci.edu/~dechter/courses/ics-295/fall-2019/texts/An_Introduction_to_Deep_Reinforcement_Learning.pdf
+ [ ] [RL] Offline Reinforcement Learning with Implicit Q-Learning, https://arxiv.org/abs/2110.06169
+ [ ] [RL] Offline Reinforcement Learning with Implicit Q-Learning, https://arxiv.org/abs/2110.06169
+ [ ] [RL] [DeepMind] DeepNash learns to play Stratego from scratch by combining game theory and model-free deep RL, https://www.deepmind.com/blog/mastering-stratego-the-classic-game-of-imperfect-information?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-_KWnjJMok4p7lxxBJgIa1f-qA07jSWCt3GXAPWGm9rOoVgsUvfcPNMlM5GrOOOH3knocHJ
+ [ ] [RL] Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback, https://arxiv.org/abs/2307.15217
+ [ ] [RL] Understanding Self-Predictive Learning for Reinforcement Learning, https://arxiv.org/abs/2212.03319
+ [ ] [RL] Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning, https://arxiv.org/abs/2109.11978
+ [ ] [RL] Course in Deep Reinforcement Learning, https://github.com/andri27-ts/Reinforcement-Learning
+ [ ] [NLP] TEXT SUMMARIZATION WITH BART LARGE ON IPU, https://www.graphcore.ai/posts/text-summarization-with-bart-large-on-ipu?utm_content=260099192&utm_medium=social&utm_source=linkedin&hss_channel=lcp-10812092
+ [ ] [NLP] Lost in the Middle: How Language Models Use Long Contexts, https://arxiv.org/abs/2307.03172
+ [ ] [NLP] FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets, https://arxiv.org/abs/2307.10928
+ [ ] [NLP] Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models, https://arxiv.org/abs/2307.11224
+ [ ] [CV] Generating Images with Multimodal Language Models, https://arxiv.org/abs/2305.17216
+ [ ] [MLP] Scaling MLPs: A Tale of Inductive Bias, https://arxiv.org/abs/2306.13575
+ [ ] Scenic is developed in JAX and uses Flax., https://github.com/google-research/scenic
+ [ ] [Transformers] Stanford CS25 - Transformers United, https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM, https://web.stanford.edu/class/cs25/
+ [ ] [Transformers] Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction, https://arxiv.org/abs/2402.04154
+ [ ] [Transformers] Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers, https://arxiv.org/abs/2401.11605
+ [ ] [Transformers] Masked Autoencoders Are Scalable Vision Learners, https://arxiv.org/abs/2111.06377
+ [ ] [Transformers] Toolformer: Language Models Can Teach Themselves to Use Tools, https://arxiv.org/abs/2302.04761
+ [ ] [Transformers] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, https://arxiv.org/abs/2101.03961,, https://magazine.sebastianraschka.com/p/ai-and-open-source-in-2023
+ [ ] [Transformers] Simplifying Transformer Blocks, https://arxiv.org/abs/2311.01906
+ [ ] [Transformers] Simplifying Transformer Blocks, https://arxiv.org/abs/2311.01906
+ [ ] [Transformers] Nougat: Neural Optical Understanding for Academic Documents, https://arxiv.org/abs/2308.13418, https://github.com/facebookresearch/nougat, https://facebookresearch.github.io/nougat/
+ [ ] [Transformers][FFN] One Wide Feedforward is All You Need, https://arxiv.org/abs/2309.01826
+ [ ] [Transformers] UForm v2: now in 21 languages, https://www.unum.cloud/blog/2023-08-17-uform-graphcore, https://www.unum.cloud/blog/2023-02-20-efficient-multimodality
+ [ ] [Transformers] CoTracker: It is Better to Track Together, https://arxiv.org/abs/2307.07635, https://github.com/facebookresearch/co-tracker, https://huggingface.co/spaces/facebook/cotracker
+ [ ] [Transformers] Fine-grained Audible Video Description, https://arxiv.org/abs/2303.15616
+ [ ] [Transformers] 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment, https://arxiv.org/abs/2308.04352, https://3d-vista.github.io
+ [ ] [Transformers] Scaling Vision Transformers to 22 Billion Parameters, https://arxiv.org/abs/2302.05442
+ [ ] [Transformers] Teaching Arithmetic to Small Transformers, https://arxiv.org/abs/2307.03381
+ [ ] [Transformers] Explainable AI: Visualizing Attention in Transformers,  And logging the results in an experiment tracking tool, https://generativeai.pub/explainable-ai-visualizing-attention-in-transformers-4eb931a2c0f8
+ [ ] [Transformers] Attending to Graph Transformers, https://arxiv.org/abs/2302.04181
+ [ ] [Transformers] Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition, https://arxiv.org/abs/2307.07421
+ [ ] [Transformers] Trainable Transformer in Transformer, Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora, https://arxiv.org/abs/2307.01189
+ [ ] [Transformers] Compositional Attention: Disentangling Search and Retrieval, https://arxiv.org/abs/2110.09419
+ [ ] [Transformers] The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning, https://attentionneuron.github.io
+ [ ] [Transformers] Awesome NLP Paper Discussions, https://github.com/huggingface/awesome-papers#planned-discussions
+ [ ] [Transformers] Compositional Attention: Disentangling Search and Retrieval, https://arxiv.org/abs/2110.09419
+ [ ] [Transformers] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, https://arxiv.org/pdf/2205.14135.pdf
+ [ ] [Transformers] Attention is not Explanation, https://arxiv.org/pdf/1902.10186.pdf
+ [ ] [Transformers] Ultimate-Awesome-Transformer-Attention, https://github.com/cmhungsteve/Awesome-Transformer-Attention
+ [ ] [Transformers] OptFormer: Towards Universal Hyperparameter Optimization with Transformers, https://ai.googleblog.com/2022/08/optformer-towards-universal.html
+ [ ] [Transformers] Scaling Transformer to 1M tokens and beyond with RMT, https://arxiv.org/abs/2304.11062
+ [ ] [Transformers] Formal Algorithms for Transformers, https://arxiv.org/abs/2207.09238
+ [ ] [Transformers] DeepNet: Scaling Transformers to 1,000 Layers, https://arxiv.org/abs/2203.00555
+ [ ] [Transformers] MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER, https://arxiv.org/pdf/2110.02178.pdf
+ [ ] [Transformers] Mobile-Former: Bridging MobileNet and Transformer, https://arxiv.org/abs/2108.05895
+ [ ] [Transformers] SCENIC: A JAX Library for Computer Vision Research and Beyond, https://arxiv.org/pdf/2110.11403.pdf
+ [ ] [Transformers] Recipe for a General, Powerful, Scalable Graph Transformer, https://arxiv.org/abs/2205.12454
+ [ ] [Transformers] Decision Transformer: Reinforcement Learning via Sequence Modeling, https://arxiv.org/abs/2106.01345
+ [ ] [Transformers] Attention Bottlenecks for Multimodal Fusion, https://arxiv.org/pdf/2107.00135.pdf
+ [ ] [Transformers] Multimodal Transformer for Unaligned Multimodal Language Sequences, https://arxiv.org/pdf/1906.00295.pdf
+ [ ] [Transformers] Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case, https://arxiv.org/abs/2001.08317
+ [ ] [Transformers] Multivariate Time Series Forecasting with Transformers, https://towardsdatascience.com/multivariate-time-series-forecasting-with-transformers-384dc6ce989b
+ [ ] [Transformers] Transformers Can Do Bayesian Inference, https://arxiv.org/abs/2112.10510
+ [ ] [Transformers] Transformer Quality in Linear Time, https://arxiv.org/pdf/2202.10447.pdf
+ [ ] [Transformers] HyperMixer: An MLP-based Green AI Alternative to Transformers, https://arxiv.org/abs/2203.03691
+ [ ] [Transformers] Bringing Old Films Back to Life, https://arxiv.org/pdf/2203.17276.pdf
+ [ ] [Transformers] Specformer: Spectral Graph Neural Networks Meet Transformers, https://arxiv.org/abs/2303.01028
+ [ ] [Transformers] Self-Attention with Relative Position Representations, https://arxiv.org/abs/1803.02155
+ [ ] [Transformers] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ [ ] [Transformers] TransformerFusion: Monocular RGB Scene Reconstruction using Transformers, https://arxiv.org/abs/2107.02191
+ [ ] [Transformers] BumbleBee: A Transformer for Music, https://arxiv.org/abs/2107.03443
+ [ ] [Transformers] Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation, https://github.com/NVIDIA-Merlin/publications/tree/main/2021_acm_recsys_transformers4rec
+ [ ] [Transformers] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
+ [ ] [Transformers] Thinking Like Transformers, https://arxiv.org/abs/2106.06981
+ [ ] [Transformers] FNet: Mixing Tokens with Fourier Transforms, https://arxiv.org/abs/2105.03824
+ [ ] [Transformers] Convolutional Xformers for Vision, https://arxiv.org/abs/2201.10271
+ [ ] [Transformers] Transformer Memory as a Differentiable Search Index, https://arxiv.org/abs/2202.06991
+ [ ] [Transformers] Flat-Lattice-Transformer, https://github.com/LeeSureman/Flat-Lattice-Transformer
+ [ ] [Transformers] When an Image is Worth 1,024 x 1,024 Words: A Case Study in Computational Pathology, https://arxiv.org/abs/2312.03558
+ [ ] [Transformers] MobileViT: A mobile-friendly Transformer-based model for image classification, https://keras.io/examples/vision/mobilevit/
+ [ ] [Transformers] Vision Transformers (ViTs)
+ [ ] [Transformers][Google] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ [ ] [Transformers][Microsoft] Swin Transformer, v1, v2, Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, https://arxiv.org/abs/2103.14030
+ [ ] [Transformers][Meta] Emerging Properties in Self-Supervised Vision Transformers, https://arxiv.org/abs/2104.14294
+ [ ] [Transformers] DeiT, DeiT III: Revenge of the ViT, https://arxiv.org/abs/2204.07118
+ [ ] [Transformers] Levenshtein Transformer, https://arxiv.org/pdf/1905.11006.pdf
+ [ ] [Transformers] [Microsoft] Unveiling Transformers with LEGO: a synthetic reasoning task, https://arxiv.org/abs/2206.04301
+ [ ] [Transformers] [Google] Frequency Effects on Syntactic Rule Learning in Transformers, https://arxiv.org/pdf/2109.07020.pdf
+ [ ] [Transformers] [Google] Multi-Game Decision Transformers, https://sites.google.com/view/multi-game-transformers
+ [ ] [Transformers] EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything, https://arxiv.org/abs/2312.00863, https://yformer.github.io/efficient-sam/
+ [ ] [Transformers][Survey] An Impartial Take to the CNN vs Transformer Robustness Contest, https://arxiv.org/pdf/2207.11347.pdf
+ [ ] [Transformers][Survey] A Survey of Transformers, https://arxiv.org/abs/2106.04554
+ [ ] [Transformers][Survey] Transformers in Vision: A Survey, https://arxiv.org/abs/2101.01169
+ [ ] [Transformers][Survey] A Survey on Vision Transformer, https://arxiv.org/abs/2012.12556
+ [ ] [Transformers][Survey] A Survey of Visual Transformers, https://arxiv.org/abs/2111.06091
+ [ ] [Transformers][Survey] 2021-A Survey of Transformers, https://www.jianshu.com/p/98d82f83b6ba
+ [ ] [Transformers][Survey] AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, https://arxiv.org/abs/2108.05542
+ [ ] [Transformers][Survey] Efficient Transformers: A Survey, https://arxiv.org/pdf/2009.06732.pdf
+ [ ] [Transformers][Survey] Pure Transformers are Powerful Graph Learners, https://arxiv.org/abs/2207.02505
+ [ ] [Transformers][HuggingFace] ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
+ [ ] [Transformers][HuggingFace] ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.
+ [ ] [Transformers][HuggingFace] AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.
+ [ ] [Transformers][HuggingFace] Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.
+ [ ] [Transformers][HuggingFace] Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.
+ [ ] [Transformers][HuggingFace] BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.
+ [ ] [Transformers][HuggingFace] BARThez (from École polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.
+ [ ] [Transformers][HuggingFace] BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.
+ [ ] [Transformers][HuggingFace] BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.
+ [ ] [Transformers] Transformers: from NLP to CV, https://github.com/IbrahimSobh/Transformers
+ [ ] [Transformers] Generating Molecular Conformer Fields, https://arxiv.org/abs/2311.17932
+ [ ] [CNN][Codes] Convolutional Neural Networks for Text, https://lena-voita.github.io/nlp_course/models/convolutional.html
+ [ ] [CNN][Paper] LeNet-5, GradientBased Learning Applied to Document Recognition, http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
+ [ ] [CNN][Paper] AlexNet, ImageNet Classification with Deep Convolutional Neural Networks, https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
+ [ ] [CNN][Paper] VGG, Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556
+ [ ] [VGG] VGG Introduced by Simonyan et al. in Very Deep Convolutional Networks for Large-Scale Image Recognition, https://paperswithcode.com/method/vgg
+ [ ] [CNN][Paper] Networks in Networks and 1x1 Convolutions, Network In Network, https://arxiv.org/abs/1312.4400
+ [ ] [CNN][Paper] Inception Network Motivation, Going Deeper with Convolutions, https://arxiv.org/abs/1409.4842
+ [ ] [CNN][Paper] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, https://arxiv.org/abs/1704.04861
+ [ ] [CNN][Paper] MobileNetV2: Inverted Residuals and Linear Bottlenecks, https://arxiv.org/abs/1801.04381
+ [ ] [CNN][Paper] Searching for MobileNetV3, https://arxiv.org/abs/1905.02244
+ [ ] [CNN][Paper] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, https://arxiv.org/abs/1905.11946
+ [ ] [CNN][Codes] MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, https://github.com/facebookresearch/moco
+ [ ] [CNN][Codes] Masked Autoencoders: A PyTorch Implementation, https://github.com/facebookresearch/mae
+ [ ] [CNN][Codes] Detectron2, https://github.com/facebookresearch/detectron2
+ [ ] [CNN][Paper] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, https://arxiv.org/abs/1312.6229
+ [ ] [CNN][Course] OverFeat Integrated    Recogni.on,    Localiza.on    and    Detec.on    using Convolu.onal    Networks Sermanet    et.    al Presenta.on    by    Eric    Holmdahl, http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf
+ [ ] [CNN][Paper] Ultralytics YOLOv8 Turns One: A Year of Breakthroughs and Innovations, https://www.ultralytics.com/blog/ultralytics-yolov8-turns-one-a-year-of-breakthroughs-and-innovations
+ [ ] [CNN][Paper] A Comprehensive Review of YOLO: From YOLOv1 and Beyond, https://arxiv.org/abs/2304.00501
+ [ ] [CNN][Paper] You Only Look Once: Unified, Real-Time Object Detection, https://arxiv.org/abs/1506.02640
+ [ ] [CNN][Paper] A Comprehensive Review of YOLO: From YOLOv1 to YOLOv8 and Beyond, https://arxiv.org/abs/2304.00501
+ [ ] [CNN][Paper] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,https://arxiv.org/abs/2207.02696
+ [ ] [CNN][Paper] YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications, https://arxiv.org/abs/2209.02976
+ [ ] [CNN][Codes] YOLOv5 🚀 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite., https://pytorch.org/hub/ultralytics_yolov5/#:~:text=YOLOv5%20🚀%20is%20a%20family,Model
+ [ ] [CNN][Paper] YOLOv4: Optimal Speed and Accuracy of Object Detection, https://arxiv.org/abs/2004.10934
+ [ ] [CNN][Paper] YOLOv3: An Incremental Improvement, https://arxiv.org/abs/1804.02767
+ [ ] [CNN][Paper] YOLO9000: Better, Faster, Stronger, https://arxiv.org/abs/1612.08242
+ [ ] [CNN][Blog] YOLO Object Detection Explained, https://www.datacamp.com/blog/yolo-object-detection-explained
+ [ ] [CNN][Paper] R-CNN, 2013 rich feature hierarchies for accurate object detection and semantic segmentation, https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf, https://ieeexplore.ieee.org/document/6909475
+ [ ] [CNN][Blog] Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5), https://www.arxiv-vanity.com/papers/1311.2524/
+ [ ] [CNN][Paper] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, https://arxiv.org/abs/1506.01497
+ [ ] [CNN][Codes] Faster R-CNN, https://paperswithcode.com/method/faster-r-cnn
+ [ ] [CNN][Paper] Fast R-CNN
+ [ ] [CNN][Paper] RFCN
+ [ ] [CNN][Paper] Mask RCNN
+ [ ] [CNN][Paper] SSD
+ [ ] [CNN][Paper] U-Net: Convolutional Networks for Biomedical Image Segmentation, https://arxiv.org/abs/1505.04597
+ [ ] [CNN][Paper] U-Net, https://paperswithcode.com/method/u-net
+ [ ] [CNN][Blog] Understand Semantic segmentation with the Fully Convolutional Network U-Net step-by-step, https://pallawi-ds.medium.com/understand-semantic-segmentation-with-the-fully-convolutional-network-u-net-step-by-step-9d287b12c852
+ [ ] [CNN][Paper] Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs, https://arxiv.org/abs/1701.08816 
+ [ ] [CNN][Paper] Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks, https://arxiv.org/abs/1705.03820
+ [ ] [CNN][Paper] DeepFace: Closing the Gap to Human-Level Performance in Face Verification, https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf, https://ranzato.github.io
+ [ ] [CNN][Paper] FaceNet: A Unified Embedding for Face Recognition and Clustering, https://arxiv.org/abs/1503.03832
+ [ ] [CNN][Paper] Visualizing and Understanding Convolutional Networks, https://arxiv.org/abs/1311.2901
+ [ ] [CNN][Paper] A Neural Algorithm of Artistic Style, https://arxiv.org/abs/1508.06576
+ [ ] [CNN][Codes] SAM (Segment Anything Model), https://huggingface.co/docs/transformers/main/en/model_doc/sam
+ [ ] [CNN][Paper] Hyena Hierarchy: Towards Larger Convolutional Language Models, https://arxiv.org/abs/2302.10866
+ [ ] [CNN][Paper] Going Deeper with Convolutions, https://arxiv.org/abs/1409.4842
+ [ ] CLIP: Connecting text and images, https://openai.com/research/clip, Codes: https://github.com/openai/CLIP; Paper: Learning Transferable Visual Models From Natural Language Supervision, https://arxiv.org/abs/2103.00020
+ [ ] Parrot Captions Teach CLIP to Spot Text,  https://arxiv.org/abs/2312.14232
+ [ ] Image GPT, https://openai.com/research/image-gpt
+ [ ] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard, Zhu, Chen, Kalenichenko, Wang, Weyand, Andreetto, & Adam, 2017), https://arxiv.org/abs/1704.04861, https://github.com/fchollet/deep-learning-with-python-notebooks
+ [ ] MobileNetV2: Inverted Residuals and Linear Bottlenecks (Sandler, Howard, Zhu, Zhmoginov &Chen, 2018), https://arxiv.org/abs/1801.04381
+ [ ] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan & Le, 2019), https://arxiv.org/abs/1905.11946
+ [ ] You Only Look Once: Unified, Real-Time Object Detection (Redmon, Divvala, Girshick & Farhadi, 2015), https://arxiv.org/abs/1506.02640
+ [ ] YOLO9000: Better, Faster, Stronger (Redmon & Farhadi, 2016), https://arxiv.org/abs/1612.08242
+ [ ] YAD2K (GitHub: allanzelener), https://github.com/allanzelener/YAD2K
+ [ ] YOLO: Real-Time Object Detection, https://pjreddie.com/darknet/yolo/
+ [ ] Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs (Novikov, Lenis, Major, Hladůvka, Wimmer & Bühler, 2017), https://arxiv.org/abs/1701.08816
+ [ ] Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks (Dong, Yang, Liu, Mo & Guo, 2017), https://arxiv.org/abs/1705.03820
+ [ ] U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger, Fischer & Brox, 2015), https://arxiv.org/abs/1505.04597
+ [ ] FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff, Kalenichenko & Philbin, 2015), https://arxiv.org/pdf/1503.03832.pdf
+ [ ] DeepFace: Closing the Gap to Human-Level Performance in Face Verification (Taigman, Yang, Ranzato & Wolf), https://scontent.ffjr7-1.fna.fbcdn.net/v/t39.8562-6/240890413_887772915161178_4705912772854439762_n.pdf?_nc_cat=109&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=F5lzaUIsBnAAX8Z7wVC&_nc_ht=scontent.ffjr7-1.fna&oh=00_AfC8DK3sP5sye19oUs9dVjfLPdibMfA-3Z-MuBENbTogqg&oe=645556BF
+ [ ] [FaceNet,DeepFace] facenet (GitHub: davidsandberg), https://github.com/davidsandberg/facenet
+ [ ] How to Develop a Face Recognition System Using FaceNet in Keras (Jason Brownlee, 2019), https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/
+ [ ] keras-facenet/notebook/tf_to_keras.ipynb (GitHub: nyoki-mtl), https://github.com/nyoki-mtl/keras-facenet/blob/master/notebook/tf_to_keras.ipynb
+ [ ] A Neural Algorithm of Artistic Style (Gatys, Ecker & Bethge, 2015), https://arxiv.org/abs/1508.06576
+ [ ] Convolutional neural networks for artistic style transfer, https://harishnarayanan.org/writing/artistic-style-transfer/
+ [ ] TensorFlow Implementation of "A Neural Algorithm of Artistic Style”, http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style
+ [ ] Very Deep Convolutional Networks For Large-Scale Image Recognition (Simonyan & Zisserman, 2015), https://arxiv.org/pdf/1409.1556.pdf
+ [ ] Pretrained models (MatConvNet), https://www.vlfeat.org/matconvnet/pretrained/
+ [ ] DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation, https://arxiv.org/abs//2306.12422
+ [ ] [ML] [Course] Introduction to Embedded Machine Learning, https://www.youtube.com/playlist?list=PL7VEa1KauMQqZFj_nWRfsCZNXaBbkuurG
+ [ ] [GAN] Scaling up GANs for Text-to-Image Synthesism, https://mingukkang.github.io/GigaGAN/
+ [ ] [GAN] StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces, https://arxiv.org/abs/2303.06146
+ [ ] [Information Theory] Information Theory, Inference, and Learning Algorithms (Hardback, 640 pages, Published September 2003), https://inference.org.uk/itila/book.html
+ [ ] [Diffusion Models] TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering, https://arxiv.org/abs/2311.16465, https://github.com/microsoft/unilm/tree/master/textdiffuser-2, https://huggingface.co/spaces/JingyeChen22/TextDiffuser-2
+ [ ] [Diffusion Models] Core ML Stable Diffusion, https://github.com/apple/ml-stable-diffusion
+ [ ] [Diffusion Models] InstructDiffusion: A Generalist Modeling Interface for Vision Tasks, https://gengzigang.github.io/instructdiffusion.github.io/
+ [ ] [Diffusion Models] Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing, https://arxiv.org/abs/2304.02051, https://github.com/aimagelab/multimodal-garment-designer
+ [ ] [Causal Inference] the Book of Why, by Judea Pearl
+ [ ] [Causal Inference] [Books] "Causal Inference and Discovery in Python by Aleksander Molak” (Python)
+ [ ] [Causal Inference] [Books]  “Causal Inference in Python by Matheus Facure" (Python).
+ [ ] [Nvidia] An Online Deep Learning Interface for HPC programs on NVIDIA GPUs, https://github.com/NVIDIA/TorchFort
+ [ ] [CV] [Samsung] Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction, https://arxiv.org/abs/2306.05872, https://github.com/SamsungLabs/NeuralHaircut, https://samsunglabs.github.io/NeuralHaircut/
+ [ ] [CV] [Object Detection] Follow Anything: Open-set detection, tracking, and following in real-time, https://arxiv.org/abs/2308.05737
+ [ ] [GNN] Graph Neural Networks Foundations, Frontiers, and Applications Lingfei Wu, Pinterest Peng Cui, Tsinghua University Jian Pei, Duke University Liang Zhao, Emory University, https://graph-neural-networks.github.io/index.html
+ [ ] [MT][Meta] Bringing the world closer together with a foundational multimodal model for speech translation, https://ai.meta.com/blog/seamless-m4t/, SeamlessM4T—Massively Multilingual & Multimodal Machine Translation, https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf, SeamlessM4T, https://github.com/facebookresearch/seamless_communication
+ [ ] [MT] Neural Machine Translation by Jointly Learning to Align and Translate, https://arxiv.org/abs/1409.0473
+ [ ] [Speech] Self-Supervised Speech Representation Learning: A Review, https://arxiv.org/abs/2205.10643
+ [ ] [Optimization] Fast as CHITA: Neural Network Pruning with Combinatorial Optimization, https://arxiv.org/abs/2302.14623
+ [ ] [Causal Analysis] Causal analysis with PyMC: Answering "What If?" with the new do operator, https://www.pymc-labs.io/blog-posts/causal-analysis-with-pymc-answering-what-if-with-the-new-do-operator/
+ [ ] [Biases] The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning, https://arxiv.org/abs/2304.05366
+ [ ] [Biases] The Cognitive Biases Compendium: Explore over 150 Cognitive Biases (with examples) to make better decisions, think critically, solve problems effectively, ... accurately. (Arificial Intelligence Book 2) Kindle Edition by Murat Durmus (Author)
+ [ ] [Labels] Labels are not necessary: Assessing peer-review helpfulness using domain adaptation based on self-training, https://aclanthology.org/2023.bea-1.15/
+ [ ] [MLOps] What's New in WebGPU (Chrome 116), https://developer.chrome.com/blog/new-in-webgpu-116/
+ [ ] [MLOps] How to break a model in 20 days. A tutorial on production model analytics, https://www.evidentlyai.com/blog/tutorial-1-model-analytics-in-production
+ [ ] [MLOps] To retrain, or not to retrain? Let's get analytical about ML model updates, https://www.evidentlyai.com/blog/retrain-or-not-retrain
+ [ ] [MLOps] Introducing Evidently 0.0.1 Release: Open-Source Tool To Analyze Data Drift, https://www.evidentlyai.com/blog/evidently-001-open-source-tool-to-analyze-data-drift
+ [ ] [MLOps] Monitoring Machine Learning Models in Production, https://christophergs.com/machine%20learning/2020/03/14/how-to-monitor-machine-learning-models/
+ [ ] [MLOps] [Evaluation] [Evidently AI] Evidently  An open-source framework to evaluate, test and monitor ML models in production, https://github.com/evidentlyai/evidently
+ [x] [MLOps] Machine Learning in Production: Why You Should Care About Data and Concept Drift, https://towardsdatascience.com/machine-learning-in-production-why-you-should-care-about-data-and-concept-drift-d96d0bc907fb
+ [ ] [MLOps] Machine Learning Monitoring, Part 1: What It Is and How It Differs, https://www.evidentlyai.com/blog/machine-learning-monitoring-what-it-is-and-how-it-differs
+ [x] [MLOps] Machine Learning Monitoring, Part 3: What Can Go Wrong With Your Data? https://www.evidentlyai.com/blog/machine-learning-monitoring-what-can-go-wrong-with-your-data
+ [ ] [MLOps] ML Monitoring, https://www.evidentlyai.com/category/ml-monitoring
+ [x] [MLOps] Large Scale Distributed Deep Networks, Jeffrey Dean,
+ [x] [MLOps] Hidden Technical Debt in Machine Learning System, https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf
+ [ ] [MLOps] MLOps Zoomcamp, https://github.com/DataTalksClub/mlops-zoomcamp
+ [ ] [MLOps] MLOps-Basics, https://github.com/graviraja/MLOps-Basics
+ [ ] [MLOps] MLOps Guide, https://mlops-guide.github.io
+ [ ] [MLOps] Awesome MLOps, https://github.com/visenger/awesome-mlops
+ [ ] [MLOps] Awesome MLOps, Awesome MLOps, https://github.com/kelvins/awesome-mlops
+ [ ] [MLOps] Awesome MLOps - Tools, https://github.com/kelvins/awesome-mlops
+ [ ] [MLOps] DTU MLOps, https://github.com/SkafteNicki/dtu_mlops
+ [ ] [MLOps] MLOps Course, https://github.com/GokuMohandas/mlops-course
+ [ ] [MLOps] MLOps-Basics, https://github.com/graviraja/MLOps-Basics/tree/main
+ [ ] [Optimization] Dropout: A Simple Way to Prevent Neural Networks from Overfitting, https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf,https://paperswithcode.com/method/dropout
+ [ ] [Optimization] Dropout is NOT All You Need to Prevent Gradient Leakage, https://arxiv.org/abs/2208.06163
+ [ ] Modified Dropout for Training Neural Network, https://www.cs.cmu.edu/~epxing/Class/10715/project-reports/DuyckLeeLei.pdf
+ [ ] ML Papers Explained, https://github.com/dair-ai/ML-Papers-Explained
+ [ ] [CV] [Semantic Segmentation] Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation, https://arxiv.org/abs/2211.14512, https://github.com/yyliu01/RPL
+ [ ] [Multi Agent] AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation, https://arxiv.org/abs/2308.08155, AutoGen - Automated Multi Agent Chat, https://microsoft.github.io/autogen/docs/Examples/AutoGen-AgentChat/
+ [ ] [DM] ICDM 2023 Tutorial Robust Time Series Analysis and Applications: A Interdisciplinary Approach The 23rd IEEE International Conference on Data Mining (ICDM'23) Dec. 2023, Shanghai, China, https://sites.google.com/view/timeseries-tutorial-aaai2023/home
+ [ ] [DM] Anomaly Detection under Distribution Shift, https://arxiv.org/abs/2303.13845, https://github.com/mala-lab/ADShift
+ [ ] [Misc] Decaf: Monocular Deformation Capture for Face and Hand Interactions, https://arxiv.org/abs/2309.16670, https://vcai.mpi-inf.mpg.de/projects/Decaf/
+ [ ] [Misc] Solution multiplicity and effects of data and eddy viscosity on Navier-Stokes solutions inferred by physics-informed neural networks, https://arxiv.org/abs/2309.06010, https://github.com/Scien42/NSFnet
+ [ ] [Misc] https://open.spotify.com/episode/6S3TPytV81NWmkDArApKYl
+ [ ] [Misc] IJCAI23 Tutorial: Deep non-IID learning, https://datasciences.org/news/ijcai-tutorial-deep-non-iid-learning/
+ [ ] [Misc] Tracking Anything with Decoupled Video Segmentation, https://arxiv.org/abs/2309.03903v1, https://github.com/hkchengrex/Tracking-Anything-with-DEVA, https://hkchengrex.com/Tracking-Anything-with-DEVA/
+ [ ] [Misc] Fast Feedforward Networks, https://arxiv.org/abs/2308.14711
+ [ ] [Misc] Enhancing Land Cover Mapping and Monitoring: An Interactive and Explainable Machine Learning Approach Using Google Earth Engine, https://www.mdpi.com/2072-4292/15/18/4585, https://github.com/GeoAIR-lab/XAI-tool4GEE
+ [ ] [Misc] Deep Imitation Learning for Humanoid Loco-manipulation through Human Teleoperation, https://arxiv.org/abs/2309.01952, https://ut-austin-rpl.github.io/TRILL/, https://github.com/UT-Austin-RPL/TRILL
+ [ ] [Misc] 4K4D: Real-Time 4D View Synthesis at 4K Resolution,https://zju3dv.github.io/4k4d/
+ [ ] [Embodied Agent] MineDojo, MineDojo is a new framework built on the popular Minecraft game for embodied agent research.,  https://minedojo.org
+ [ ] [Autonomous Driving] LINGO-1: Exploring Natural Language for Autonomous Driving, https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/
+ [ ] [ML] DSG: An End-to-End Document Structure Generator, https://arxiv.org/abs/2310.09118v1
+ [ ] [CV] Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding, https://arxiv.org/abs/2402.16739
+ [ ] [CV] Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation, https://arxiv.org/abs/2305.09662
+ [ ] [CV] Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning, https://arxiv.org/abs/2212.14532
+ [ ] [CV] NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection, https://arxiv.org/abs/2307.14620
+ [ ] [CV] The Stable Signature: Rooting Watermarks in Latent Diffusion Models, https://arxiv.org/abs/2303.15435
+ [ ] [CV] Diffusion Models as Masked Autoencoders, https://arxiv.org/abs/2304.03283
+ [ ] [CV] FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation, https://arxiv.org/abs/2307.01492, https://github.com/NVlabs/FB-BEV
+ [ ] [CV][Evaluation] Evaluating the fairness of computer vision models, https://ai.meta.com/blog/dinov2-facet-computer-vision-fairness-evaluation/
+ [ ] [CV][Evaluation] LightGlue: Local Feature Matching at Light Speed, https://arxiv.org/abs/2306.13643, https://github.com/cvg/LightGlue
+ [ ] [CV] AG3D: Learning to Generate 3D Avatars from 2D Image Collections, https://zj-dong.github.io/AG3D/assets/paper.pdf, https://zj-dong.github.io/AG3D/
+ [ ] [DL] [Initialization] Initializing Models with Larger Ones, https://arxiv.org/abs/2311.18823
+ [ ] [DL] [Book] Understanding Deep Learning, https://udlbook.github.io/udlbook/
+ [ ] [DL] Copy Suppression: Comprehensively Understanding an Attention Head, https://arxiv.org/abs/2310.04625
+ [ ] [HPC] Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models, https://arxiv.org/abs/2311.04386
+ [ ] [AI] [Operation Theory] Nash Learning from Human Feedback, https://misovalko.github.io/publications/munos2024nash.pdf
+ [ ] [AI] GAIA: a benchmark for General AI Assistants, https://arxiv.org/abs/2311.12983
+ [ ] [Image Generation] LEDITS++: Limitless Image Editing using Text-to-Image Models, https://arxiv.org/abs/2311.16711
+ [ ] [Music Generation] Simple and Controllable Music Generation, https://arxiv.org/abs/2306.05284
+ [ ] [Mixture-of-Experts (MoE)] [RL] Mixtures of Experts Unlock Parameter Scaling for Deep RL, https://arxiv.org/abs/2402.08609
+ [ ] [Mixture-of-Experts (MoE)] Mixtral of Experts, https://arxiv.org/abs/2401.04088
+ [ ] [Mixture-of-Experts (MoE)] AppAgent: Multimodal Agents as Smartphone Users, https://arxiv.org/abs/2312.13771
+ [ ] [Mixture-of-Experts (MoE)] The Sparsely Gated Mixture of Experts Layer for PyTorch, https://github.com/davidmrau/mixture-of-experts
+ [ ] [Mixture-of-Experts (MoE)] Stanford CS25: V1 I Mixture of Experts (MoE) paradigm and the Switch Transformer, https://www.youtube.com/watch?v=U8J32Z3qV8s
+ [ ] [Mixture-of-Experts (MoE)] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, https://arxiv.org/abs/1701.06538
+ [ ] [Mixture-of-Experts (MoE)] GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, https://arxiv.org/abs/2006.16668
+ [ ] [Mixture-of-Experts (MoE)] MegaBlocks: Efficient Sparse Training with Mixture-of-Experts, https://arxiv.org/abs/2211.15841
+ [ ] [Mixture-of-Experts (MoE)] Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models, https://arxiv.org/abs/2305.14705
+ [ ] [Mixture-of-Experts (MoE)] Learning Factored Representations in a Deep Mixture of Experts, https://arxiv.org/abs/1312.431i4
+ [ ] [ML] Reward-Free Curricula for Training Robust World Models, https://arxiv.org/abs/2306.09205
+ [ ] [ML] Assumption violations in causal discovery and the robustness of score matching, https://arxiv.org/abs/2310.13387
+ [ ] [ML] DeeprETA: An ETA Post-processing System at Scale, https://arxiv.org/abs/2206.02127
+ [ ] [ML] Vincent Zoonekynd's Blog,http://zoonek.free.fr/blosxom/ML/2024-01-01_2023_in_ML.html
+ [ ] [ML] MODEL BASED MACHINE LEARNING John Winn w/ Christopher M. Bishop, Thomas Diethe, John Guiver & Yordan Zaykov, https://www.mbmlbook.com/MBMLbook.pdf
+ [ ] [ML] Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale, https://arxiv.org/abs/2311.08430
+ [ ] [ML] Model scale versus domain knowledge in statistical forecasting of chaotic systems, https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.5.043252
+ [ ] [ML][Optimization] Symbolic Discovery of Optimization Algorithms, https://arxiv.org/abs/2302.06675
+ [ ] [ML][Optimization] Unexpected Improvements to Expected Improvement for Bayesian Optimization, https://arxiv.org/abs/2310.20708
+ [ ] [ML][Course] CS 329S: Machine Learning Systems Design Stanford, Winter 2022, https://stanford-cs329s.github.io, https://www.youtube.com/playlist?app=desktop&list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy&cbrd=1
+ [ ] [DL] Daily Papers, https://huggingface.co/papers
+ [ ] [DL] Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives, https://arxiv.org/abs/2311.18259
+ [ ] [DL] paper pages from Ahsen Khaliq, https://www.linkedin.com/in/ahsenkhaliq/
+ [ ] [CV] Local Feature Matching Using Deep Learning: A Survey, https://arxiv.org/abs/2401.17592
+ [ ] [CV] [Alibaba] ReplaceAnything as you want: Ultra-high quality content replacement, https://huggingface.co/spaces/modelscope/ReplaceAnything
+ [ ] [CV] InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes, https://arxiv.org/abs/2401.05335
+ [ ] [CV] Boundary Attention: Learning to Find Faint Boundaries at Any Resolution, https://arxiv.org/abs/2401.00935
+ [ ] [CV] RubiksNet Learnable 3D-Shift for Efficient Video Action Recognition, https://stanfordvl.github.io/rubiksnet-site/
+ [ ] [CV] Pinpointing Why Object Recognition Performance Degrades Across Income Levels and Geographies, https://arxiv.org/abs/2304.05391
+ [ ] [CV] EgoEnv: Human-centric environment representations from egocentric video, https://arxiv.org/abs/2207.11365
+ [ ] [Representation Learning] Matryoshka Representation Learning, https://arxiv.org/abs/2205.13147
+ [ ] [Meta Learning] Learning Universal Predictors, https://huggingface.co/papers/2401.14953
+ [AI] [Health] Combinatorial prediction of therapeutic perturbations using causally-inspired neural networks, https://www.biorxiv.org/content/10.1101/2024.01.03.573985v1, https://zitniklab.hms.harvard.edu/projects/PDGrapher/
+ [AI] [Blog] From Zero to Research Scientist full resources guide, https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide
+ [Mathematics] DeepOnet Based Preconditioning Strategies For Solving Parametric Linear Systems of Equations, https://arxiv.org/abs/2401.02016
+ [NVIDIA] Neuralangelo: High-Fidelity Neural Surface Reconstruction, https://research.nvidia.com/labs/dir/neuralangelo/
+ [NVIDIA] Magic3D: High-Resolution Text-to-3D Content Creation, https://research.nvidia.com/labs/dir/magic3d/
+ [NVIDIA] Interactive Hair Simulation on the GPU Using ADMM, https://research.nvidia.com/publication/2023-08_interactive-hair-simulation-gpu-using-admm
+ [NVIDIA] Eureka: Human-Level Reward Design via Coding Large Language Models, https://eureka-research.github.io
+ [NVIDIA] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models, https://research.nvidia.com/labs/toronto-ai/VideoLDM/
+ [NVIDIA] A Textured Approach: NVIDIA Research Shows How Gen AI Helps Create and Edit Photorealistic Materials, https://blogs.nvidia.com/blog/siggraph-research-generative-ai-materials-3d-scenes/
+ [NVIDIA] CALM: Conditional Adversarial Latent Models for Directable Virtual Characters, https://research.nvidia.com/labs/par/calm/
+ [NVIDIA] Learning Physically Simulated Tennis Skills from Broadcast Videos, https://research.nvidia.com/labs/toronto-ai/vid2player3d/
+ [NVIDIA] Flexible Isosurface Extraction for Gradient-Based Mesh Optimization, https://research.nvidia.com/labs/toronto-ai/flexicubes/
+ [NVIDIA] eDiff-I: Text-to-Image Diffusion Models with Ensemble of Expert Denoisers, https://research.nvidia.com/labs/dir/eDiff-I/
+ [ ] [Dataset Bias] Industry Applications of Class Imbalance Techniques: A Chapter-wise Overview, https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/blob/main/industry-applications-imbalanced-data.md
+ [ ] [Sound] Incremental FastPitch: Chunk-based High Quality Text to Speech, https://arxiv.org/abs/2401.01755
+ [ ] [Generative AI][2023] Most Impactful Generative AI Papers of 2023, https://aisupremacy.substack.com/p/most-impactful-generative-ai-papers
+ [ ] [NeurIPS][2023] https://ai.meta.com/events/neurips-2023/
+ [ ] [PyTorch] [Normalization] Layer Normalization, https://paperswithcode.com/method/layer-normalization
+ [ ] [PyTorch] [Normalization] Instance Normalization, https://paperswithcode.com/method/instance-normalization
+ [ ] [PyTorch] [Normalization] Instance Normalization: The Missing Ingredient for Fast Stylization, https://arxiv.org/abs/1607.08022
+ [ ] [PyTorch] [Normalization] Group Normalization, https://arxiv.org/abs/1803.08494
+ [ ] [PyTorch] [Normalization] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, https://arxiv.org/abs/1502.03167
+ [ ] [PyTorch] 3D rotations and spatial transformations made easy with RoMa, https://medium.com/pytorch/3d-rotations-and-spatial-transformations-made-easy-with-roma-356a495a20c4
+ [ ] [PyTorch] Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem, https://pytorch.org/blog/finetune-llms/
+ [ ] [PyTorch] Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond, https://pytorch.org/blog/inside-the-matrix/
+ [ ] [PyTorch] Accelerating Generative AI Part III: Diffusion, Fast, https://pytorch.org/blog/accelerating-generative-ai-3/?utm_content=277363729
+ [ ] [PyTorch] Relational Deep Learning: Graph Representation Learning on Relational Databases, https://arxiv.org/abs/2312.04615
+ [ ] [PyTorch] NVIDIA. 2023. The NVIDIA Collective Communication Library (NCCL). https: //developer.nvidia.com/nccl
+ [ ] [PyTorch] PyTorch distributed training features such as DistributeDataParallel (DDP), Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704 (2020), https://arxiv.org/abs/2006.15704
+ [ ] [PyTorch] Key PyTorch core components including Tensor implemen- tation, dispatcher system, and CUDA memory caching allocator
+ [ ] [PyTorch] Training Neural Networks from Scratch with Parallel Low-Rank Adapters, https://arxiv.org/abs/2402.16828
+ [x] [PyTorch] PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel, https://arxiv.org/abs/2304.11277
+ [x] Attention Is All You Need, https://arxiv.org/abs/1706.03762
+ [x] [CNN][ResNet] ResNet, Deep Residual Learning for Image Recognition, https://arxiv.org/abs/1512.03385, https://paperswithcode.com/method/resnet
+ [ ] [CNN] Convolutional Neural Network From Scratch, https://medium.com/@luuisotorres/convolutional-neural-network-from-scratch-6b1c856e1c07, https://www.kaggle.com/code/lusfernandotorres/convolutional-neural-network-from-scratch/notebook
+ [x] [CNN] A ConvNet for the 2020s, https://arxiv.org/abs/2201.03545, Codes: https://github.com/facebookresearch/ConvNeXt, https://huggingface.co/docs/transformers/main/model_doc/convnext, https://pytorch.org/vision/main/models/convnext.html, https://paperswithcode.com/paper/convnext-v2-co-designing-and-scaling-convnets
+ [x] [CNN][TextCNN] Convolutional Neural Networks for Sentence Classification, https://arxiv.org/abs/1408.5882


# [Papers for Information Retrieval]
+ [ ][IR] Vector Search with OpenAI Embeddings: Lucene Is All You Need, https://arxiv.org/abs/2308.14963

# [Papers for Data Sciences]
+ [Spatial Statistics] Spatial Statistics for Data Science: Theory and Practice with R, https://www.paulamoraga.com/book-spatial/
+ [ ] [Topological Data Analysis (TDA)] Algebraic Topology for Data Scientists, https://arxiv.org/abs/2308.10825

# [Papers to be for Biology]
+ [ ] [Music] Music can be reconstructed from human auditory cortex activity using nonlinear decoding models, https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3002176

# [Papers to be for Reproduction]
+ [Roadmap] Deep Learning Papers Reading Roadmap, https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap

# [Papers to be for Quantum Mechanics]
+ [ ] Two-Worlds Interpretation of Quantum Mechanics, https://arxiv.org/abs/2309.03151

# [Papers to be for Quantum Mechanics]
+ The Information Theory of Aging, https://www.nature.com/articles/s43587-023-00527-6.epdf

## 2023.Jul.04 - for Classical Convolution Neural Networks related Models
+ [CNN] Character-level Convolutional Networks for Text Classification, https://arxiv.org/abs/1509.01626
+ [CNN] A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification, http://arxiv.org/abs/1510.03820
+ [VGG] VGG Introduced by Simonyan et al. in Very Deep Convolutional Networks for Large-Scale Image Recognition, https://paperswithcode.com/method/vgg
+ [CNN][Book] Chapter 9 Convolutional Networks, https://www.deeplearningbook.org/contents/convnets.html
+ [CNN][Standford CS-230] Convolutional Neural Networks cheatsheet, https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#
+ [CNN][Standford] Welcome to the Deep Learning Tutorial! http://ufldl.stanford.edu/tutorial/
+ [CNN][Codes] https://github.com/kaiminghe
+ TART: A plug-and-play Transformer module for task-agnostic reasoning, https://arxiv.org/abs/2306.07536
+ Preference Ranking Optimization for Human Alignment, https://arxiv.org/abs/2306.17492
+ Restart Sampling for Improving Generative Processes, https://arxiv.org/abs/2306.14878
+ Bring Your Own Data! Self-Supervised Evaluation for Large Language Models, https://arxiv.org/abs/2306.13651
+ Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects, https://arxiv.org/abs/2306.10125
+ Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations, https://arxiv.org/abs/2306.09255
+ Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, https://arxiv.org/abs/1911.08265
+ LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering, https://arxiv.org/abs/2305.03403
+ Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation, https://arxiv.org/abs/2306.07954
+ Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow, https://arxiv.org/abs/2306.07209
+ Evaluating the Social Impact of Generative AI Systems in Systems and Society, https://arxiv.org/abs/2306.05949
+ Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models, https://arxiv.org/abs/2306.08997
+ DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection, https://arxiv.org/abs/2306.10347
+ Reinforcement Learning, An Introduction, 2nd Edition, Richard S. Sutton and Andrew G. Barto, http://incompleteideas.net/book/RLbook2020.pdf
+ Towards Integrative AI, Xuedong Huang, Microsoft Corporation, USA
