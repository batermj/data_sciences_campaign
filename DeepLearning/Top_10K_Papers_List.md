Top 10K Papers List

# [Papers to be Read]
+ [Roadmap] Deep Learning Papers Reading Roadmap, https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap
+ [CNN][Codes] MoCo: Momentum Contrast for Unsupervised Visual Representation Learning, https://github.com/facebookresearch/moco
+ [CNN][Codes] Masked Autoencoders: A PyTorch Implementation, https://github.com/facebookresearch/mae
+ [CNN][Codes] Detectron2, https://github.com/facebookresearch/detectron2
+ [CNN][Paper] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, https://arxiv.org/abs/1312.6229
+ [CNN][Course] OverFeat Integrated    Recogni.on,    Localiza.on    and    Detec.on    using Convolu.onal    Networks Sermanet    et.    al Presenta.on    by    Eric    Holmdahl, http://vision.stanford.edu/teaching/cs231b_spring1415/slides/overfeat_eric.pdf
+ [CNN][Paper] You Only Look Once: Unified, Real-Time Object Detection, https://arxiv.org/abs/1506.02640
+ [CNN][Paper] A Comprehensive Review of YOLO: From YOLOv1 to YOLOv8 and Beyond, https://arxiv.org/abs/2304.00501
+ [CNN][Paper] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,https://arxiv.org/abs/2207.02696
+ [CNN][Paper] YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications, https://arxiv.org/abs/2209.02976
+ [CNN][Codes] YOLOv5 üöÄ is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite., https://pytorch.org/hub/ultralytics_yolov5/#:~:text=YOLOv5%20üöÄ%20is%20a%20family,Model
+ [CNN][Paper] YOLOv4: Optimal Speed and Accuracy of Object Detection, https://arxiv.org/abs/2004.10934
+ [CNN][Paper] YOLOv3: An Incremental Improvement, https://arxiv.org/abs/1804.02767
+ [CNN][Paper] YOLO9000: Better, Faster, Stronger, https://arxiv.org/abs/1612.08242
+ [CNN][Blog] YOLO Object Detection Explained, https://www.datacamp.com/blog/yolo-object-detection-explained
+ [CNN][Paper] R-CNN, 2013 rich feature hierarchies for accurate object detection and semantic segmentation, https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf, https://ieeexplore.ieee.org/document/6909475
+ [CNN][Blog] Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5), https://www.arxiv-vanity.com/papers/1311.2524/
+ [CNN][Paper] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, https://arxiv.org/abs/1506.01497
+ [CNN][Codes] Faster R-CNN, https://paperswithcode.com/method/faster-r-cnn
+ [CNN][Paper] Fast R-CNN
+ [CNN][Paper] RFCN
+ [CNN][Paper] Mask RCNN
+ [CNN][Paper] SSD
+ [CNN][Paper] U-Net: Convolutional Networks for Biomedical Image Segmentation, https://arxiv.org/abs/1505.04597
+ [CNN][Paper] U-Net, https://paperswithcode.com/method/u-net
+ [CNN][Blog] Understand Semantic segmentation with the Fully Convolutional Network U-Net step-by-step, https://pallawi-ds.medium.com/understand-semantic-segmentation-with-the-fully-convolutional-network-u-net-step-by-step-9d287b12c852
+ [CNN][Paper] Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs, https://arxiv.org/abs/1701.08816 
+ [CNN][Paper] Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks, https://arxiv.org/abs/1705.03820
+ [CNN][Paper] DeepFace: Closing the Gap to Human-Level Performance in Face Verification, https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf, https://ranzato.github.io
+ [CNN][Paper] FaceNet: A Unified Embedding for Face Recognition and Clustering, https://arxiv.org/abs/1503.03832
+ [CNN][Paper] Visualizing and Understanding Convolutional Networks, https://arxiv.org/abs/1311.2901
+ [CNN][Paper] A Neural Algorithm of Artistic Style, https://arxiv.org/abs/1508.06576
+ [CNN][Codes] SAM (Segment Anything Model), https://huggingface.co/docs/transformers/main/en/model_doc/sam
+ [CNN][Paper] Hyena Hierarchy: Towards Larger Convolutional Language Models, https://arxiv.org/abs/2302.10866
+ [CNN][Paper] Going Deeper with Convolutions, https://arxiv.org/abs/1409.4842
+ CLIP: Connecting text and images, https://openai.com/research/clip, Codes: https://github.com/openai/CLIP; Paper: Learning Transferable Visual Models From Natural Language Supervision, https://arxiv.org/abs/2103.00020
+ Image GPT, https://openai.com/research/image-gpt
+ MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard, Zhu, Chen, Kalenichenko, Wang, Weyand, Andreetto, & Adam, 2017), https://arxiv.org/abs/1704.04861, https://github.com/fchollet/deep-learning-with-python-notebooks
+ MobileNetV2: Inverted Residuals and Linear Bottlenecks (Sandler, Howard, Zhu, Zhmoginov &Chen, 2018), https://arxiv.org/abs/1801.04381
+ EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan & Le, 2019), https://arxiv.org/abs/1905.11946
+ You Only Look Once: Unified, Real-Time Object Detection (Redmon, Divvala, Girshick & Farhadi, 2015), https://arxiv.org/abs/1506.02640
+ YOLO9000: Better, Faster, Stronger (Redmon & Farhadi, 2016), https://arxiv.org/abs/1612.08242
+ YAD2K (GitHub: allanzelener), https://github.com/allanzelener/YAD2K
+ YOLO: Real-Time Object Detection, https://pjreddie.com/darknet/yolo/
+ Fully Convolutional Architectures for Multi-Class Segmentation in Chest Radiographs (Novikov, Lenis, Major, Hlad≈Øvka, Wimmer & B√ºhler, 2017), https://arxiv.org/abs/1701.08816
+ Automatic Brain Tumor Detection and Segmentation Using U-Net Based Fully Convolutional Networks (Dong, Yang, Liu, Mo & Guo, 2017), https://arxiv.org/abs/1705.03820
+ U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger, Fischer & Brox, 2015), https://arxiv.org/abs/1505.04597
+ FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff, Kalenichenko & Philbin, 2015), https://arxiv.org/pdf/1503.03832.pdf
+ DeepFace: Closing the Gap to Human-Level Performance in Face Verification (Taigman, Yang, Ranzato & Wolf), https://scontent.ffjr7-1.fna.fbcdn.net/v/t39.8562-6/240890413_887772915161178_4705912772854439762_n.pdf?_nc_cat=109&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=F5lzaUIsBnAAX8Z7wVC&_nc_ht=scontent.ffjr7-1.fna&oh=00_AfC8DK3sP5sye19oUs9dVjfLPdibMfA-3Z-MuBENbTogqg&oe=645556BF
+ [FaceNet,DeepFace] facenet (GitHub: davidsandberg), https://github.com/davidsandberg/facenet
+ How to Develop a Face Recognition System Using FaceNet in Keras (Jason Brownlee, 2019), https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/
+ keras-facenet/notebook/tf_to_keras.ipynb (GitHub: nyoki-mtl), https://github.com/nyoki-mtl/keras-facenet/blob/master/notebook/tf_to_keras.ipynb
+ A Neural Algorithm of Artistic Style (Gatys, Ecker & Bethge, 2015), https://arxiv.org/abs/1508.06576
+ Convolutional neural networks for artistic style transfer, https://harishnarayanan.org/writing/artistic-style-transfer/
+ TensorFlow Implementation of "A Neural Algorithm of Artistic Style‚Äù, http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style
+ Very Deep Convolutional Networks For Large-Scale Image Recognition (Simonyan & Zisserman, 2015), https://arxiv.org/pdf/1409.1556.pdf
+ Pretrained models (MatConvNet), https://www.vlfeat.org/matconvnet/pretrained/
+ Dropout: A Simple Way to Prevent Neural Networks from Overfitting, https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
+ Dropout: A Simple Way to Prevent Neural Networks from Overfitting, https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,
+ Dropout is NOT All You Need to Prevent Gradient Leakage, https://arxiv.org/abs/2208.06163
+  Dropout, https://paperswithcode.com/method/dropout
+ Modified Dropout for Training Neural Network, https://www.cs.cmu.edu/~epxing/Class/10715/project-reports/DuyckLeeLei.pdf


# [Paper Reproduction]



## 2023.Jul.04
+ [LLM] A Survey of Large Language Models, https://arxiv.org/pdf/2303.18223.pdf
+ [CNN] Convolutional Neural Networks for Sentence Classification, https://arxiv.org/abs/1408.5882
+ [CNN] Character-level Convolutional Networks for Text Classification, https://arxiv.org/abs/1509.01626
+ [CNN] A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification, http://arxiv.org/abs/1510.03820
+ [ResNet] Residual Network Introduced by He et al. in Deep Residual Learning for Image Recognition, https://paperswithcode.com/method/resnet
+ [VGG] VGG Introduced by Simonyan et al. in Very Deep Convolutional Networks for Large-Scale Image Recognition, https://paperswithcode.com/method/vgg
+ [CNN][Book] Chapter 9 Convolutional Networks, https://www.deeplearningbook.org/contents/convnets.html
+ [CNN][Standford CS-230] Convolutional Neural Networks cheatsheet, https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks#
+ [CNN][Standford] Welcome to the Deep Learning Tutorial! http://ufldl.stanford.edu/tutorial/
+ [CNN][Course] Convolutional Neural Networks, https://www.coursera.org/learn/convolutional-neural-networks/
+ [CNN][Codes] Convolutional Neural Networks for Text, https://lena-voita.github.io/nlp_course/models/convolutional.html
+ [CNN][Paper] LeNet-5, GradientBased Learning Applied to Document Recognition, http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf
+ [CNN][Paper] AlexNet, ImageNet Classification with Deep Convolutional Neural Networks, https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
+ [CNN][Paper] VGG, Very Deep Convolutional Networks for Large-Scale Image Recognition, https://arxiv.org/abs/1409.1556
+ [VGG] VGG Introduced by Simonyan et al. in Very Deep Convolutional Networks for Large-Scale Image Recognition, https://paperswithcode.com/method/vgg
+ [CNN][Paper] ResNet, Deep Residual Learning for Image Recognition, https://arxiv.org/abs/1512.03385
+ [CNN][Paper] Networks in Networks and 1x1 Convolutions, Network In Network, https://arxiv.org/abs/1312.4400
+ [CNN][Paper] Inception Network Motivation, Going Deeper with Convolutions, https://arxiv.org/abs/1409.4842
+ [CNN][Paper] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, https://arxiv.org/abs/1704.04861
+ [CNN][Paper] MobileNetV2: Inverted Residuals and Linear Bottlenecks, https://arxiv.org/abs/1801.04381
+ [CNN][Paper] Searching for MobileNetV3, https://arxiv.org/abs/1905.02244
+ [CNN][Paper] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, https://arxiv.org/abs/1905.11946
+ [CNN][Codes] https://github.com/kaiminghe
+ TART: A plug-and-play Transformer module for task-agnostic reasoning, https://arxiv.org/abs/2306.07536
+ Preference Ranking Optimization for Human Alignment, https://arxiv.org/abs/2306.17492
+ Restart Sampling for Improving Generative Processes, https://arxiv.org/abs/2306.14878
+ Bring Your Own Data! Self-Supervised Evaluation for Large Language Models, https://arxiv.org/abs/2306.13651
+ Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects, https://arxiv.org/abs/2306.10125
+ Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations, https://arxiv.org/abs/2306.09255
+ Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model, https://arxiv.org/abs/1911.08265
+ LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering, https://arxiv.org/abs/2305.03403
+ Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation, https://arxiv.org/abs/2306.07954
+ Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow, https://arxiv.org/abs/2306.07209
+ Evaluating the Social Impact of Generative AI Systems in Systems and Society, https://arxiv.org/abs/2306.05949
+ Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models, https://arxiv.org/abs/2306.08997
+ DCdetector: Dual Attention Contrastive Representation Learning for Time Series Anomaly Detection, https://arxiv.org/abs/2306.10347
+ Reinforcement Learning, An Introduction, 2nd Edition, Richard S. Sutton and Andrew G. Barto, http://incompleteideas.net/book/RLbook2020.pdf
+ Towards Integrative AI, Xuedong Huang, Microsoft Corporation, USA

## 2023.Jul.02
+ ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
+ ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.
+ AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.
+ Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.
+ Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.
+ BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.
+ BARThez (from √âcole polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.
+ BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.
+ BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.

