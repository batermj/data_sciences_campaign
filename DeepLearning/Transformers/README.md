Transformers

- - -

# [Courses]
+ Attention Is All You Need, https://www.youtube.com/watch?v=iDulhoQ2pro
+ å°å¤§æå®æ¯…21å¹´æœºå™¨å­¦ä¹ è¯¾ç¨‹ self-attentionå’Œtransformer, https://www.bilibili.com/video/BV1Xp4y1b7ih/?spm_id_from=autoNext
+ Transformer models - Hugging Face Course, https://huggingface.co/course/chapter1
+ 


# [Blogs]
+ The Annotated Transformer, http://nlp.seas.harvard.edu/2018/04/03/attention.html
+ multiscale-vision-transformers-an-architecture-for-modeling-visual-data, https://ai.facebook.com/blog/multiscale-vision-transformers-an-architecture-for-modeling-visual-data/
+ The Illustrated Transformer, https://jalammar.github.io/illustrated-transformer/
+ Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention),https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
+ Self-Attentionå’ŒTransformer, https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer
+ A Complete Learning Path To Transformers (With Guide To 23 Architectures), https://analyticsindiamag.com/a-complete-learning-path-to-transformers/
+ The Essential Guide to Transformers, the Key to Modern SOTA AI, https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html
+ How Transformers work in deep learning and NLP: an intuitive introduction, https://theaisummer.com/transformer/
+ Comprehensive Guide To Transformers, https://analyticsindiamag.com/a-comprehensive-guide-to-transformers/
+ Zero-Shot Controlle Generation with Encoder-Decoder Transformers, https://www.amazon.science/latest-news/controlling-language-generation-models-without-training-data?utm_content=buffer9ee31&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer
+ This course will teach you about natural language processing (NLP) using libraries from the Hugging Face ecosystem â€” ğŸ¤— Transformers, ğŸ¤— Datasets, ğŸ¤— Tokenizers, and ğŸ¤— Accelerate â€” as well as the Hugging Face Hub. Itâ€™s completely free and without ads., https://huggingface.co/course/chapter1
+ Transformeråœ¨å›¾åƒå¤åŸé¢†åŸŸçš„é™ç»´æ‰“å‡»ï¼ETHæå‡ºSwinIRï¼šå„é¡¹ä»»åŠ¡å…¨é¢é¢†å…ˆ, https://mp.weixin.qq.com/s/w28DdccPeW_2vl2S_t8sAA
+ Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training, https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training
+ åä¸ºè”åˆåŒ—å¤§ã€æ‚‰å°¼å¤§å­¦å¯¹ Visual Transformer çš„æœ€æ–°ç»¼è¿°, https://zhuanlan.zhihu.com/p/339418612
+ [***å…¬å¼åŒ–è¡¨ç¤ºï¼Œå…¬å¼æ¨å¯¼æµç•…ï¼Œæ¦‚å¿µæ¸…æ™°***] Transformer|æ·±åº¦å­¦ä¹ ï¼ˆæå®æ¯…ï¼‰ï¼ˆå…«ï¼‰,https://zhuanlan.zhihu.com/p/325965103
+ 10åˆ†é’Ÿå¸¦ä½ æ·±å…¥ç†è§£TransformeråŸç†åŠå®ç°, https://zhuanlan.zhihu.com/p/80986272
+ Transformerçš„ä¸€å®¶ï¼https://zhuanlan.zhihu.com/p/350987218
+ å¤šç§Attentionä¹‹é—´çš„å¯¹æ¯”(ä¸Šï¼‰ï¼Œhttps://zhuanlan.zhihu.com/p/336352895
+ Transformerç»“æ„ä¸­è·å¾—ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„æ¢ç©¶ï¼Œ https://zhuanlan.zhihu.com/p/100112141
+ The Transformer: Attention Is All You Needï¼Œ https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/
+ The Illustrated Transformerï¼Œ https://jalammar.github.io/illustrated-transformer/
+ è¯¦ç»†è§£è¯»Googleæ–°ä½œ | æ•™ä½ How to trainè‡ªå·±çš„Transfomeræ¨¡å‹ï¼Ÿhttps://zhuanlan.zhihu.com/p/383020224
+ Transformer in Deep Learningï¼šè¶…è¯¦ç»†è®²è§£Attentionæœºåˆ¶ï¼ˆäºŒï¼‰ï¼Œhttps://zhuanlan.zhihu.com/p/404631212
+ 2020å¹´9æœˆè°·æ­Œç ”ç©¶ç»™å‡ºçš„ç»¼è¿°â€œEfficient Transformers: A Surveyâ€ï¼Œ https://zhuanlan.zhihu.com/p/316865623
+ The Annotated Transformerï¼Œ http://nlp.seas.harvard.edu/2018/04/03/attention.html
+ Transformer|æ·±åº¦å­¦ä¹ ï¼ˆæå®æ¯…ï¼‰ï¼ˆå…«ï¼‰,https://zhuanlan.zhihu.com/p/325965103
+ æ‰‹æ’•Transformer-XLæºç ï¼šè®ºæ–‡é‡Œçš„è¿·ä¹‹å›°æƒ‘çš„è§£ç­”éƒ½åœ¨è¿™é‡Œå•¦,https://zhuanlan.zhihu.com/p/151444825
+ Vision Transformer è¶…è¯¦ç»†è§£è¯» (åŸç†åˆ†æ+ä»£ç è§£è¯») (ä¸ƒ)https://zhuanlan.zhihu.com/p/358102861
+ ç‰›æ´¥å¤§å­¦æå‡ºViPï¼šç”¨Transformerè¡¨ç¤ºéƒ¨åˆ†-æ•´ä½“å±‚æ¬¡ç»“æ„, https://zhuanlan.zhihu.com/p/390066749
+ Vision Transformer, https://zhuanlan.zhihu.com/p/273652295
+ So-ViTï¼šè§†è§‰Transformerçš„Mind Visual Tokens, https://zhuanlan.zhihu.com/p/367508871
+ Fastformer: Additive Attention Can Be All You Need | Paper Explained, https://www.youtube.com/watch?v=Ich5TIvdYRE
+ 



# [Papers-Surveys]
+ A Survey of Transformers, https://arxiv.org/abs/2106.04554
+ Transformers in Vision: A Survey, https://arxiv.org/abs/2101.01169
+ A Survey on Vision Transformer, https://arxiv.org/abs/2012.12556
+ 2021-A Survey of Transformers, https://www.jianshu.com/p/98d82f83b6ba
+ 

# [Papers- Computer Vision]
+ Vision Transformer with Progressive Sampling, https://arxiv.org/abs/2108.01684
+ ViTGAN: Training GANs with Vision Transformers, https://arxiv.org/abs/2107.04589
+ Do Vision Transformers See Like Convolutional Neural Networks? https://arxiv.org/abs/2108.08810
+ Focal Self-attention for Local-Global Interactions in Vision Transformers, https://arxiv.org/abs/2107.00641
+ Is it Time to Replace CNNs with Transformers for Medical Images? https://arxiv.org/abs/2108.09038
+ Video Relation Detection via Tracklet based Visual Transformer, https://arxiv.org/abs/2108.08669
+ [***Vision Transformer(ViT)***] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, https://arxiv.org/abs/2106.10270
+ [moco v3] An Empirical Study of Training Self-Supervised Vision Transformers, https://arxiv.org/abs/2104.02057
+ [***Facebook***] Training data-efficient image transformers & distillation through attention, https://arxiv.org/abs/2012.12877
+ Visual Transformers: Token-based Image Representation and Processing for Computer Vision, https://arxiv.org/abs/2006.03677
+ Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, https://arxiv.org/abs/2101.11986
+ [***Microsoft***] CvT: Introducing Convolutions to Vision Transformers, https://arxiv.org/abs/2103.15808, Codes: #1, https://github.com/leoxiaobin/CvT; #2, 
+ [***Microsoft***] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, https://arxiv.org/abs/2103.14030
+ SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257
+ Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers, https://arxiv.org/abs/2012.15840
+ Rethinking Spatial Dimensions of Vision Transformers, https://arxiv.org/abs/2103.16302
+ Pre-Trained Image Processing Transformer, https://arxiv.org/abs/2012.00364
+ All Tokens Matter: Token Labeling for Training Better Vision Transformers, https://arxiv.org/abs/2104.10858
+ Emerging Properties in Self-Supervised Vision Transformers, https://arxiv.org/abs/2104.14294
+ DeepViT: Towards Deeper Vision Transformer, https://arxiv.org/abs/2103.11886
+ Going deeper with Image Transformers, https://arxiv.org/abs/2103.17239
+ Incorporating Convolution Designs into Visual Transformers, https://arxiv.org/abs/2103.11816
+ Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation, https://arxiv.org/abs/2105.05537
+ LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference, https://arxiv.org/abs/2104.01136
+ Pre-Trained Image Processing Transformer (IPT),https://github.com/huawei-noah/Pretrained-IPT
+ 

# [Papers- Object Detection]
+ [***DETR (DEtection TRansformer)***][***Facebook***] End-to-End Object Detection with Transformers, https://arxiv.org/abs/2005.12872, Codes: https://github.com/facebookresearch/detr
+ Deformable DETR: Deformable Transformers for End-to-End Object Detection, https://arxiv.org/abs/2010.04159, CODES: https://arxiv.org/pdf/2010.04159.pdf
+ 
+ 

# [Papers- Image Super-Resolution]
+ Learning Texture Transformer Network for Image Super-Resolution, https://arxiv.org/abs/2006.04139
+ 

# [Papers- Image Segmentation]

## [Papers- Medical Image Segmentation]
+ TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation, https://arxiv.org/abs/2102.04306
+ UNETR: Transformers for 3D Medical Image Segmentation, https://arxiv.org/abs/2103.10504, Codes: https://github.com/Project-MONAI/research-contributions/tree/master/UNETR, Tutorial: https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/unetr_btcv_segmentation_3d.ipynb
+ 


# [Papers- Image Classification]
+ [***CrossViT***] [***IBM***] CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification, https://arxiv.org/abs/2103.14899, Codes: https://github.com/IBM/CrossViT

# [Papers- NLP]
* Fastformer: Additive Attention is All You Need,https://arxiv.org/abs/2108.09084
* Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text  Models, https://arxiv.org/abs/2108.08877
* [Attention Is All You Need - arXiv 2017)](https://arxiv.org/abs/1706.03762)  
* [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context - arXiv 2019)](https://arxiv.org/abs/1901.02860)  
* [Universal Transformers - ICLR 2019)](https://arxiv.org/abs/1807.03819) 
* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer - arXiv 2019)](https://arxiv.org/abs/1910.10683) 
* [Reformer: The Efficient Transformer - ICLR 2020)](https://arxiv.org/abs/2001.04451) 
* [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799) (ACL2019)
* [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) (ACL2019) [[github](https://github.com/kimiyoung/transformer-xl)]
* [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
* [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015) (EMNLP2019)
* [Compressive Transformers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)
* [The Evolved Transformer](https://arxiv.org/abs/1901.11117) (ICML2019)
* [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) (ICLR2020) [[github](https://github.com/google/trax/tree/master/trax/models/reformer)]
* [GRET: Global Representation Enhanced Transformer](https://arxiv.org/abs/2002.10101) (AAAI2020)
* [Transformer on a Diet](https://arxiv.org/abs/2002.06170) [[github](https://github.com/cgraywang/transformer-on-diet)]
* [Efficient Content-Based Sparse Attention with Routing Transformers](https://openreview.net/forum?id=B1gjs6EtDr)
* [BP-Transformer: Modelling Long-Range Context via Binary Partitioning](https://arxiv.org/abs/1911.04070)  
* [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf)   
* [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)  
* [UnifiedQA: Crossing Format Boundaries With a Single QA System](https://arxiv.org/pdf/2005.00700.pdf) [[github](https://github.com/allenai/unifiedqa)]  
* [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf) 
* Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting, https://arxiv.org/abs/2012.07436, Codes: https://github.com/zhouhaoyi/Informer2020
* The Evolved Transformer, https://arxiv.org/abs/1901.11117
* 

# [Papers- Machine Translation]
* [The Evolved Transformer - David R. So(2019)](https://arxiv.org/pdf/1901.11117.pdf)  


# [Papers- GANs]
+ Self-Attention Generative Adversarial Networks, https://arxiv.org/abs/1805.08318

# [Papers- OCR]
+ TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models, https://arxiv.org/abs/2109.10282
+ 

# [Papers- MISC]
+ Self-Attention with Relative Position Representations, https://arxiv.org/abs/1803.02155
+ An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ TransformerFusion: Monocular RGB Scene Reconstruction using Transformers, https://arxiv.org/abs/2107.02191
+ BumbleBee: A Transformer for Music, https://arxiv.org/abs/2107.03443
+ Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation, https://github.com/NVIDIA-Merlin/publications/tree/main/2021_acm_recsys_transformers4rec
+ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
+ Thinking Like Transformers, https://arxiv.org/abs/2106.06981
+ Mobile-Former: Bridging MobileNet and Transformer, https://arxiv.org/abs/2108.05895
+ FNet: Mixing Tokens with Fourier Transforms, https://arxiv.org/abs/2105.03824
+



# [Books]
+ ***Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques***

# [Codes]
+  Transformerçš„PyTorchå®ç°ï¼Œ https://www.bilibili.com/video/BV1mk4y1q7eK?p=2ï¼Œ ï¼ŒTransformer çš„ PyTorch å®ç°ï¼Œ https://wmathor.com/index.php/archives/1455/
+  Transformers, https://github.com/huggingface/transformers
+  


