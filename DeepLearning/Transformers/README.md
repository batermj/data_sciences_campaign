Transformers

- - -

# [Top-Players]
+ DeepMind, Predicting gene expression with AI, https://deepmind.com/
+ Facebook AI, https://ai.facebook.com
+ HuggingFace, 
+ 

# [Books]
+ Natural Language Processing with Transformers Book, https://transformersbook.com
+ 

# [Courses]
+ Attention Is All You Need, https://www.youtube.com/watch?v=iDulhoQ2pro
+ å°å¤§æå®æ¯…21å¹´æœºå™¨å­¦ä¹ è¯¾ç¨‹ self-attentionå’Œtransformer, https://www.bilibili.com/video/BV1Xp4y1b7ih/?spm_id_from=autoNext
+ Transformer models - Hugging Face Course, https://huggingface.co/course/chapter1
+ 


# [Blogs]
+ The Annotated Transformer, http://nlp.seas.harvard.edu/2018/04/03/attention.html
+ multiscale-vision-transformers-an-architecture-for-modeling-visual-data, https://ai.facebook.com/blog/multiscale-vision-transformers-an-architecture-for-modeling-visual-data/
+ The Illustrated Transformer, https://jalammar.github.io/illustrated-transformer/
+ Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention),https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
+ Self-Attentionå’ŒTransformer, https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer
+ A Complete Learning Path To Transformers (With Guide To 23 Architectures), https://analyticsindiamag.com/a-complete-learning-path-to-transformers/
+ The Essential Guide to Transformers, the Key to Modern SOTA AI, https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html
+ How Transformers work in deep learning and NLP: an intuitive introduction, https://theaisummer.com/transformer/
+ Comprehensive Guide To Transformers, https://analyticsindiamag.com/a-comprehensive-guide-to-transformers/
+ Zero-Shot Controlle Generation with Encoder-Decoder Transformers, https://www.amazon.science/latest-news/controlling-language-generation-models-without-training-data?utm_content=buffer9ee31&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer
+ This course will teach you about natural language processing (NLP) using libraries from the Hugging Face ecosystem â€” ğŸ¤— Transformers, ğŸ¤— Datasets, ğŸ¤— Tokenizers, and ğŸ¤— Accelerate â€” as well as the Hugging Face Hub. Itâ€™s completely free and without ads., https://huggingface.co/course/chapter1
+ Transformeråœ¨å›¾åƒå¤åŸé¢†åŸŸçš„é™ç»´æ‰“å‡»ï¼ETHæå‡ºSwinIRï¼šå„é¡¹ä»»åŠ¡å…¨é¢é¢†å…ˆ, https://mp.weixin.qq.com/s/w28DdccPeW_2vl2S_t8sAA
+ Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training, https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training
+ åä¸ºè”åˆåŒ—å¤§ã€æ‚‰å°¼å¤§å­¦å¯¹ Visual Transformer çš„æœ€æ–°ç»¼è¿°, https://zhuanlan.zhihu.com/p/339418612
+ [***å…¬å¼åŒ–è¡¨ç¤ºï¼Œå…¬å¼æ¨å¯¼æµç•…ï¼Œæ¦‚å¿µæ¸…æ™°***] Transformer|æ·±åº¦å­¦ä¹ ï¼ˆæå®æ¯…ï¼‰ï¼ˆå…«ï¼‰,https://zhuanlan.zhihu.com/p/325965103
+ 10åˆ†é’Ÿå¸¦ä½ æ·±å…¥ç†è§£TransformeråŸç†åŠå®ç°, https://zhuanlan.zhihu.com/p/80986272
+ Transformerçš„ä¸€å®¶ï¼https://zhuanlan.zhihu.com/p/350987218
+ å¤šç§Attentionä¹‹é—´çš„å¯¹æ¯”(ä¸Šï¼‰ï¼Œhttps://zhuanlan.zhihu.com/p/336352895
+ Transformerç»“æ„ä¸­è·å¾—ç›¸å¯¹ä½ç½®ä¿¡æ¯çš„æ¢ç©¶ï¼Œ https://zhuanlan.zhihu.com/p/100112141
+ The Transformer: Attention Is All You Needï¼Œ https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/
+ The Illustrated Transformerï¼Œ https://jalammar.github.io/illustrated-transformer/
+ è¯¦ç»†è§£è¯»Googleæ–°ä½œ | æ•™ä½ How to trainè‡ªå·±çš„Transfomeræ¨¡å‹ï¼Ÿhttps://zhuanlan.zhihu.com/p/383020224
+ Transformer in Deep Learningï¼šè¶…è¯¦ç»†è®²è§£Attentionæœºåˆ¶ï¼ˆäºŒï¼‰ï¼Œhttps://zhuanlan.zhihu.com/p/404631212
+ 2020å¹´9æœˆè°·æ­Œç ”ç©¶ç»™å‡ºçš„ç»¼è¿°â€œEfficient Transformers: A Surveyâ€ï¼Œ https://zhuanlan.zhihu.com/p/316865623
+ The Annotated Transformerï¼Œ http://nlp.seas.harvard.edu/2018/04/03/attention.html
+ Transformer|æ·±åº¦å­¦ä¹ ï¼ˆæå®æ¯…ï¼‰ï¼ˆå…«ï¼‰,https://zhuanlan.zhihu.com/p/325965103
+ æ‰‹æ’•Transformer-XLæºç ï¼šè®ºæ–‡é‡Œçš„è¿·ä¹‹å›°æƒ‘çš„è§£ç­”éƒ½åœ¨è¿™é‡Œå•¦,https://zhuanlan.zhihu.com/p/151444825
+ Vision Transformer è¶…è¯¦ç»†è§£è¯» (åŸç†åˆ†æ+ä»£ç è§£è¯») (ä¸ƒ)https://zhuanlan.zhihu.com/p/358102861
+ ç‰›æ´¥å¤§å­¦æå‡ºViPï¼šç”¨Transformerè¡¨ç¤ºéƒ¨åˆ†-æ•´ä½“å±‚æ¬¡ç»“æ„, https://zhuanlan.zhihu.com/p/390066749
+ Vision Transformer, https://zhuanlan.zhihu.com/p/273652295
+ So-ViTï¼šè§†è§‰Transformerçš„Mind Visual Tokens, https://zhuanlan.zhihu.com/p/367508871
+ Fastformer: Additive Attention Can Be All You Need | Paper Explained, https://www.youtube.com/watch?v=Ich5TIvdYRE
+ Anatomy of the Beast with many heads! [with code], https://www.linkedin.com/pulse/anatomy-beast-many-heads-code-ibrahim-sobh-phd/
+ DeepMind Introduces â€˜Enformerâ€™, A Deep Learning Architecture For Predicting Gene Expression From DNA Sequence, https://www.marktechpost.com/2021/10/06/deepmind-introduces-enformer-a-deep-learning-architecture-for-predicting-gene-expression-from-dna-sequence/
+ Predicting gene expression with AI, https://deepmind.com/blog/article/enformer
+ The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning, https://attentionneuron.github.io
+ DeepMind Introduces â€˜Enformerâ€™, A Deep Learning Architecture For Predicting Gene Expression From DNA Sequence, https://www.marktechpost.com/2021/10/06/deepmind-introduces-enformer-a-deep-learning-architecture-for-predicting-gene-expression-from-dna-sequence/
+ Anticipative Video Transformer: Improving AIâ€™s ability to predict whatâ€™s next in a video, https://ai.facebook.com/blog/anticipative-video-transformer-improving-ais-ability-to-predict-whats-next-in-a-video
+ Google Research Introduces â€˜SCENICâ€™: An Open-Source JAX Library For Computer Vision Research, https://www.marktechpost.com/2021/10/30/google-research-introduces-scenic-an-open-source-jax-library-for-computer-vision-research/
+ Paper Review: Swin Transformer V2 Scaling Up Capacity and Resolution, https://andlukyane.com/blog/paper-review-swin-v2
+ Transformers from Scratch, https://e2eml.school/transformers.html
+ The Researchers Propose a Family of Next Generation Transformer Models That Use Sparse Layers to Scale Efficiently and Perform Unbatched Decoding Much Faster than the Standard Type, https://www.marktechpost.com/2021/12/09/the-researchers-propose-a-family-of-next-generation-transformer-models-that-use-sparse-layers-to-scale-efficiently-and-perform-unbatched-decoding-much-faster-than-the-standard-type/
+ Training Generalist Agents with Multi-Game Decision Transformers, https://ai.googleblog.com/2022/07/training-generalist-agents-with-multi.html


# [Papers]

## [Papers- Attension]
+ Compositional Attention: Disentangling Search and Retrieval, https://arxiv.org/abs/2110.09419
+ The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning, https://attentionneuron.github.io
+ Awesome NLP Paper Discussions, https://github.com/huggingface/awesome-papers#planned-discussions
+ Compositional Attention: Disentangling Search and Retrieval, https://arxiv.org/abs/2110.09419
+ FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, https://arxiv.org/pdf/2205.14135.pdf
+ Attention is not Explanation, https://arxiv.org/pdf/1902.10186.pdf


## [Papers- Surveys]
+ An Impartial Take to the CNN vs Transformer Robustness Contest, https://arxiv.org/pdf/2207.11347.pdf
+ A Survey of Transformers, https://arxiv.org/abs/2106.04554
+ Transformers in Vision: A Survey, https://arxiv.org/abs/2101.01169
+ A Survey on Vision Transformer, https://arxiv.org/abs/2012.12556
+ A Survey of Visual Transformers, https://arxiv.org/abs/2111.06091
+ 2021-A Survey of Transformers, https://www.jianshu.com/p/98d82f83b6ba
+ AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, https://arxiv.org/abs/2108.05542
+ Efficient Transformers: A Survey, https://arxiv.org/pdf/2009.06732.pdf
+ Pure Transformers are Powerful Graph Learners, https://arxiv.org/abs/2207.02505



## [Papers- Formal Algorithms]
+ Formal Algorithms for Transformers, https://arxiv.org/abs/2207.09238

## [Papers- Deep Transformer]
+ DeepNet: Scaling Transformers to 1,000 Layers, https://arxiv.org/abs/2203.00555

## [Papers- Mobile]
+ MOBILEVIT: LIGHT-WEIGHT, GENERAL-PURPOSE, AND MOBILE-FRIENDLY VISION TRANSFORMER, https://arxiv.org/pdf/2110.02178.pdf
+ Mobile-Former: Bridging MobileNet and Transformer, https://arxiv.org/abs/2108.05895

## [Papers- JAX Library]
+ SCENIC: A JAX Library for Computer Vision Research and Beyond, https://arxiv.org/pdf/2110.11403.pdf
+ 


## [Papers- Graph Transformer]
+ Recipe for a General, Powerful, Scalable Graph Transformer, https://arxiv.org/abs/2205.12454
+ 


## [Papers- Reinforcement Learning]
+ Decision Transformer: Reinforcement Learning via Sequence Modeling, https://arxiv.org/abs/2106.01345


## [Papers- Multi Modal]
+ Attention Bottlenecks for Multimodal Fusion, https://arxiv.org/pdf/2107.00135.pdf
+ Multimodal Transformer for Unaligned Multimodal Language Sequences, https://arxiv.org/pdf/1906.00295.pdf


## [Papers- Time Series]
+ Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case, https://arxiv.org/abs/2001.08317
+ Multivariate Time Series Forecasting with Transformers, https://towardsdatascience.com/multivariate-time-series-forecasting-with-transformers-384dc6ce989b
+ 

## [Papers- Bayesian Inference]
+ Transformers Can Do Bayesian Inference, https://arxiv.org/abs/2112.10510
+ 

## [Papers- High Transformer Quality in Linear Time]
+ Transformer Quality in Linear Time, https://arxiv.org/pdf/2202.10447.pdf
+ 

## [Papers- AI Alternative to Transformers]
+ HyperMixer: An MLP-based Green AI Alternative to Transformers, https://arxiv.org/abs/2203.03691
+ 

## [Papers- Bringing Old Films Back to Life]
+ Bringing Old Films Back to Life, https://arxiv.org/pdf/2203.17276.pdf


## [Papers- MISC]
+ Self-Attention with Relative Position Representations, https://arxiv.org/abs/1803.02155
+ An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ TransformerFusion: Monocular RGB Scene Reconstruction using Transformers, https://arxiv.org/abs/2107.02191
+ BumbleBee: A Transformer for Music, https://arxiv.org/abs/2107.03443
+ Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation, https://github.com/NVIDIA-Merlin/publications/tree/main/2021_acm_recsys_transformers4rec
+ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805
+ Thinking Like Transformers, https://arxiv.org/abs/2106.06981
+ FNet: Mixing Tokens with Fourier Transforms, https://arxiv.org/abs/2105.03824
+ Convolutional Xformers for Vision, https://arxiv.org/abs/2201.10271
+ Transformer Memory as a Differentiable Search Index, https://arxiv.org/abs/2202.06991
+ [Microsoft] Unveiling Transformers with LEGO: a synthetic reasoning task, https://arxiv.org/abs/2206.04301
+ [Google] Frequency Effects on Syntactic Rule Learning in Transformers, https://arxiv.org/pdf/2109.07020.pdf
+ [Google] Multi-Game Decision Transformers, https://sites.google.com/view/multi-game-transformers
+ Levenshtein Transformer, https://arxiv.org/pdf/1905.11006.pdf


# [Books]
+ ***Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques***

# [Codes]
+  Transformerçš„PyTorchå®ç°ï¼Œ https://www.bilibili.com/video/BV1mk4y1q7eK?p=2ï¼Œ ï¼ŒTransformer çš„ PyTorch å®ç°ï¼Œ https://wmathor.com/index.php/archives/1455/
+  Transformers, https://github.com/huggingface/transformers
+  Saint Lightning This repository contains an implementation of SAINT (Self-Attention and Intersample Attention Transformer) using Pytorch-Lightning as a framework and Hydra for the configuration. Find the paper on arxiv, https://github.com/Actis92/lit-saint
+  NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion., https://github.com/microsoft/NUWA
+  

## [Codes- Deep Transformer]
+ AI Fundamentals DeepNet: scaling Transformers to 1,000 Layers and beyond, https://github.com/microsoft/unilm

## [Codes- Attention]
+ A TensorFlow Implementation of Attention Is All You Need, https://github.com/Kyubyong/transformer
+ FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, https://github.com/hazyresearch/flash-attention
+ 

## [Codes- Chatbots]
+ æ™ºèƒ½èŠå¤©æœºå™¨äººï¼Œä½ çš„AIå¥³å‹, https://github.com/godweiyang/chatbot

## [Codes- Computer Vision]
+ Swin Transformer, https://github.com/microsoft/Swin-Transformer
+ 

## [Codes- NER]
+ Flat-Lattice-Transformer, https://github.com/LeeSureman/Flat-Lattice-Transformer
+ 

## [Codes- Mobile]
+ MobileViT: A mobile-friendly Transformer-based model for image classification, https://keras.io/examples/vision/mobilevit/


## [Codes- HuggingFace]
+ An End-to-End Pipeline with Hugging Face transformers, https://valohai.com/blog/hugging-face-pipeline/?utm_campaign=Blogs+Feed+ads+(New+Target+Group)&utm_source=linkedin&utm_medium=paid&hsa_acc=503139555&hsa_cam=607695413&hsa_grp=192295833&hsa_ad=165444873&hsa_net=linkedin&hsa_ver=3
+ 

## [Codes- JAX library]
+ Scenic is developed in JAX and uses Flax., https://github.com/google-research/scenic
+ 

## [Codes- Bringing Old Films Back to Life]
+ Bringing Old Films Back to Life, https://github.com/raywzy/Bringing-Old-Films-Back-to-Life
+ Bringing Old Films Back to Life, http://raywzy.com/Old_Film/
+ 


## [Papers- Multi Modal]
+ Multimodal Transformer for Unaligned Multimodal Language Sequences, https://github.com/yaohungt/Multimodal-Transformer

## [Codes- Misc]
+ [Google Research: Language] Frequency Effects on Syntactic Rule Learning in Transformers, https://github.com/google-research/language
+ [Facebook] Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks., https://github.com/facebookresearch/fairseq


