Computer Vision

# [Blogs]
+ MSRA的Transformer跨界超越CNN，还解决了计算复杂度难题, https://zhuanlan.zhihu.com/p/361009162
+ [Swin Transformer] Hierarchical Vision Transformer using ShiftedWindows, https://liumengyang.xyz/swin-transformer/
+ Vision Transformers (ViT) in Image Recognition – 2022 Guide, https://viso.ai/deep-learning/vision-transformer-vit/
+ Vision Transformer, https://towardsdatascience.com/tagged/vision-transformer
+ Vision Transformer Explained, https://www.youtube.com/watch?v=01KZ4AAZC6k
+ Vision Transformers: A Review — Part I, https://sertiscorp.medium.com/vision-transformers-a-review-part-i-19153d1f5c3b
+ Vision Transformers: A Review — Part II, https://sertiscorp.medium.com/vision-transformers-a-review-part-ii-a31136cf848d
+ Vision Transformers: A Review — Part III, https://sertiscorp.medium.com/vision-transformers-a-review-part-iii-cd07d699a504
+ 

# [Papers- Survey]
+ A Survey on Vision Transformer, https://arxiv.org/abs/2012.12556

# [Papers- Computer Vision]
+ Vision Transformer with Progressive Sampling, https://arxiv.org/abs/2108.01684
+ ViTGAN: Training GANs with Vision Transformers, https://arxiv.org/abs/2107.04589
+ Do Vision Transformers See Like Convolutional Neural Networks? https://arxiv.org/abs/2108.08810
+ Focal Self-attention for Local-Global Interactions in Vision Transformers, https://arxiv.org/abs/2107.00641
+ Is it Time to Replace CNNs with Transformers for Medical Images? https://arxiv.org/abs/2108.09038
+ Video Relation Detection via Tracklet based Visual Transformer, https://arxiv.org/abs/2108.08669
+ [***Vision Transformer(ViT)***] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, https://arxiv.org/abs/2010.11929
+ How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, https://arxiv.org/abs/2106.10270
+ [moco v3] An Empirical Study of Training Self-Supervised Vision Transformers, https://arxiv.org/abs/2104.02057
+ [***Facebook***] Training data-efficient image transformers & distillation through attention, https://arxiv.org/abs/2012.12877
+ Visual Transformers: Token-based Image Representation and Processing for Computer Vision, https://arxiv.org/abs/2006.03677
+ Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, https://arxiv.org/abs/2101.11986
+ [***Microsoft***] CvT: Introducing Convolutions to Vision Transformers, https://arxiv.org/abs/2103.15808, Codes: #1, https://github.com/leoxiaobin/CvT; #2, 
+ [***Microsoft***] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, https://arxiv.org/abs/2103.14030
+ [***Microsoft***] Swin Transformer V2: Scaling Up Capacity and Resolution, https://arxiv.org/abs/2111.09883
+ SwinIR: Image Restoration Using Swin Transformer, https://arxiv.org/abs/2108.10257
+ Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers, https://arxiv.org/abs/2012.15840
+ Rethinking Spatial Dimensions of Vision Transformers, https://arxiv.org/abs/2103.16302
+ Pre-Trained Image Processing Transformer, https://arxiv.org/abs/2012.00364
+ All Tokens Matter: Token Labeling for Training Better Vision Transformers, https://arxiv.org/abs/2104.10858
+ Emerging Properties in Self-Supervised Vision Transformers, https://arxiv.org/abs/2104.14294
+ DeepViT: Towards Deeper Vision Transformer, https://arxiv.org/abs/2103.11886
+ Going deeper with Image Transformers, https://arxiv.org/abs/2103.17239
+ Incorporating Convolution Designs into Visual Transformers, https://arxiv.org/abs/2103.11816
+ Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation, https://arxiv.org/abs/2105.05537
+ LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference, https://arxiv.org/abs/2104.01136
+ Pre-Trained Image Processing Transformer (IPT),https://github.com/huawei-noah/Pretrained-IPT
+ [***Facebook***] Anticipative Video Transformer: Improving AI’s ability to predict what’s next in a video, https://ai.facebook.com/blog/anticipative-video-transformer-improving-ais-ability-to-predict-whats-next-in-a-video
+ The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning, https://attentionneuron.github.io
+ NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion, https://arxiv.org/abs/2111.12417
+ PolyViT: Co-training Vision Transformers on Images, Videos and Audio, https://arxiv.org/abs/2111.12993
+ Vision Transformers are Robust Learners, https://arxiv.org/abs/2105.07581
+ 

## [Papers- Object Detection]
+ [***DETR (DEtection TRansformer)***][***Facebook***] End-to-End Object Detection with Transformers, https://arxiv.org/abs/2005.12872, Codes: https://github.com/facebookresearch/detr
+ Deformable DETR: Deformable Transformers for End-to-End Object Detection, https://arxiv.org/abs/2010.04159, CODES: https://arxiv.org/pdf/2010.04159.pdf
+ 
+ 

## [Papers- Object Tracking]
+ Efficient Visual Tracking with Exemplar Transformers, https://arxiv.org/pdf/2112.09686.pdf
+ 



## [Papers- Image Super-Resolution]
+ Learning Texture Transformer Network for Image Super-Resolution, https://arxiv.org/abs/2006.04139
+ 

## [Papers- Image Segmentation]
+ Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers, https://arxiv.org/abs/2012.15840
+ 

### [Papers- Medical Image Segmentation]
+ TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation, https://arxiv.org/abs/2102.04306
+ UNETR: Transformers for 3D Medical Image Segmentation, https://arxiv.org/abs/2103.10504, Codes: https://github.com/Project-MONAI/research-contributions/tree/master/UNETR, Tutorial: https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/unetr_btcv_segmentation_3d.ipynb
+ 


## [Papers- Image Classification]
+ [***CrossViT***] [***IBM***] CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification, https://arxiv.org/abs/2103.14899, Codes: https://github.com/IBM/CrossViT

## [Papers- Video-Language]
+ VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling, https://arxiv.org/abs/2111.12681
+ 

## [Papers- Human Pose]
+ End-to-End Human Pose and Mesh Reconstruction with Transformers, https://arxiv.org/abs/2012.09760
+ 

## [Papers- GANs]
+ Self-Attention Generative Adversarial Networks, https://arxiv.org/abs/1805.08318

## [Papers- OCR]
+ TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models, https://arxiv.org/abs/2109.10282


# [Codes]
+ OPTIMIZING VISION TRANSFORMER MODEL FOR DEPLOYMENT, https://pytorch.org/tutorials/beginner/vt_tutorial.html
+ Learning to tokenize in Vision Transformers, https://keras.io/examples/vision/token_learner/
+ The fine-tuning code and pre-trained ViT models are available at the GitHub of Google Research. You find them here:  The ViT models were pre-trained on the ImageNet and ImageNet-21k datasets.  Vision Transformer and MLP-Mixer Architectures, https://github.com/google-research/vision_transformer
+ https://github.com/huggingface/transformers
+ Vision Transformer (ViT), https://huggingface.co/docs/transformers/model_doc/vit
+ PyTorch Image Models, https://github.com/rwightman/pytorch-image-models
+ Vision Transformer - Pytorch, https://github.com/lucidrains/vit-pytorch
+ Keras, Learning to tokenize in Vision Transformers, https://keras.io/examples/vision/token_learner/
+ 

